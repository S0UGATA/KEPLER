transformer
[array(['T1003.001', 'T1005', 'T1012', 'T1016', 'T1021.001', 'T1027',
       'T1033', 'T1036.005', 'T1041', 'T1047', 'T1053.005', 'T1055',
       'T1056.001', 'T1057', 'T1059.003', 'T1068', 'T1070.004',
       'T1071.001', 'T1072', 'T1074.001', 'T1078', 'T1082', 'T1083',
       'T1090', 'T1095', 'T1105', 'T1106', 'T1110', 'T1112', 'T1113',
       'T1140', 'T1190', 'T1204.002', 'T1210', 'T1218.011', 'T1219',
       'T1484.001', 'T1518.001', 'T1543.003', 'T1547.001', 'T1548.002',
       'T1552.001', 'T1557.001', 'T1562.001', 'T1564.001', 'T1566.001',
       'T1569.002', 'T1570', 'T1573.001', 'T1574.002'], dtype=object)]
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(31090, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=50, bias=True)
)
tensor([[  102,   238,   348,  ...,     0,     0,     0],
        [  102,   111, 19912,  ...,     0,     0,     0],
        [  102,   111,  1542,  ...,     0,     0,     0],
        ...,
        [  102,   111, 26172,  ...,     0,     0,     0],
        [  102,   501,   168,  ...,     0,     0,     0],
        [  102,   168,   624,  ...,     0,     0,     0]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4071.)
                  P         R        F1      #
T1003.001  0.857143  0.571429  0.685714   21.0
T1005      0.750000  0.375000  0.500000   16.0
T1012      0.714286  1.000000  0.833333    5.0
T1016      0.473684  0.818182  0.600000   11.0
T1021.001  0.880000  0.814815  0.846154   27.0
T1027      0.728395  0.813793  0.768730  145.0
T1033      0.833333  0.909091  0.869565   11.0
T1036.005  0.280000  0.636364  0.388889   11.0
T1041      1.000000  0.416667  0.588235   12.0
T1047      0.916667  0.916667  0.916667   12.0
T1053.005  0.958333  1.000000  0.978723   23.0
T1055      0.938776  0.754098  0.836364   61.0
T1056.001  0.727273  1.000000  0.842105    8.0
T1057      0.823529  0.933333  0.875000   15.0
T1059.003  0.959184  0.746032  0.839286   63.0
T1068      0.222222  0.666667  0.333333    3.0
T1070.004  0.739130  0.894737  0.809524   19.0
T1071.001  0.862069  0.757576  0.806452   33.0
T1072      0.000000  0.000000  0.000000    3.0
T1074.001  0.272727  0.500000  0.352941    6.0
T1078      0.818182  0.720000  0.765957   25.0
T1082      0.842105  0.800000  0.820513   40.0
T1083      0.666667  0.941176  0.780488   17.0
T1090      0.500000  0.920000  0.647887   25.0
T1095      0.636364  0.700000  0.666667   10.0
T1105      0.758621  0.758621  0.758621   58.0
T1106      0.821429  0.621622  0.707692   37.0
T1110      0.666667  0.909091  0.769231   11.0
T1112      0.888889  0.941176  0.914286   17.0
T1113      1.000000  0.846154  0.916667   13.0
T1140      0.914634  0.842697  0.877193   89.0
T1190      0.833333  0.416667  0.555556   12.0
T1204.002  0.857143  0.705882  0.774194   17.0
T1210      0.333333  0.250000  0.285714    4.0
T1218.011  0.818182  1.000000  0.900000    9.0
T1219      0.416667  0.384615  0.400000   13.0
T1484.001  0.000000  0.000000  0.000000    5.0
T1518.001  0.500000  1.000000  0.666667    3.0
T1543.003  0.444444  1.000000  0.615385    4.0
T1547.001  0.846154  0.785714  0.814815   14.0
T1548.002  0.600000  0.857143  0.705882    7.0
T1552.001  0.500000  0.200000  0.285714    5.0
T1562.001  0.642857  0.818182  0.720000   11.0
T1564.001  0.500000  1.000000  0.666667    3.0
T1566.001  0.888889  0.800000  0.842105   20.0
T1569.002  0.285714  0.666667  0.400000    3.0
T1570      1.000000  0.142857  0.250000   14.0
T1573.001  1.000000  0.111111  0.200000    9.0
T1574.002  0.850000  0.944444  0.894737   18.0
(micro)    0.758350  0.758350  0.758350    NaN
(macro)    0.689123  0.706291  0.658646    NaN
