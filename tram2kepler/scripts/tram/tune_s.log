transformer
[array(['T1003.001', 'T1005', 'T1012', 'T1016', 'T1021.001', 'T1027',
       'T1033', 'T1036.005', 'T1041', 'T1047', 'T1053.005', 'T1055',
       'T1056.001', 'T1057', 'T1059.003', 'T1068', 'T1070.004',
       'T1071.001', 'T1072', 'T1074.001', 'T1078', 'T1082', 'T1083',
       'T1090', 'T1095', 'T1105', 'T1106', 'T1110', 'T1112', 'T1113',
       'T1140', 'T1190', 'T1204.002', 'T1210', 'T1218.011', 'T1219',
       'T1484.001', 'T1518.001', 'T1543.003', 'T1547.001', 'T1548.002',
       'T1552.001', 'T1557.001', 'T1562.001', 'T1564.001', 'T1566.001',
       'T1569.002', 'T1570', 'T1573.001', 'T1574.002'], dtype=object)]
                                                text      label
0  This file extracts credentials from LSASS simi...  T1003.001
1  It calls OpenProcess on lsass.exe with access ...  T1003.001
2  It spreads to Microsoft Windows machines using...      T1210
3                   SMB exploitation via EternalBlue      T1210
4                 SMBv1 Exploitation via EternalBlue      T1210
5  has the capability to exploit SMBv1 via the we...      T1210
6                      SMB copy and remote execution      T1570
7  This thread is then used to execute the SMB co...      T1570
8                      SMB copy and remote execution      T1570
9                      SMB Copy and Remote Execution      T1570
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[ 4758, 42797,    82,  ..., 50256, 50256, 50256],
        [11209,   347,   963,  ..., 50256, 50256, 50256],
        [   86,  9383,   220,  ..., 50256, 50256, 50256],
        ...,
        [17227, 16954,  5050,  ..., 50256, 50256, 50256],
        [28758,  1220,    66,  ..., 50256, 50256, 50256],
        [26239,  4433,   734,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4071.)
epoch:1|loss:0.10347410897706069|val_loss:0.07759969593847499
epoch:2|loss:0.06227658295949154|val_loss:0.05258585231415197
epoch:3|loss:0.03808190169808108|val_loss:0.04118843318200579
                  P         R        F1      #
T1003.001  0.809524  1.000000  0.894737   17.0
T1005      0.538462  0.538462  0.538462   13.0
T1012      0.000000  0.000000  0.000000    5.0
T1016      0.800000  0.615385  0.695652   13.0
T1021.001  0.580645  0.947368  0.720000   19.0
T1027      0.783019  0.658730  0.715517  126.0
T1033      1.000000  0.250000  0.400000    8.0
T1036.005  0.428571  0.250000  0.315789   12.0
T1041      0.666667  0.421053  0.516129   19.0
T1047      0.727273  0.842105  0.780488   19.0
T1053.005  0.928571  0.866667  0.896552   15.0
T1055      0.833333  0.800000  0.816327   50.0
T1056.001  0.684211  0.866667  0.764706   15.0
T1057      0.857143  0.666667  0.750000   18.0
T1059.003  0.688889  0.885714  0.775000   70.0
T1068      0.000000  0.000000  0.000000    2.0
T1070.004  0.500000  0.136364  0.214286   22.0
T1071.001  0.655172  0.826087  0.730769   23.0
T1072      0.000000  0.000000  0.000000    1.0
T1074.001  0.000000  0.000000  0.000000    6.0
T1078      0.700000  0.636364  0.666667   33.0
T1082      0.843750  0.771429  0.805970   35.0
T1083      0.214286  0.500000  0.300000   18.0
T1090      0.782609  0.545455  0.642857   33.0
T1095      1.000000  0.125000  0.222222    8.0
T1105      0.536082  0.881356  0.666667   59.0
T1106      0.630435  0.878788  0.734177   33.0
T1110      0.700000  0.777778  0.736842   18.0
T1112      0.645161  0.909091  0.754717   22.0
T1113      0.800000  0.571429  0.666667    7.0
T1140      0.867925  0.978723  0.920000   94.0
T1190      0.666667  0.500000  0.571429   12.0
T1204.002  0.750000  0.400000  0.521739   15.0
T1210      0.000000  0.000000  0.000000    5.0
T1218.011  0.909091  1.000000  0.952381   10.0
T1219      0.000000  0.000000  0.000000   12.0
T1484.001  0.000000  0.000000  0.000000    8.0
T1518.001  1.000000  0.400000  0.571429    5.0
T1543.003  0.666667  0.571429  0.615385    7.0
T1547.001  0.269231  0.538462  0.358974   13.0
T1548.002  1.000000  0.571429  0.727273    7.0
T1552.001  0.000000  0.000000  0.000000    6.0
T1562.001  0.695652  0.761905  0.727273   21.0
T1564.001  0.000000  0.000000  0.000000    4.0
T1566.001  1.000000  0.944444  0.971429   18.0
T1569.002  0.000000  0.000000  0.000000    2.0
T1570      0.437500  0.538462  0.482759   13.0
T1573.001  0.000000  0.000000  0.000000   12.0
T1574.002  0.777778  0.933333  0.848485   15.0
(micro)    0.691552  0.691552  0.691552    NaN
(macro)    0.558659  0.516452  0.509995    NaN
