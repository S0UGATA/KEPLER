transformer
[array(['T1003.001', 'T1005', 'T1012', 'T1016', 'T1021.001', 'T1027',
       'T1033', 'T1036.005', 'T1041', 'T1047', 'T1053.005', 'T1055',
       'T1056.001', 'T1057', 'T1059.003', 'T1068', 'T1070.004',
       'T1071.001', 'T1072', 'T1074.001', 'T1078', 'T1082', 'T1083',
       'T1090', 'T1095', 'T1105', 'T1106', 'T1110', 'T1112', 'T1113',
       'T1140', 'T1190', 'T1204.002', 'T1210', 'T1218.011', 'T1219',
       'T1484.001', 'T1518.001', 'T1543.003', 'T1547.001', 'T1548.002',
       'T1552.001', 'T1557.001', 'T1562.001', 'T1564.001', 'T1566.001',
       'T1569.002', 'T1570', 'T1573.001', 'T1574.002'], dtype=object)]
                                                text      label
0  This file extracts credentials from LSASS simi...  T1003.001
1  It calls OpenProcess on lsass.exe with access ...  T1003.001
2  It spreads to Microsoft Windows machines using...      T1210
3                   SMB exploitation via EternalBlue      T1210
4                 SMBv1 Exploitation via EternalBlue      T1210
5  has the capability to exploit SMBv1 via the we...      T1210
6                      SMB copy and remote execution      T1570
7  This thread is then used to execute the SMB co...      T1570
8                      SMB copy and remote execution      T1570
9                      SMB Copy and Remote Execution      T1570
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[ 1212, 44080,   515,  ..., 50256, 50256, 50256],
        [  464, 11532, 32161,  ..., 50256, 50256, 50256],
        [  464, 47758,  6797,  ..., 50256, 50256, 50256],
        ...,
        [ 1169,   875,  9043,  ..., 50256, 50256, 50256],
        [ 1484,   329,   875,  ..., 50256, 50256, 50256],
        [ 1640,  1429, 20596,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4071.)
                  P         R        F1      #
T1003.001  0.954545  1.000000  0.976744   21.0
T1005      0.611111  0.687500  0.647059   16.0
T1012      0.000000  0.000000  0.000000    5.0
T1016      0.857143  0.545455  0.666667   11.0
T1021.001  0.722222  0.962963  0.825397   27.0
T1027      0.924242  0.841379  0.880866  145.0
T1033      1.000000  0.272727  0.428571   11.0
T1036.005  0.875000  0.636364  0.736842   11.0
T1041      0.857143  0.500000  0.631579   12.0
T1047      0.785714  0.916667  0.846154   12.0
T1053.005  1.000000  1.000000  1.000000   23.0
T1055      0.951613  0.967213  0.959350   61.0
T1056.001  0.636364  0.875000  0.736842    8.0
T1057      0.928571  0.866667  0.896552   15.0
T1059.003  0.787500  1.000000  0.881119   63.0
T1068      0.000000  0.000000  0.000000    3.0
T1070.004  1.000000  0.631579  0.774194   19.0
T1071.001  0.783784  0.878788  0.828571   33.0
T1072      0.000000  0.000000  0.000000    3.0
T1074.001  0.000000  0.000000  0.000000    6.0
T1078      0.777778  0.840000  0.807692   25.0
T1082      0.829268  0.850000  0.839506   40.0
T1083      0.466667  0.823529  0.595745   17.0
T1090      0.875000  0.840000  0.857143   25.0
T1095      1.000000  0.300000  0.461538   10.0
T1105      0.750000  0.982759  0.850746   58.0
T1106      0.700000  0.945946  0.804598   37.0
T1110      0.833333  0.909091  0.869565   11.0
T1112      0.551724  0.941176  0.695652   17.0
T1113      1.000000  0.923077  0.960000   13.0
T1140      0.924731  0.966292  0.945055   89.0
T1190      0.785714  0.916667  0.846154   12.0
T1204.002  0.875000  0.823529  0.848485   17.0
T1210      0.000000  0.000000  0.000000    4.0
T1218.011  1.000000  1.000000  1.000000    9.0
T1219      1.000000  0.230769  0.375000   13.0
T1484.001  0.000000  0.000000  0.000000    5.0
T1518.001  0.000000  0.000000  0.000000    3.0
T1543.003  1.000000  1.000000  1.000000    4.0
T1547.001  0.666667  0.714286  0.689655   14.0
T1548.002  1.000000  0.571429  0.727273    7.0
T1552.001  0.000000  0.000000  0.000000    5.0
T1562.001  0.818182  0.818182  0.818182   11.0
T1564.001  0.000000  0.000000  0.000000    3.0
T1566.001  1.000000  0.900000  0.947368   20.0
T1569.002  0.000000  0.000000  0.000000    3.0
T1570      0.631579  0.857143  0.727273   14.0
T1573.001  1.000000  0.444444  0.615385    9.0
T1574.002  1.000000  1.000000  1.000000   18.0
(micro)    0.829077  0.829077  0.829077    NaN
(macro)    0.676747  0.636339  0.632623    NaN
