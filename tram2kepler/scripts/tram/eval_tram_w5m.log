transformer
[array(['T1003.001', 'T1005', 'T1012', 'T1016', 'T1021.001', 'T1027',
       'T1033', 'T1036.005', 'T1041', 'T1047', 'T1053.005', 'T1055',
       'T1056.001', 'T1057', 'T1059.003', 'T1068', 'T1070.004',
       'T1071.001', 'T1072', 'T1074.001', 'T1078', 'T1082', 'T1083',
       'T1090', 'T1095', 'T1105', 'T1106', 'T1110', 'T1112', 'T1113',
       'T1140', 'T1190', 'T1204.002', 'T1210', 'T1218.011', 'T1219',
       'T1484.001', 'T1518.001', 'T1543.003', 'T1547.001', 'T1548.002',
       'T1552.001', 'T1557.001', 'T1562.001', 'T1564.001', 'T1566.001',
       'T1569.002', 'T1570', 'T1573.001', 'T1574.002'], dtype=object)]
                                                text      label
0  This file extracts credentials from LSASS simi...  T1003.001
1  It calls OpenProcess on lsass.exe with access ...  T1003.001
2  It spreads to Microsoft Windows machines using...      T1210
3                   SMB exploitation via EternalBlue      T1210
4                 SMBv1 Exploitation via EternalBlue      T1210
5  has the capability to exploit SMBv1 via the we...      T1210
6                      SMB copy and remote execution      T1570
7  This thread is then used to execute the SMB co...      T1570
8                      SMB copy and remote execution      T1570
9                      SMB Copy and Remote Execution      T1570
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
tensor([[  101, 14108,  6764,  ...,     0,     0,     0],
        [  101,  2009, 22139,  ...,     0,     0,     0],
        [  101,  2139, 16429,  ...,     0,     0,     0],
        ...,
        [  101, 16999,  2000,  ...,     0,     0,     0],
        [  101,  5678,  1010,  ...,     0,     0,     0],
        [  101,  2862,  1997,  ...,     0,     0,     0]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 1.],
        [1., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4071.)
                  P         R        F1      #
T1003.001  0.017682  1.000000  0.034749   18.0
T1005      0.000000  0.000000  0.000000    7.0
T1012      0.000000  0.000000  0.000000    7.0
T1016      0.000000  0.000000  0.000000    8.0
T1021.001  0.000000  0.000000  0.000000   21.0
T1027      0.000000  0.000000  0.000000  145.0
T1033      0.000000  0.000000  0.000000    5.0
T1036.005  0.000000  0.000000  0.000000   18.0
T1041      0.000000  0.000000  0.000000   18.0
T1047      0.000000  0.000000  0.000000   19.0
T1053.005  0.000000  0.000000  0.000000   27.0
T1055      0.000000  0.000000  0.000000   56.0
T1056.001  0.000000  0.000000  0.000000   17.0
T1057      0.000000  0.000000  0.000000   19.0
T1059.003  0.000000  0.000000  0.000000   62.0
T1068      0.000000  0.000000  0.000000    3.0
T1070.004  0.000000  0.000000  0.000000   18.0
T1071.001  0.000000  0.000000  0.000000   30.0
T1072      0.000000  0.000000  0.000000    1.0
T1074.001  0.000000  0.000000  0.000000    7.0
T1078      0.000000  0.000000  0.000000   25.0
T1082      0.000000  0.000000  0.000000   24.0
T1083      0.000000  0.000000  0.000000   20.0
T1090      0.000000  0.000000  0.000000   30.0
T1095      0.000000  0.000000  0.000000    9.0
T1105      0.000000  0.000000  0.000000   57.0
T1106      0.000000  0.000000  0.000000   36.0
T1110      0.000000  0.000000  0.000000   20.0
T1112      0.000000  0.000000  0.000000   21.0
T1113      0.000000  0.000000  0.000000   16.0
T1140      0.000000  0.000000  0.000000   91.0
T1190      0.000000  0.000000  0.000000    8.0
T1204.002  0.000000  0.000000  0.000000   14.0
T1210      0.000000  0.000000  0.000000    4.0
T1218.011  0.000000  0.000000  0.000000   11.0
T1219      0.000000  0.000000  0.000000   10.0
T1484.001  0.000000  0.000000  0.000000    4.0
T1518.001  0.000000  0.000000  0.000000    6.0
T1543.003  0.000000  0.000000  0.000000    8.0
T1547.001  0.000000  0.000000  0.000000    6.0
T1548.002  0.000000  0.000000  0.000000    6.0
T1552.001  0.000000  0.000000  0.000000    4.0
T1562.001  0.000000  0.000000  0.000000   18.0
T1564.001  0.000000  0.000000  0.000000    3.0
T1566.001  0.000000  0.000000  0.000000   16.0
T1569.002  0.000000  0.000000  0.000000    6.0
T1570      0.000000  0.000000  0.000000   14.0
T1573.001  0.000000  0.000000  0.000000    9.0
T1574.002  0.000000  0.000000  0.000000   16.0
(micro)    0.017682  0.017682  0.017682    NaN
(macro)    0.000361  0.020408  0.000709    NaN
