transformer
MultiLabelBinarizer(classes=['T1003.001', 'T1005', 'T1012', 'T1016',
                             'T1021.001', 'T1027', 'T1033', 'T1036.005',
                             'T1041', 'T1047', 'T1053.005', 'T1055',
                             'T1056.001', 'T1057', 'T1059.003', 'T1068',
                             'T1070.004', 'T1071.001', 'T1072', 'T1074.001',
                             'T1078', 'T1082', 'T1083', 'T1090', 'T1095',
                             'T1105', 'T1106', 'T1110', 'T1112', 'T1113', ...])
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[14214,   291,  1045,  ..., 50256, 50256, 50256],
        [   39,   425, 10544,  ..., 50256, 50256, 50256],
        [   32,  4947,   286,  ..., 50256, 50256, 50256],
        ...,
        [  464, 21437,   318,  ..., 50256, 50256, 50256],
        [ 2437,   284, 31572,  ..., 50256, 50256, 50256],
        [  464, 49134, 28883,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4112.)
           predicted                 actual
0                 ()                     ()
1                 ()            (T1021.001)
2                 ()                     ()
3                 ()                     ()
4                 ()                     ()
...              ...                    ...
3831              ()            (T1204.002)
3832              ()                     ()
3833         (T1140)                (T1140)
3834  (T1140, T1027)  (T1140, T1027, T1106)
3835              ()                     ()

[3836 rows x 2 columns]
                  P         R        F1      #
T1140      0.819277  0.764045  0.790698   89.0
T1027      0.551948  0.559211  0.555556  152.0
T1059.003  0.870968  0.380282  0.529412   71.0
T1055      1.000000  0.038462  0.074074   52.0
T1570      0.000000  0.000000  0.000000   11.0
T1095      0.000000  0.000000  0.000000   11.0
T1041      0.000000  0.000000  0.000000   10.0
T1113      0.000000  0.000000  0.000000    9.0
T1057      0.000000  0.000000  0.000000    9.0
T1016      0.000000  0.000000  0.000000    9.0
T1110      0.000000  0.000000  0.000000    8.0
T1033      0.000000  0.000000  0.000000    7.0
T1518.001  0.000000  0.000000  0.000000    7.0
T1548.002  0.000000  0.000000  0.000000    7.0
T1190      0.000000  0.000000  0.000000    6.0
T1036.005  0.000000  0.000000  0.000000   11.0
T1484.001  0.000000  0.000000  0.000000    6.0
T1056.001  0.000000  0.000000  0.000000    5.0
T1074.001  0.000000  0.000000  0.000000    5.0
T1012      0.000000  0.000000  0.000000    5.0
T1210      0.000000  0.000000  0.000000    5.0
T1569.002  0.000000  0.000000  0.000000    4.0
T1564.001  0.000000  0.000000  0.000000    4.0
T1068      0.000000  0.000000  0.000000    3.0
T1557.001  0.000000  0.000000  0.000000    3.0
T1552.001  0.000000  0.000000  0.000000    6.0
T1005      0.000000  0.000000  0.000000   12.0
T1574.002  0.000000  0.000000  0.000000   12.0
T1090      0.000000  0.000000  0.000000   22.0
T1105      0.000000  0.000000  0.000000   48.0
T1106      0.000000  0.000000  0.000000   41.0
T1078      0.000000  0.000000  0.000000   34.0
T1204.002  0.000000  0.000000  0.000000   28.0
T1071.001  0.000000  0.000000  0.000000   26.0
T1082      0.000000  0.000000  0.000000   26.0
T1053.005  0.000000  0.000000  0.000000   25.0
T1003.001  0.000000  0.000000  0.000000   25.0
T1562.001  0.000000  0.000000  0.000000   23.0
T1083      0.000000  0.000000  0.000000   21.0
T1219      0.000000  0.000000  0.000000   12.0
T1070.004  0.000000  0.000000  0.000000   21.0
T1047      0.000000  0.000000  0.000000   21.0
T1021.001  0.000000  0.000000  0.000000   20.0
T1112      0.000000  0.000000  0.000000   20.0
T1547.001  0.000000  0.000000  0.000000   20.0
T1218.011  0.000000  0.000000  0.000000   15.0
T1566.001  0.000000  0.000000  0.000000   15.0
T1543.003  0.000000  0.000000  0.000000   14.0
T1573.001  0.000000  0.000000  0.000000   13.0
T1072      0.000000  0.000000  0.000000    2.0
(macro)    0.064844  0.034840  0.038995    NaN
(micro)    0.674074  0.176528  0.279785    NaN
