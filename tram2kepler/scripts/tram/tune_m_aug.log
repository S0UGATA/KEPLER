transformer
MultiLabelBinarizer(classes=['T1003.001', 'T1005', 'T1012', 'T1016',
                             'T1021.001', 'T1027', 'T1033', 'T1036.005',
                             'T1041', 'T1047', 'T1053.005', 'T1055',
                             'T1056.001', 'T1057', 'T1059.003', 'T1068',
                             'T1070.004', 'T1071.001', 'T1072', 'T1074.001',
                             'T1078', 'T1082', 'T1083', 'T1090', 'T1095',
                             'T1105', 'T1106', 'T1110', 'T1112', 'T1113', ...])
                                            sentence   labels
0  title: NotPetya Technical Analysis – A Triple ...       []
1  Executive Summary This technical analysis prov...       []
2  For more information on CrowdStrike’s proactiv...       []
3  NotPetya combines ransomware with the ability ...       []
4  It spreads to Microsoft Windows machines using...  [T1210]
5  This is the same vulnerability Microsoft repor...       []
6  Once a machine is infected by NotPetya, a seri...  [T1570]
7                             Do NOT pay the ransom.       []
8  No files will be recovered if the ransom is pa...       []
9                                     Dropped Files        []
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[ 3791,  7220,    13,  ..., 50256, 50256, 50256],
        [22229,   284,   257,  ..., 50256, 50256, 50256],
        [ 1135,   561,  1842,  ..., 50256, 50256, 50256],
        ...,
        [    4, 28929,  4663,  ..., 50256, 50256, 50256],
        [22229,   284,   663,  ..., 50256, 50256, 50256],
        [ 7089,  1140,   268,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4106.)
epoch:1|loss:0.038983889323953504|val_loss:0.02918085466080811
epoch:2|loss:0.026025033052343306|val_loss:0.02603746707973187
     predicted       actual
0           ()           ()
1           ()           ()
2           ()  (T1218.011)
3           ()           ()
4           ()           ()
...        ...          ...
3831        ()           ()
3832        ()           ()
3833        ()           ()
3834        ()           ()
3835        ()           ()

[3836 rows x 2 columns]
                  P         R        F1      #
T1140      0.700000  0.656250  0.677419   96.0
T1027      0.397059  0.415385  0.406015  130.0
T1059.003  0.652174  0.283019  0.394737   53.0
T1055      1.000000  0.015152  0.029851   66.0
T1219      0.000000  0.000000  0.000000   14.0
T1190      0.000000  0.000000  0.000000   12.0
T1056.001  0.000000  0.000000  0.000000   12.0
T1033      0.000000  0.000000  0.000000   12.0
T1016      0.000000  0.000000  0.000000   12.0
T1095      0.000000  0.000000  0.000000   10.0
T1518.001  0.000000  0.000000  0.000000   10.0
T1110      0.000000  0.000000  0.000000   10.0
T1005      0.000000  0.000000  0.000000    9.0
T1113      0.000000  0.000000  0.000000    8.0
T1548.002  0.000000  0.000000  0.000000    7.0
T1083      0.000000  0.000000  0.000000   15.0
T1569.002  0.000000  0.000000  0.000000    6.0
T1074.001  0.000000  0.000000  0.000000    3.0
T1564.001  0.000000  0.000000  0.000000    3.0
T1012      0.000000  0.000000  0.000000    3.0
T1068      0.000000  0.000000  0.000000    3.0
T1552.001  0.000000  0.000000  0.000000    3.0
T1557.001  0.000000  0.000000  0.000000    2.0
T1072      0.000000  0.000000  0.000000    2.0
T1210      0.000000  0.000000  0.000000    2.0
T1057      0.000000  0.000000  0.000000    7.0
T1547.001  0.000000  0.000000  0.000000   15.0
T1570      0.000000  0.000000  0.000000   15.0
T1053.005  0.000000  0.000000  0.000000   23.0
T1105      0.000000  0.000000  0.000000   41.0
T1106      0.000000  0.000000  0.000000   40.0
T1071.001  0.000000  0.000000  0.000000   35.0
T1082      0.000000  0.000000  0.000000   33.0
T1078      0.000000  0.000000  0.000000   31.0
T1090      0.000000  0.000000  0.000000   27.0
T1562.001  0.000000  0.000000  0.000000   24.0
T1204.002  0.000000  0.000000  0.000000   23.0
T1070.004  0.000000  0.000000  0.000000   23.0
T1041      0.000000  0.000000  0.000000   21.0
T1218.011  0.000000  0.000000  0.000000   16.0
T1112      0.000000  0.000000  0.000000   20.0
T1574.002  0.000000  0.000000  0.000000   19.0
T1003.001  0.000000  0.000000  0.000000   19.0
T1573.001  0.000000  0.000000  0.000000   18.0
T1543.003  0.000000  0.000000  0.000000   17.0
T1047      0.000000  0.000000  0.000000   17.0
T1036.005  0.000000  0.000000  0.000000   17.0
T1566.001  0.000000  0.000000  0.000000   16.0
T1021.001  0.000000  0.000000  0.000000   16.0
T1484.001  0.000000  0.000000  0.000000    1.0
(macro)    0.054985  0.027396  0.030160    NaN
(micro)    0.532000  0.128255  0.206682    NaN
