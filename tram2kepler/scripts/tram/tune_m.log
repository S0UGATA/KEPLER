transformer
MultiLabelBinarizer(classes=['T1003.001', 'T1005', 'T1012', 'T1016',
                             'T1021.001', 'T1027', 'T1033', 'T1036.005',
                             'T1041', 'T1047', 'T1053.005', 'T1055',
                             'T1056.001', 'T1057', 'T1059.003', 'T1068',
                             'T1070.004', 'T1071.001', 'T1072', 'T1074.001',
                             'T1078', 'T1082', 'T1083', 'T1090', 'T1095',
                             'T1105', 'T1106', 'T1110', 'T1112', 'T1113', ...])
                                            sentence   labels
0  title: NotPetya Technical Analysis – A Triple ...       []
1  Executive Summary This technical analysis prov...       []
2  For more information on CrowdStrike’s proactiv...       []
3  NotPetya combines ransomware with the ability ...       []
4  It spreads to Microsoft Windows machines using...  [T1210]
5  This is the same vulnerability Microsoft repor...       []
6  Once a machine is infected by NotPetya, a seri...  [T1570]
7                             Do NOT pay the ransom.       []
8  No files will be recovered if the ransom is pa...       []
9                                     Dropped Files        []
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[ 7191,   428,  1429,  ..., 50256, 50256, 50256],
        [ 1135,   423,  4367,  ..., 50256, 50256, 50256],
        [ 3620,    53, 10206,  ..., 50256, 50256, 50256],
        ...,
        [18257,   641,  2176,  ..., 50256, 50256, 50256],
        [11964,  6188, 23455,  ..., 50256, 50256, 50256],
        [10943,  5446,  3535,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4144.)
epoch:1|loss:0.03853193225053894|val_loss:0.02745731690932492
epoch:2|loss:0.02597301536367096|val_loss:0.024919640974379337
epoch:3|loss:0.02163387055027963|val_loss:0.022705897741010023
     predicted       actual
0           ()           ()
1           ()           ()
2           ()      (T1047)
3           ()           ()
4           ()           ()
...        ...          ...
3831        ()           ()
3832        ()           ()
3833        ()           ()
3834        ()           ()
3835        ()  (T1566.001)

[3836 rows x 2 columns]
                  P         R        F1      #
T1140      0.821918  0.631579  0.714286   95.0
T1055      0.577778  0.530612  0.553191   49.0
T1059.003  0.772727  0.269841  0.400000   63.0
T1027      0.446154  0.224806  0.298969  129.0
T1003.001  0.750000  0.111111  0.193548   27.0
T1105      0.500000  0.044444  0.081633   45.0
T1570      0.000000  0.000000  0.000000    7.0
T1573.001  0.000000  0.000000  0.000000   11.0
T1036.005  0.000000  0.000000  0.000000   11.0
T1016      0.000000  0.000000  0.000000   10.0
T1219      0.000000  0.000000  0.000000   10.0
T1033      0.000000  0.000000  0.000000    9.0
T1548.002  0.000000  0.000000  0.000000    9.0
T1547.001  0.000000  0.000000  0.000000    8.0
T1566.001  0.000000  0.000000  0.000000    8.0
T1569.002  0.000000  0.000000  0.000000    5.0
T1074.001  0.000000  0.000000  0.000000    6.0
T1552.001  0.000000  0.000000  0.000000    5.0
T1218.011  0.000000  0.000000  0.000000   11.0
T1110      0.000000  0.000000  0.000000    4.0
T1012      0.000000  0.000000  0.000000    4.0
T1210      0.000000  0.000000  0.000000    3.0
T1068      0.000000  0.000000  0.000000    3.0
T1484.001  0.000000  0.000000  0.000000    3.0
T1564.001  0.000000  0.000000  0.000000    3.0
T1557.001  0.000000  0.000000  0.000000    2.0
T1095      0.000000  0.000000  0.000000   11.0
T1113      0.000000  0.000000  0.000000   13.0
T1190      0.000000  0.000000  0.000000   12.0
T1562.001  0.000000  0.000000  0.000000   21.0
T1106      0.000000  0.000000  0.000000   39.0
T1047      0.000000  0.000000  0.000000   17.0
T1078      0.000000  0.000000  0.000000   42.0
T1071.001  0.000000  0.000000  0.000000   34.0
T1082      0.000000  0.000000  0.000000   29.0
T1090      0.000000  0.000000  0.000000   27.0
T1053.005  0.000000  0.000000  0.000000   21.0
T1574.002  0.000000  0.000000  0.000000   21.0
T1070.004  0.000000  0.000000  0.000000   20.0
T1543.003  0.000000  0.000000  0.000000   12.0
T1112      0.000000  0.000000  0.000000   20.0
T1021.001  0.000000  0.000000  0.000000   20.0
T1083      0.000000  0.000000  0.000000   20.0
T1204.002  0.000000  0.000000  0.000000   18.0
T1005      0.000000  0.000000  0.000000   16.0
T1057      0.000000  0.000000  0.000000   16.0
T1041      0.000000  0.000000  0.000000   15.0
T1056.001  0.000000  0.000000  0.000000   14.0
T1072      0.000000  0.000000  0.000000    1.0
(macro)    0.078951  0.036988  0.045747    NaN
(micro)    0.619910  0.137137  0.224590    NaN
