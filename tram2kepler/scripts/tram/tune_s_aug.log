transformer
[array(['T1003.001', 'T1005', 'T1012', 'T1016', 'T1021.001', 'T1027',
       'T1033', 'T1036.005', 'T1041', 'T1047', 'T1053.005', 'T1055',
       'T1056.001', 'T1057', 'T1059.003', 'T1068', 'T1070.004',
       'T1071.001', 'T1072', 'T1074.001', 'T1078', 'T1082', 'T1083',
       'T1090', 'T1095', 'T1105', 'T1106', 'T1110', 'T1112', 'T1113',
       'T1140', 'T1190', 'T1204.002', 'T1210', 'T1218.011', 'T1219',
       'T1484.001', 'T1518.001', 'T1543.003', 'T1547.001', 'T1548.002',
       'T1552.001', 'T1557.001', 'T1562.001', 'T1564.001', 'T1566.001',
       'T1569.002', 'T1570', 'T1573.001', 'T1574.002'], dtype=object)]
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[ 1212, 44080,   515,  ..., 50256, 50256, 50256],
        [  464, 11532, 32161,  ..., 50256, 50256, 50256],
        [  464, 47758,  6797,  ..., 50256, 50256, 50256],
        ...,
        [ 1169,   875,  9043,  ..., 50256, 50256, 50256],
        [ 1484,   329,   875,  ..., 50256, 50256, 50256],
        [ 1640,  1429, 20596,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4071.)
                  P         R        F1      #
T1003.001  1.000000  0.952381  0.975610   21.0
T1005      1.000000  0.312500  0.476190   16.0
T1012      0.000000  0.000000  0.000000    5.0
T1016      1.000000  0.636364  0.777778   11.0
T1021.001  1.000000  0.925926  0.961538   27.0
T1027      0.871622  0.889655  0.880546  145.0
T1033      0.727273  0.727273  0.727273   11.0
T1036.005  1.000000  0.272727  0.428571   11.0
T1041      0.571429  1.000000  0.727273   12.0
T1047      0.900000  0.750000  0.818182   12.0
T1053.005  0.785714  0.956522  0.862745   23.0
T1055      0.850746  0.934426  0.890625   61.0
T1056.001  0.888889  1.000000  0.941176    8.0
T1057      0.928571  0.866667  0.896552   15.0
T1059.003  0.900000  1.000000  0.947368   63.0
T1068      0.000000  0.000000  0.000000    3.0
T1070.004  0.894737  0.894737  0.894737   19.0
T1071.001  0.968750  0.939394  0.953846   33.0
T1072      0.000000  0.000000  0.000000    3.0
T1074.001  0.500000  0.166667  0.250000    6.0
T1078      0.880000  0.880000  0.880000   25.0
T1082      0.829268  0.850000  0.839506   40.0
T1083      0.562500  0.529412  0.545455   17.0
T1090      0.888889  0.960000  0.923077   25.0
T1095      1.000000  0.800000  0.888889   10.0
T1105      0.915254  0.931034  0.923077   58.0
T1106      0.739130  0.918919  0.819277   37.0
T1110      0.533333  0.727273  0.615385   11.0
T1112      0.809524  1.000000  0.894737   17.0
T1113      0.722222  1.000000  0.838710   13.0
T1140      0.955056  0.955056  0.955056   89.0
T1190      0.714286  0.833333  0.769231   12.0
T1204.002  0.592593  0.941176  0.727273   17.0
T1210      0.000000  0.000000  0.000000    4.0
T1218.011  0.750000  1.000000  0.857143    9.0
T1219      1.000000  0.692308  0.818182   13.0
T1484.001  1.000000  1.000000  1.000000    5.0
T1518.001  0.000000  0.000000  0.000000    3.0
T1543.003  0.666667  0.500000  0.571429    4.0
T1547.001  0.923077  0.857143  0.888889   14.0
T1548.002  1.000000  0.857143  0.923077    7.0
T1552.001  0.000000  0.000000  0.000000    5.0
T1562.001  0.666667  0.909091  0.769231   11.0
T1564.001  0.000000  0.000000  0.000000    3.0
T1566.001  0.708333  0.850000  0.772727   20.0
T1569.002  0.000000  0.000000  0.000000    3.0
T1570      0.900000  0.642857  0.750000   14.0
T1573.001  1.000000  0.555556  0.714286    9.0
T1574.002  0.947368  1.000000  0.972973   18.0
(micro)    0.850688  0.850688  0.850688    NaN
(macro)    0.703916  0.681950  0.674849    NaN
