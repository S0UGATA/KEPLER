transformer
[array(['T1003.001', 'T1005', 'T1012', 'T1016', 'T1021.001', 'T1027',
       'T1033', 'T1036.005', 'T1041', 'T1047', 'T1053.005', 'T1055',
       'T1056.001', 'T1057', 'T1059.003', 'T1068', 'T1070.004',
       'T1071.001', 'T1072', 'T1074.001', 'T1078', 'T1082', 'T1083',
       'T1090', 'T1095', 'T1105', 'T1106', 'T1110', 'T1112', 'T1113',
       'T1140', 'T1190', 'T1204.002', 'T1210', 'T1218.011', 'T1219',
       'T1484.001', 'T1518.001', 'T1543.003', 'T1547.001', 'T1548.002',
       'T1552.001', 'T1557.001', 'T1562.001', 'T1564.001', 'T1566.001',
       'T1569.002', 'T1570', 'T1573.001', 'T1574.002'], dtype=object)]
                                                text      label
0  This file extracts credentials from LSASS simi...  T1003.001
1  It calls OpenProcess on lsass.exe with access ...  T1003.001
2  It spreads to Microsoft Windows machines using...      T1210
3                   SMB exploitation via EternalBlue      T1210
4                 SMBv1 Exploitation via EternalBlue      T1210
5  has the capability to exploit SMBv1 via the we...      T1210
6                      SMB copy and remote execution      T1570
7  This thread is then used to execute the SMB co...      T1570
8                      SMB copy and remote execution      T1570
9                      SMB Copy and Remote Execution      T1570
GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50265, 768)
    (wpe): Embedding(514, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=50, bias=False)
)
tensor([[ 4863,   257, 30802,  ..., 50256, 50256, 50256],
        [ 3500,   262,  5499,  ..., 50256, 50256, 50256],
        [ 2664,   262,  1592,  ..., 50256, 50256, 50256],
        ...,
        [28758,  1220,    34,  ..., 50256, 50256, 50256],
        [12685,  9043, 21437,  ..., 50256, 50256, 50256],
        [ 1169, 14553, 21437,  ..., 50256, 50256, 50256]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4071.)
epoch:1|loss:0.10554060272361133|val_loss:0.07707784496539948
epoch:2|loss:0.06081532888963599|val_loss:0.05274525093024268
epoch:3|loss:0.03573552530724555|val_loss:0.04223399872288985
                  P         R        F1      #
T1003.001  0.900000  0.666667  0.765957   27.0
T1005      1.000000  0.142857  0.250000   21.0
T1012      0.000000  0.000000  0.000000    6.0
T1016      1.000000  0.600000  0.750000   10.0
T1021.001  0.842105  0.800000  0.820513   20.0
T1027      0.714286  0.744681  0.729167  141.0
T1033      0.600000  0.272727  0.375000   11.0
T1036.005  0.666667  0.083333  0.148148   24.0
T1041      0.370370  0.769231  0.500000   13.0
T1047      0.777778  0.466667  0.583333   15.0
T1053.005  0.459459  0.850000  0.596491   20.0
T1055      0.629630  0.894737  0.739130   57.0
T1056.001  0.875000  0.875000  0.875000   16.0
T1057      0.666667  0.705882  0.685714   17.0
T1059.003  0.687500  0.916667  0.785714   60.0
T1068      0.000000  0.000000  0.000000    2.0
T1070.004  0.384615  0.454545  0.416667   11.0
T1071.001  0.851852  0.821429  0.836364   28.0
T1072      0.000000  0.000000  0.000000    1.0
T1074.001  0.000000  0.000000  0.000000    3.0
T1078      0.738095  0.775000  0.756098   40.0
T1082      0.655172  0.593750  0.622951   32.0
T1083      0.461538  0.444444  0.452830   27.0
T1090      0.772727  0.680000  0.723404   25.0
T1095      0.666667  0.181818  0.285714   11.0
T1105      0.673469  0.673469  0.673469   49.0
T1106      0.557692  0.783784  0.651685   37.0
T1110      0.285714  0.333333  0.307692   12.0
T1112      0.727273  0.615385  0.666667   13.0
T1113      0.181818  0.800000  0.296296    5.0
T1140      0.877778  0.940476  0.908046   84.0
T1190      0.500000  0.666667  0.571429    9.0
T1204.002  0.347826  0.470588  0.400000   17.0
T1210      0.000000  0.000000  0.000000    4.0
T1218.011  0.823529  0.875000  0.848485   16.0
T1219      0.800000  0.333333  0.470588   12.0
T1484.001  1.000000  0.200000  0.333333    5.0
T1518.001  0.000000  0.000000  0.000000    7.0
T1543.003  0.833333  0.555556  0.666667    9.0
T1547.001  0.769231  0.666667  0.714286   15.0
T1548.002  1.000000  0.250000  0.400000    4.0
T1552.001  0.000000  0.000000  0.000000    5.0
T1557.001  0.000000  0.000000  0.000000    2.0
T1562.001  0.764706  0.866667  0.812500   15.0
T1564.001  0.000000  0.000000  0.000000    5.0
T1566.001  0.458333  0.733333  0.564103   15.0
T1569.002  0.000000  0.000000  0.000000    5.0
T1570      0.428571  0.230769  0.300000   13.0
T1573.001  1.000000  0.125000  0.222222    8.0
T1574.002  0.736842  1.000000  0.848485   14.0
(micro)    0.664047  0.664047  0.664047    NaN
(macro)    0.549725  0.477189  0.467083    NaN
