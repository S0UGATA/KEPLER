transformer
MultiLabelBinarizer(classes=['T1003.001', 'T1005', 'T1012', 'T1016',
                             'T1021.001', 'T1027', 'T1033', 'T1036.005',
                             'T1041', 'T1047', 'T1053.005', 'T1055',
                             'T1056.001', 'T1057', 'T1059.003', 'T1068',
                             'T1070.004', 'T1071.001', 'T1072', 'T1074.001',
                             'T1078', 'T1082', 'T1083', 'T1090', 'T1095',
                             'T1105', 'T1106', 'T1110', 'T1112', 'T1113', ...])
                                            sentence   labels
0  title: NotPetya Technical Analysis – A Triple ...       []
1  Executive Summary This technical analysis prov...       []
2  For more information on CrowdStrike’s proactiv...       []
3  NotPetya combines ransomware with the ability ...       []
4  It spreads to Microsoft Windows machines using...  [T1210]
5  This is the same vulnerability Microsoft repor...       []
6  Once a machine is infected by NotPetya, a seri...  [T1570]
7                             Do NOT pay the ransom.       []
8  No files will be recovered if the ransom is pa...       []
9                                     Dropped Files        []
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(31090, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=50, bias=True)
)
tensor([[  102,  6123,  1098,  ...,     0,     0,     0],
        [  102,   603,   130,  ...,     0,     0,     0],
        [  102,   860,   370,  ...,     0,     0,     0],
        ...,
        [  102, 12106, 30137,  ...,     0,     0,     0],
        [  102,   781,   111,  ...,     0,     0,     0],
        [  102,  2869,   911,  ...,     0,     0,     0]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4089.)
     predicted                     actual
0           ()                         ()
1           ()                         ()
2           ()                         ()
3           ()                         ()
4      (T1027)  (T1027, T1036.005, T1140)
...        ...                        ...
3831        ()                         ()
3832        ()                         ()
3833        ()                         ()
3834        ()                         ()
3835        ()                         ()

[3836 rows x 2 columns]
                  P         R        F1      #
T1021.001  0.892857  0.925926  0.909091   27.0
T1574.002  1.000000  0.812500  0.896552   16.0
T1056.001  1.000000  0.727273  0.842105   11.0
T1047      1.000000  0.714286  0.833333   21.0
T1140      0.835052  0.801980  0.818182  101.0
T1059.003  0.890625  0.750000  0.814286   76.0
T1562.001  1.000000  0.650000  0.787879   20.0
T1112      0.846154  0.733333  0.785714   15.0
T1053.005  0.923077  0.631579  0.750000   19.0
T1055      0.804348  0.672727  0.732673   55.0
T1027      0.809160  0.646341  0.718644  164.0
T1090      0.863636  0.612903  0.716981   31.0
T1105      0.666667  0.740741  0.701754   54.0
T1566.001  1.000000  0.538462  0.700000   13.0
T1003.001  0.823529  0.583333  0.682927   24.0
T1106      0.769231  0.476190  0.588235   42.0
T1071.001  0.722222  0.481481  0.577778   27.0
T1204.002  1.000000  0.391304  0.562500   23.0
T1570      0.833333  0.416667  0.555556   12.0
T1041      0.545455  0.500000  0.521739   12.0
T1070.004  0.857143  0.352941  0.500000   17.0
T1057      1.000000  0.307692  0.470588   13.0
T1082      1.000000  0.304348  0.466667   23.0
T1218.011  1.000000  0.272727  0.428571   11.0
T1083      0.800000  0.250000  0.380952   16.0
T1548.002  1.000000  0.166667  0.285714    6.0
T1078      0.625000  0.178571  0.277778   28.0
T1547.001  1.000000  0.071429  0.133333   14.0
T1484.001  0.000000  0.000000  0.000000    7.0
T1569.002  0.000000  0.000000  0.000000    2.0
T1074.001  0.000000  0.000000  0.000000    3.0
T1210      0.000000  0.000000  0.000000    3.0
T1557.001  0.000000  0.000000  0.000000    3.0
T1564.001  0.000000  0.000000  0.000000    4.0
T1012      0.000000  0.000000  0.000000    4.0
T1113      0.000000  0.000000  0.000000    6.0
T1518.001  0.000000  0.000000  0.000000    7.0
T1110      0.000000  0.000000  0.000000    9.0
T1543.003  0.000000  0.000000  0.000000    7.0
T1033      0.000000  0.000000  0.000000   10.0
T1190      0.000000  0.000000  0.000000   11.0
T1016      0.000000  0.000000  0.000000   11.0
T1005      0.000000  0.000000  0.000000   13.0
T1219      0.000000  0.000000  0.000000   14.0
T1573.001  0.000000  0.000000  0.000000   15.0
T1095      0.000000  0.000000  0.000000   16.0
T1036.005  0.000000  0.000000  0.000000   17.0
T1072      0.000000  0.000000  0.000000    1.0
(macro)    0.510573  0.306488  0.363324    NaN
(micro)    0.830015  0.514231  0.635032    NaN
