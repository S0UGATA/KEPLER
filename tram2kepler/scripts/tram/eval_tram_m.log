transformer
MultiLabelBinarizer(classes=['T1003.001', 'T1005', 'T1012', 'T1016',
                             'T1021.001', 'T1027', 'T1033', 'T1036.005',
                             'T1041', 'T1047', 'T1053.005', 'T1055',
                             'T1056.001', 'T1057', 'T1059.003', 'T1068',
                             'T1070.004', 'T1071.001', 'T1072', 'T1074.001',
                             'T1078', 'T1082', 'T1083', 'T1090', 'T1095',
                             'T1105', 'T1106', 'T1110', 'T1112', 'T1113', ...])
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(31090, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=50, bias=True)
)
tensor([[  102,   557,  3961,  ...,     0,     0,     0],
        [  102,  3690, 30107,  ...,     0,     0,     0],
        [  102,   106,  3445,  ...,     0,     0,     0],
        ...,
        [  102,   111, 19355,  ...,     0,     0,     0],
        [  102,   539,   147,  ...,     0,     0,     0],
        [  102,   111, 27627,  ...,     0,     0,     0]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4112.)
        predicted                 actual
0              ()                     ()
1     (T1021.001)            (T1021.001)
2              ()                     ()
3              ()                     ()
4              ()                     ()
...           ...                    ...
3831  (T1204.002)            (T1204.002)
3832           ()                     ()
3833      (T1140)                (T1140)
3834      (T1027)  (T1027, T1106, T1140)
3835           ()                     ()

[3836 rows x 2 columns]
                  P         R        F1      #
T1574.002  1.000000  0.916667  0.956522   12.0
T1021.001  0.944444  0.850000  0.894737   20.0
T1140      0.820225  0.820225  0.820225   89.0
T1047      1.000000  0.666667  0.800000   21.0
T1059.003  0.890909  0.690141  0.777778   71.0
T1105      0.755102  0.770833  0.762887   48.0
T1562.001  1.000000  0.608696  0.756757   23.0
T1056.001  1.000000  0.600000  0.750000    5.0
T1090      0.875000  0.636364  0.736842   22.0
T1027      0.803279  0.644737  0.715328  152.0
T1055      0.861111  0.596154  0.704545   52.0
T1053.005  0.875000  0.560000  0.682927   25.0
T1003.001  0.875000  0.560000  0.682927   25.0
T1070.004  0.916667  0.523810  0.666667   21.0
T1071.001  0.789474  0.576923  0.666667   26.0
T1106      0.666667  0.585366  0.623377   41.0
T1218.011  1.000000  0.400000  0.571429   15.0
T1204.002  1.000000  0.357143  0.526316   28.0
T1566.001  0.800000  0.266667  0.400000   15.0
T1112      0.545455  0.300000  0.387097   20.0
T1083      1.000000  0.238095  0.384615   21.0
T1041      0.363636  0.400000  0.380952   10.0
T1078      0.888889  0.235294  0.372093   34.0
T1082      0.800000  0.153846  0.258065   26.0
T1548.002  1.000000  0.142857  0.250000    7.0
T1113      1.000000  0.111111  0.200000    9.0
T1570      0.500000  0.090909  0.153846   11.0
T1033      0.000000  0.000000  0.000000    7.0
T1557.001  0.000000  0.000000  0.000000    3.0
T1068      0.000000  0.000000  0.000000    3.0
T1564.001  0.000000  0.000000  0.000000    4.0
T1569.002  0.000000  0.000000  0.000000    4.0
T1210      0.000000  0.000000  0.000000    5.0
T1074.001  0.000000  0.000000  0.000000    5.0
T1012      0.000000  0.000000  0.000000    5.0
T1552.001  0.000000  0.000000  0.000000    6.0
T1190      0.000000  0.000000  0.000000    6.0
T1484.001  0.000000  0.000000  0.000000    6.0
T1518.001  0.000000  0.000000  0.000000    7.0
T1110      0.000000  0.000000  0.000000    8.0
T1057      0.000000  0.000000  0.000000    9.0
T1095      0.000000  0.000000  0.000000   11.0
T1036.005  0.000000  0.000000  0.000000   11.0
T1219      0.000000  0.000000  0.000000   12.0
T1005      0.000000  0.000000  0.000000   12.0
T1573.001  0.000000  0.000000  0.000000   13.0
T1543.003  0.000000  0.000000  0.000000   14.0
T1547.001  0.000000  0.000000  0.000000   20.0
T1016      0.000000  0.000000  0.000000    9.0
T1072      0.000000  0.000000  0.000000    2.0
(macro)    0.459417  0.266050  0.317652    NaN
(micro)    0.824621  0.474297  0.602217    NaN
