transformer
MultiLabelBinarizer(classes=['T1003.001', 'T1005', 'T1012', 'T1016',
                             'T1021.001', 'T1027', 'T1033', 'T1036.005',
                             'T1041', 'T1047', 'T1053.005', 'T1055',
                             'T1056.001', 'T1057', 'T1059.003', 'T1068',
                             'T1070.004', 'T1071.001', 'T1072', 'T1074.001',
                             'T1078', 'T1082', 'T1083', 'T1090', 'T1095',
                             'T1105', 'T1106', 'T1110', 'T1112', 'T1113', ...])
BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(1124, 32, padding_idx=0)
      (position_embeddings): Embedding(512, 32)
      (token_type_embeddings): Embedding(16, 32)
      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-4): 5 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=32, out_features=32, bias=True)
              (key): Linear(in_features=32, out_features=32, bias=True)
              (value): Linear(in_features=32, out_features=32, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=32, out_features=32, bias=True)
              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=32, out_features=37, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=37, out_features=32, bias=True)
            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=32, out_features=32, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=32, out_features=50, bias=True)
)
tensor([[  2,  61, 805,  ...,   0,   0,   0],
        [  2,  50, 790,  ...,   0,   0,   0],
        [  2,  43,  45,  ...,   0,   0,   0],
        ...,
        [  2,  62, 803,  ...,   0,   0,   0],
        [  2,  50, 792,  ...,   0,   0,   0],
        [  2,  62, 803,  ...,   0,   0,   0]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor(4112.)
                                              predicted                 actual
0     (T1012, T1005, T1113, T1078, T1047, T1552.001,...                     ()
1     (T1012, T1005, T1113, T1218.011, T1078, T1047,...            (T1021.001)
2     (T1012, T1005, T1113, T1218.011, T1078, T1047,...                     ()
3     (T1012, T1005, T1113, T1218.011, T1078, T1047,...                     ()
4     (T1012, T1005, T1113, T1078, T1047, T1552.001,...                     ()
...                                                 ...                    ...
3831  (T1012, T1005, T1113, T1218.011, T1078, T1047,...            (T1204.002)
3832  (T1012, T1005, T1113, T1218.011, T1078, T1047,...                     ()
3833  (T1012, T1005, T1113, T1218.011, T1078, T1047,...                (T1140)
3834  (T1012, T1005, T1113, T1218.011, T1078, T1047,...  (T1140, T1027, T1106)
3835  (T1012, T1005, T1113, T1218.011, T1078, T1047,...                     ()

[3836 rows x 2 columns]
                  P       R        F1      #
T1055      0.013556  1.0000  0.026749   52.0
T1105      0.012513  1.0000  0.024717   48.0
T1078      0.008863  1.0000  0.017571   34.0
T1204.002  0.007299  1.0000  0.014493   28.0
T1071.001  0.006778  1.0000  0.013465   26.0
T1082      0.006778  1.0000  0.013465   26.0
T1562.001  0.005996  1.0000  0.011920   23.0
T1047      0.005474  1.0000  0.010889   21.0
T1547.001  0.005214  1.0000  0.010373   20.0
T1566.001  0.003910  1.0000  0.007790   15.0
T1543.003  0.003650  1.0000  0.007273   14.0
T1218.011  0.003148  0.4000  0.006247   15.0
T1574.002  0.003128  1.0000  0.006237   12.0
T1005      0.003128  1.0000  0.006237   12.0
T1570      0.002868  1.0000  0.005719   11.0
T1095      0.002868  1.0000  0.005719   11.0
T1016      0.002346  1.0000  0.004681    9.0
T1113      0.002346  1.0000  0.004681    9.0
T1110      0.002086  1.0000  0.004162    8.0
T1518.001  0.001825  1.0000  0.003643    7.0
T1033      0.001825  1.0000  0.003643    7.0
T1190      0.001564  1.0000  0.003123    6.0
T1484.001  0.001564  1.0000  0.003123    6.0
T1552.001  0.001564  1.0000  0.003123    6.0
T1056.001  0.001303  1.0000  0.002603    5.0
T1210      0.001303  1.0000  0.002603    5.0
T1012      0.001303  1.0000  0.002603    5.0
T1569.002  0.001043  1.0000  0.002083    4.0
T1068      0.000782  1.0000  0.001563    3.0
T1112      0.000000  0.0000  0.000000   20.0
T1557.001  0.000000  0.0000  0.000000    3.0
T1564.001  0.000000  0.0000  0.000000    4.0
T1074.001  0.000000  0.0000  0.000000    5.0
T1548.002  0.000000  0.0000  0.000000    7.0
T1057      0.000000  0.0000  0.000000    9.0
T1041      0.000000  0.0000  0.000000   10.0
T1036.005  0.000000  0.0000  0.000000   11.0
T1219      0.000000  0.0000  0.000000   12.0
T1573.001  0.000000  0.0000  0.000000   13.0
T1021.001  0.000000  0.0000  0.000000   20.0
T1070.004  0.000000  0.0000  0.000000   21.0
T1083      0.000000  0.0000  0.000000   21.0
T1090      0.000000  0.0000  0.000000   22.0
T1003.001  0.000000  0.0000  0.000000   25.0
T1053.005  0.000000  0.0000  0.000000   25.0
T1106      0.000000  0.0000  0.000000   41.0
T1059.003  0.000000  0.0000  0.000000   71.0
T1140      0.000000  0.0000  0.000000   89.0
T1027      0.000000  0.0000  0.000000  152.0
T1072      0.000000  0.0000  0.000000    2.0
(macro)    0.002321  0.5680  0.004610    NaN
(micro)    0.004016  0.4258  0.007957    NaN
