from IPython import get_ipython

get_ipython().run_cell_magic('bash', '', '\nTOTAL_UPDATES=125                                       # Total number of training steps\nWARMUP_UPDATES=10                                       # Warmup the learning rate over this many updates\nLR=6e-04                                                # Peak LR for polynomial LR scheduler.\nNUM_CLASSES=2                           \nMAX_SENTENCES=3                                         # Batch size.\nNUM_NODES=1\t\t\t                                    # Number of machines\nROBERTA_PATH=../data/keplerModels/KEPLERforNLP.pt       # Path to the original roberta model\nCHECKPOINT_PATH=../data/checkpoints                     # Directory to store the checkpoints\nUPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\n\nDATA_DIR=../data/output/single\n\n#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\nKE_DATA=$DATA_DIR/KE1_0:\n\nDIST_SIZE=`expr $NUM_NODES`\n\npython -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\n        --save-dir $CHECKPOINT_PATH \\\n        --max-sentences $MAX_SENTENCES \\\n        --tokens-per-sample 512 \\\n        --task MLMetKE \\\n        --sample-break-mode complete \\\n        --required-batch-size-multiple 1 \\\n        --arch roberta_base \\\n        --criterion MLMetKE \\\n        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n        --optimizer adam --adam-betas "(0.9, 0.98)" --adam-eps 1e-06 \\\n        --clip-norm 0.0 \\\n        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\n        --update-freq "$UPDATE_FREQ" \\\n        --negative-sample-size 1 --ke-model TransE \\\n        --init-token 0 \\\n        --separator-token 2 \\\n        --gamma 4 --nrelation 822 \\\n        --skip-invalid-size-inputs-valid-test \\\n        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n        --reset-optimizer --distributed-world-size "${DIST_SIZE}" --ddp-backend no_c10d --distributed-port 23456 \\\n        --log-format simple --log-interval 1 \\\n        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)\n')
