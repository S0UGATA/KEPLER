{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:49:39.550579Z",
     "start_time": "2024-04-17T17:49:38.333451Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1bf61ccb51e514",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Get tram single label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fafa36e6fc8fdee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:49:41.316676Z",
     "start_time": "2024-04-17T17:49:41.267951Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-21 15:15:05--  https://raw.githubusercontent.com/center-for-threat-informed-defense/tram/main/data/tram2-data/single_label.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1024483 (1000K) [text/plain]\n",
      "Saving to: ‘../data/input/single_label.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 53.0M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 69.0M 0s\n",
      "   100K .......... .......... .......... .......... .......... 14%  304M 0s\n",
      "   150K .......... .......... .......... .......... .......... 19%  301M 0s\n",
      "   200K .......... .......... .......... .......... .......... 24%  186M 0s\n",
      "   250K .......... .......... .......... .......... .......... 29% 84.6M 0s\n",
      "   300K .......... .......... .......... .......... .......... 34%  491M 0s\n",
      "   350K .......... .......... .......... .......... .......... 39%  473M 0s\n",
      "   400K .......... .......... .......... .......... .......... 44%  419M 0s\n",
      "   450K .......... .......... .......... .......... .......... 49%  532M 0s\n",
      "   500K .......... .......... .......... .......... .......... 54%  517M 0s\n",
      "   550K .......... .......... .......... .......... .......... 59%  526M 0s\n",
      "   600K .......... .......... .......... .......... .......... 64%  475M 0s\n",
      "   650K .......... .......... .......... .......... .......... 69%  507M 0s\n",
      "   700K .......... .......... .......... .......... .......... 74%  537M 0s\n",
      "   750K .......... .......... .......... .......... .......... 79%  520M 0s\n",
      "   800K .......... .......... .......... .......... .......... 84%  449M 0s\n",
      "   850K .......... .......... .......... .......... .......... 89%  537M 0s\n",
      "   900K .......... .......... .......... .......... .......... 94%  539M 0s\n",
      "   950K .......... .......... .......... .......... .......... 99%  557M 0s\n",
      "  1000K                                                       100% 41.8M=0.004s\n",
      "\n",
      "2024-04-21 15:15:06 (235 MB/s) - ‘../data/input/single_label.json’ saved [1024483/1024483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/input\n",
    "wget -O ../data/input/single_label.json https://raw.githubusercontent.com/center-for-threat-informed-defense/tram/main/data/tram2-data/single_label.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d97678482b091",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this version, we will consider the text, tactic and document title, all 3 of them as nodes.\n",
    "The ontology then will be:\n",
    "\n",
    "Nodes: \n",
    "    text, technique, doc_title\n",
    "    \n",
    "Relationships: \n",
    "    uses, found-in\n",
    "\n",
    "Graph triple types will be:\n",
    "    text uses technique\n",
    "    text found-in doc_title\n",
    "    technique found-in doc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f78d849a99e5804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:09.692464Z",
     "start_time": "2024-04-17T17:50:09.670288Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/input/single_label.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9065632917dda87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:11.723533Z",
     "start_time": "2024-04-17T17:50:11.710257Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This file extracts credentials from LSASS simi...</td>\n",
       "      <td>T1003.001</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It calls OpenProcess on lsass.exe with access ...</td>\n",
       "      <td>T1003.001</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It spreads to Microsoft Windows machines using...</td>\n",
       "      <td>T1210</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMB exploitation via EternalBlue</td>\n",
       "      <td>T1210</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SMBv1 Exploitation via EternalBlue</td>\n",
       "      <td>T1210</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>collects local files and information from the ...</td>\n",
       "      <td>T1005</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>uses HTTPS to communicate with its C2 servers</td>\n",
       "      <td>T1071.001</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>samples have used HTTP over ports 447 and 8082...</td>\n",
       "      <td>T1071.001</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>downloads several additional files and saves t...</td>\n",
       "      <td>T1105</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>uses a custom crypter leveraging Microsoft’s C...</td>\n",
       "      <td>T1573.001</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5089 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      label  \\\n",
       "0     This file extracts credentials from LSASS simi...  T1003.001   \n",
       "1     It calls OpenProcess on lsass.exe with access ...  T1003.001   \n",
       "2     It spreads to Microsoft Windows machines using...      T1210   \n",
       "3                      SMB exploitation via EternalBlue      T1210   \n",
       "4                    SMBv1 Exploitation via EternalBlue      T1210   \n",
       "...                                                 ...        ...   \n",
       "5084  collects local files and information from the ...      T1005   \n",
       "5085      uses HTTPS to communicate with its C2 servers  T1071.001   \n",
       "5086  samples have used HTTP over ports 447 and 8082...  T1071.001   \n",
       "5087  downloads several additional files and saves t...      T1105   \n",
       "5088  uses a custom crypter leveraging Microsoft’s C...  T1573.001   \n",
       "\n",
       "                                              doc_title  \n",
       "0     NotPetya Technical Analysis  A Triple Threat F...  \n",
       "1     NotPetya Technical Analysis  A Triple Threat F...  \n",
       "2     NotPetya Technical Analysis  A Triple Threat F...  \n",
       "3     NotPetya Technical Analysis  A Triple Threat F...  \n",
       "4     NotPetya Technical Analysis  A Triple Threat F...  \n",
       "...                                                 ...  \n",
       "5084                          AA21076A TrickBot Malware  \n",
       "5085                          AA21076A TrickBot Malware  \n",
       "5086                          AA21076A TrickBot Malware  \n",
       "5087                          AA21076A TrickBot Malware  \n",
       "5088                          AA21076A TrickBot Malware  \n",
       "\n",
       "[5089 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a11df2ebf91f89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Getting all unique labels, doc_titles and text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30af359f89d46de8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:13.760136Z",
     "start_time": "2024-04-17T17:50:13.753668Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1003.001', 'T1210', 'T1570', 'T1140', 'T1218.011', 'T1059.003',\n",
       "       'T1057', 'T1518.001', 'T1106', 'T1082', 'T1016', 'T1078', 'T1047',\n",
       "       'T1027', 'T1056.001', 'T1083', 'T1053.005', 'T1070.004', 'T1105',\n",
       "       'T1090', 'T1005', 'T1574.002', 'T1071.001', 'T1484.001',\n",
       "       'T1204.002', 'T1055', 'T1562.001', 'T1033', 'T1566.001', 'T1219',\n",
       "       'T1547.001', 'T1021.001', 'T1543.003', 'T1569.002', 'T1036.005',\n",
       "       'T1112', 'T1041', 'T1110', 'T1190', 'T1564.001', 'T1113',\n",
       "       'T1573.001', 'T1095', 'T1552.001', 'T1012', 'T1074.001',\n",
       "       'T1548.002', 'T1068', 'T1072', 'T1557.001'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_techniques = data['label'].explode().dropna().unique()\n",
    "all_techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37005b13e7409616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:14.674637Z",
     "start_time": "2024-04-17T17:50:14.667667Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NotPetya Technical Analysis  A Triple Threat File Encryption MFT Encryption Credential Theft',\n",
       "       'Earth Zhulong Familiar Patterns Target Southeast Asian Firms',\n",
       "       'Malware Spotlight Camaro Dragons TinyNote Backdoor',\n",
       "       'Rorschach  A New Sophisticated and Fast Ransomware  Check Point Research',\n",
       "       'Bypassing Intel CET with Counterfeit Objects  OffSec',\n",
       "       'Emotet Strikes Again  LNK File Leads to Domain Wide Ransomware  The DFIR Report',\n",
       "       'Malware Analysis LummaC2 Stealer',\n",
       "       'FedEx Phishing Campaign Abusing TrustedForm and PAAY',\n",
       "       'Take a NetWalk on the Wild Side',\n",
       "       'Malicious OAuth applications used to compromise email servers and spread spam  Microsoft Security Blog',\n",
       "       'Nefilim Ransomware',\n",
       "       'Deja Vu All Over Again Tax Scammers at Large',\n",
       "       'Threat Assessment Black Basta Ransomware',\n",
       "       'Hafniuminspired cyberattacks neutralized by AI',\n",
       "       'eSentire Threat Intelligence Malware Analysis BatLoader',\n",
       "       'Early Bird Catches the Wormhole Observations from the StellarParticle Campaign',\n",
       "       '3CXDesktopApp Backdoored in a Suspected Lazarus Campaign\\xa0\\xa0',\n",
       "       'Not just an infostealer Gopuram backdoor deployed through 3CX supply chain attack',\n",
       "       'AA21200A Tactics Techniques and Procedures of Indicted APT40 Actors Associated with Chinas MSS Hainan State Security Department',\n",
       "       'Operation Spalax Targeted malware attacks in Colombia',\n",
       "       'LAPSUS Recent techniques tactics and procedures',\n",
       "       'Detecting Credential Stealing Attacks Through Active InNetwork Defense',\n",
       "       'Understanding DNS attacks Identifying and patching vulnerabilities  Snyk',\n",
       "       'EvilExtractor  AllinOne Stealer',\n",
       "       'LockBit 20 How This RaaS Operates and How to Protect Against It',\n",
       "       'SOC Team Essentials  How to Investigate and Track the 8220 Gang Cloud Threat',\n",
       "       'Fantasy  a new Agrius wiper deployed through a supplychain attack',\n",
       "       'AA22320A Iranian GovernmentSponsored APT Actors Compromise Federal Network Deploy Crypto Miner Credential Harvester',\n",
       "       'New Horabot campaign targets the Americas',\n",
       "       'Vice Society leverages PrintNightmare in ransomware attacks',\n",
       "       'UNC215 Spotlight on a Chinese Espionage Campaign in Israel',\n",
       "       'Dark Web Profile MuddyWater APT Group',\n",
       "       'Cobalt Strike a Defenders Guide  Part 2',\n",
       "       'Tailoring Sandbox Techniques to Hidden Threats',\n",
       "       'Babadeda Crypter targeting crypto NFT and DeFi communities',\n",
       "       'Threat Assessment BlackCat Ransomware',\n",
       "       'AA21200B Chinese StateSponsored Cyber Operations Observed TTPs',\n",
       "       'Vulnerability in Essential Addons for Elementor Leads to Mass Infection',\n",
       "       'StopRansomware Royal Ransomware',\n",
       "       'GuLoader VBScript Variant Returns with PowerShell Updates',\n",
       "       'Xollam the Latest Face of TargetCompany',\n",
       "       'Operation Tainted Love  Chinese APTs Target Telcos in New Attacks',\n",
       "       'Defending Users NAS Devices From Evolving Threats',\n",
       "       'Enigma Stealer Targets Cryptocurrency Industry with Fake Jobs',\n",
       "       'ExConti and FIN7 Actors Collaborate with New Domino Backdoor',\n",
       "       'Fat Cats',\n",
       "       'Analysis on recent wiper attacks examples and how wiper malware works',\n",
       "       'Higaisa or Winnti APT41 backdoors old and new',\n",
       "       'Operation Harvest A Deep Dive into a Longterm Campaign',\n",
       "       'Prilex Brazilian PoS malware evolution',\n",
       "       'The Rising Trend of OneNote Documents for Malware delivery',\n",
       "       'ZeroDay Vulnerability in MOVEit Transfer Exploited for Data Theft',\n",
       "       'Iranian GovernmentSponsored APT Actors Compromise Federal Network Deploy Crypto Miner Credential Harvester',\n",
       "       'FinSpy unseen findings',\n",
       "       'Vice Society Profiling a Persistent Threat to the Education Sector',\n",
       "       'Spike in LokiBot Activity During Final Week of 2022',\n",
       "       'Nationstate threat actor Mint Sandstorm refines tradecraft to attack highvalue targets',\n",
       "       'GuLoader Demystified Unraveling its Vectored Exception Handler Approach',\n",
       "       'Pack it Secretly Earth Pretas Updated Stealthy Strategies',\n",
       "       'Malware Reverse Engineering for Beginners  Part 2',\n",
       "       'Supply Chain Risk from Gigabyte App Center Backdoor  Eclypsium  Supply Chain Security for the Modern Enterprise',\n",
       "       'Gotta Catch Em All  Understanding the NetSupport RAT Campaigns Hiding Behind Pokemon Lures',\n",
       "       'Uncommon infection methodspart 2',\n",
       "       'German users targeted with Gootkit banker or REvil ransomware',\n",
       "       'New IcedID variants shift from bank fraud to malware delivery',\n",
       "       'BumbleBee Roasts Its Way to Domain Admin',\n",
       "       'Operation CMDStealer Financially Motivated Campaign Leverages CMDBased Scripts and LOLBaS for Online Banking Theft in Portugal Peru and Mexico',\n",
       "       'AA20336A Advanced Persistent Threat Actors Targeting US Think Tanks',\n",
       "       'Do Not Cross The RedLine Stealer Detections and Analysis',\n",
       "       'Conti Team One Splinter Group Resurfaces as Royal Ransomware with Callback Phishing Attacks',\n",
       "       'Threat Advisory 3CX Softphone Supply Chain Compromise',\n",
       "       'Malware Disguised as Document from Ukraines Energoatom Delivers Havoc Demon Backdoor',\n",
       "       'In the footsteps of the Fancy Bear PowerPoint\\xa0mouseover event abused to deliver Graphite implants',\n",
       "       'Can You See It Now An Emerging LockBit Campaign',\n",
       "       'WTB Remote Mac Exploitation Via Custom URL Schemes',\n",
       "       'Dead or Alive An Emotet Story', 'Conti Ransomware',\n",
       "       'Microsoft research uncovers new Zerobot capabilities  Microsoft Security Blog',\n",
       "       'Just Because Its Old Doesnt Mean You Throw It Away Including Malware',\n",
       "       'Carbon Blacks TrueBot Detection',\n",
       "       'ITG10 Likely Targeting South Korean Entities of Interest to the Democratic Peoples Republic of Korea DPRK',\n",
       "       'Abusing cloud services to fly under the radar',\n",
       "       'Transparent Tribe APT36  PakistanAligned Threat Actor Expands Interest in Indian Education Sector',\n",
       "       'SYS01 Stealer Will Steal Your Facebook Info',\n",
       "       'Banking Trojan Techniques How Financially Motivated Malware Became Infrastructure',\n",
       "       'Tracking Traces of Malware Disguised as Hancom Office Document File and Being Distributed RedEyes',\n",
       "       'Attackers use domain fronting technique to target Myanmar with Cobalt Strike',\n",
       "       'Akira Ransomware is bringin 1988 back',\n",
       "       'WTB Adwind Trojan Circumvents Antivirus Software To Infect Your PC',\n",
       "       'Hungry for data ModPipe backdoor hits POS software used in hospitality sector',\n",
       "       'Update 2 3CX users under DLLsideloading attack What you need to know',\n",
       "       'GoBruteforcer GolangBased Botnet Actively Harvests Web Servers',\n",
       "       'The LockBit ransomware kinda comes for macOS',\n",
       "       'IronNetInjector Turlas New Malware Loading Tool',\n",
       "       'Dissecting One of APT29s Fileless WMI and PowerShell Backdoors POSHSPY',\n",
       "       'Hancitor Infection Chain Analysis An Examination of its Unpacking Routine and Execution Techniques',\n",
       "       'Smoking Out a DARKSIDE Affiliates Supply Chain Software Compromise',\n",
       "       'AA20258A Chinese Ministry of State SecurityAffiliated Cyber Threat Actor Activity',\n",
       "       'Phishing Campaign Targets Chinese Nuclear Energy Industry',\n",
       "       'Latin American Governments Targeted By Ransomware',\n",
       "       'Chinese Threat Actor Used Modified Cobalt Strike Variant to Attack Taiwanese Critical Infrastructure',\n",
       "       'OpenSource Gh0st RAT Still Haunting Inboxes 15 Years After Release',\n",
       "       'CrowdStrike Uncovers I2Pminer MacOS Mineware Variant',\n",
       "       'Evasive NoEscape Ransomware Uses Reflective DLL Injection',\n",
       "       'Stolen certificates in two waves of ransomware and wiper attacks',\n",
       "       'BlueNoroff introduces new methods bypassing MoTW',\n",
       "       'Fork in the Ice The New Era of IcedID',\n",
       "       'Kimsuky Strikes Again  New Social Engineering Campaign Aims to Steal Credentials and Gather Strategic Intelligence',\n",
       "       'Warning New attack campaign utilized a new 0day RCE vulnerability on Microsoft Exchange Server',\n",
       "       'Recent TZW Campaigns Revealed As Part of GlobeImposter Malware Family',\n",
       "       'MERCURY and DEV1084 Destructive attack on hybrid environment',\n",
       "       'ZipJar a little bit unexpected attack chain',\n",
       "       'Whos swimming in South Korean waters Meet ScarCrufts Dolphin',\n",
       "       'Iron Tigers SysUpdate Reappears Adds Linux Targeting',\n",
       "       'Unwrapping Ursnifs Gifts  The DFIR Report',\n",
       "       'Qakbot Returns to ISO Delivery For Now',\n",
       "       'BeeWare of Trigona An Emerging Ransomware Strain',\n",
       "       'SharpPanda APT Campaign Expands its Arsenal Targeting G20 Nations',\n",
       "       'Increasing The Sting of HIVE Ransomware',\n",
       "       'ViperSoftX Updates Encryption Steals Data',\n",
       "       'CISA Red Team Shares Key Findings to Improve Monitoring and Hardening of Networks',\n",
       "       'McAfee Defenders Blog NetWalker',\n",
       "       'Technical Analysis Black Basta Malware Overview',\n",
       "       'Earth Pretas Cyberespionage Campaign Hits Over 200',\n",
       "       'Updated New Evidence Emerges to Suggest WatchDog Was Behind Crypto Campaign',\n",
       "       'These arent the apps youre looking for fake installers targeting Southeast and East Asia',\n",
       "       'Tax firms targeted by precision malware attacks',\n",
       "       'BazarLoader Mocks Researchers in December 2020 Malspam Campaign',\n",
       "       'Investigation with a twist an accidental APT attack and averted data destruction',\n",
       "       'Threat actors strive to cause Tax Day headaches',\n",
       "       'Revisiting the NSISbased crypter',\n",
       "       '\\xa0LockBit Ransomware 20 Resurfaces',\n",
       "       'Too Log Didnt Read  Unknown Actor Using CLFS Log Files for Stealth',\n",
       "       'Ransom Cartel Ransomware A Possible Connection With REvil',\n",
       "       'Malicious ISO File Leads to Domain Wide Ransomware  The DFIR Report',\n",
       "       'SeroXen RAT for sale',\n",
       "       'A lookback under the TA410 umbrella Its cyberespionage TTPs and activity',\n",
       "       'When byte code bites Who checks the contents of compiled Python files',\n",
       "       'MoonBounce the dark side of UEFI firmware',\n",
       "       'Threat Actors Use MSBuild to Deliver RATs Filelessly',\n",
       "       'How to Detect Cobalt Strike',\n",
       "       'New RapperBot Campaign  We Know What You Bruting for this Time',\n",
       "       'Inside the Mind of a Cyber Attacker from Malware creation to Data Exfiltration Part 1',\n",
       "       'CatB Ransomware  File Locker Sharpens Its Claws to Steal Data with MSDTC Service DLL Hijacking',\n",
       "       'Analyzing Solorigate the compromised DLL file that started a sophisticated cyberattack and how Microsoft Defender helps protect customers',\n",
       "       'Horabot campaign targeted businesses for more than two years before finally being discovered',\n",
       "       'StopRansomware Hive Ransomware',\n",
       "       'Linux malware strengthens links between Lazarus and the 3CX supplychain attack',\n",
       "       'AA21076A TrickBot Malware'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_titles = data['doc_title'].explode().dropna().unique()\n",
    "doc_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7afaf4f6e9d6f3cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:15.651855Z",
     "start_time": "2024-04-17T17:50:15.647656Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This file extracts credentials from LSASS similar to Mimikatz.',\n",
       "       'It calls OpenProcess on lsass.exe with access flag set to VM_READ, and looks for the modules wdigest.dll and lsasrv.dll loaded in the lsass.exe process.',\n",
       "       'It spreads to Microsoft Windows machines using several propagation methods, including the EternalBlue exploit for the CVE-2017-0144 vulnerability in the SMB service.',\n",
       "       ..., 'samples have used HTTP over ports 447 and 8082 for C2.',\n",
       "       \"downloads several additional files and saves them to the victim's machine.\",\n",
       "       'uses a custom crypter leveraging Microsoft’s CryptoAPI to encrypt C2 traffic.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data['text'].to_numpy()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7539994504a132",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Adding them all in one place labels, text, doc_titles:\n",
    "there are 50 labels, 149 doc_titles and 5089 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8feb6375c394f709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:16.780250Z",
     "start_time": "2024-04-17T17:50:16.776297Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1003.001', 'T1210', 'T1570', ...,\n",
       "       'samples have used HTTP over ports 447 and 8082 for C2.',\n",
       "       \"downloads several additional files and saves them to the victim's machine.\",\n",
       "       'uses a custom crypter leveraging Microsoft’s CryptoAPI to encrypt C2 traffic.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = np.concatenate((all_techniques, doc_titles, text))\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3a4f932718d8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The node list will then have \n",
    "    0-49 techniques\n",
    "    50-198 doc_titles \n",
    "    199-5287 text\n",
    "    \n",
    "Now to make the numeric triples, we will use the indexes of the nodes from the nodes list.\n",
    "\n",
    "\n",
    "Let us say that of the two relationships, uses = 0 and found-in = 1\n",
    "\n",
    "1. we make the triples for text uses technique\n",
    "2. we make the triples for text found-in doc_title\n",
    "3. we make the triples for technique found-in doc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "285010d75de580e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:20.297650Z",
     "start_time": "2024-04-17T17:50:19.190732Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "tech2doc = []\n",
    "\n",
    "np_data = data.to_numpy()\n",
    "\n",
    "for row in np_data:\n",
    "    text_index = np.where(nodes == row[0])[0][0]\n",
    "    technique_index = np.where(nodes == row[1])[0][0]\n",
    "    doc_title_index = np.where(nodes == row[2])[0][0]\n",
    "\n",
    "    triples.extend(\n",
    "        ((text_index, 0, technique_index),\n",
    "         (text_index, 1, doc_title_index))\n",
    "    )\n",
    "    tech2doc.append((technique_index, 1, doc_title_index))\n",
    "\n",
    "tech2doc = np.unique(tech2doc, axis=0)\n",
    "triples = np.array(triples)\n",
    "triples = np.append(triples, tech2doc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81537cb5a95cf7de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:21.064770Z",
     "start_time": "2024-04-17T17:50:21.060980Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[199,   0,   0],\n",
       "       [199,   1,  50],\n",
       "       [200,   0,   0],\n",
       "       ...,\n",
       "       [ 48,   1, 170],\n",
       "       [ 48,   1, 183],\n",
       "       [ 49,   1, 170]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a764d37b4b88e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:21.789890Z",
     "start_time": "2024-04-17T17:50:21.786022Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73d7e1731c3ec27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:22.581859Z",
     "start_time": "2024-04-17T17:50:22.578719Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(triples) == 2 * len(np_data) + len(tech2doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdbedc0434fb2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "split the triples into train, validation, test and save them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffaa092cfdc6cb7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:23.809495Z",
     "start_time": "2024-04-17T17:50:23.802217Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/output/single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17aea83473e80888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:33.655573Z",
     "start_time": "2024-04-17T17:50:33.629760Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output = \"../data/output/single\"\n",
    "pd.DataFrame(triples).to_csv(output + '/triples.txt', index=False, header=False, sep=' ')\n",
    "train, valid = train_test_split(triples, test_size=0.05)\n",
    "pd.DataFrame(train).to_csv(output + '/train.txt', index=False, header=False, sep=' ')\n",
    "pd.DataFrame(valid).to_csv(output + '/valid.txt', index=False, header=False, sep=' ')\n",
    "assert len(train) + len(valid) == len(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e884a9dc0d5054",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Also train test validation split the nodes.txt for MLM \n",
    "\n",
    "Question: If we split the nodes, what happens to the indexes of the nodes. they should not be shuffled, otherwise the triples will be wrong.\n",
    "Look at kepler.\n",
    "_ Answer: its ok, kepler use the whole node.bpe for ke tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7227ffd7e1cac4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:35.478932Z",
     "start_time": "2024-04-17T17:50:35.475900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def write_file(file_path, _list):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _row in _list:\n",
    "            f.write(_row.replace(\"\\n\", r\"\\n\").replace(\"\\t\", r\"\\t\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cba7dd0a1007c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:36.237309Z",
     "start_time": "2024-04-17T17:50:36.225854Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_train, n_test = train_test_split(nodes, test_size=0.2)\n",
    "\n",
    "n_train, n_valid = train_test_split(n_train, test_size=0.05)\n",
    "\n",
    "assert len(n_train) + len(n_test) + len(n_valid) == len(nodes)\n",
    "\n",
    "write_file(output + '/nodes_train.txt', n_train)\n",
    "write_file(output + '/nodes_valid.txt', n_valid)\n",
    "write_file(output + '/nodes_test.txt', n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65138b47f06d9c1d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "save the nodes to a file, this is somewhat tricky, since some of the node texts contain newline characters, and we need to preserve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "629835bdd91d2459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:50:37.436379Z",
     "start_time": "2024-04-17T17:50:37.429875Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "write_file(output + '/nodes.txt', nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7127ab0453243",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we follow Kepler@s Readme.md and prepare the KE and MLM data from the above files.\n",
    "\n",
    "    We will use the nodes...txt as our MLM data.\n",
    "    We will use the triples...txt as our KE data.\n",
    "    \n",
    "\n",
    "We first install the local version of kepler, which is built by extending fairsec:\n",
    "We now start with KE data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b3418abc686916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/sougata/projects/MyKEPLER\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cffi in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (1.16.0)\n",
      "Requirement already satisfied: fastBPE in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (0.1.0)\n",
      "Requirement already satisfied: numpy in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (1.26.4)\n",
      "Requirement already satisfied: regex in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (2023.12.25)\n",
      "Requirement already satisfied: torch in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (4.66.2)\n",
      "Requirement already satisfied: pycparser in /home/sougata/.local/lib/python3.10/site-packages (from cffi->fairseq==0.8.0) (2.22)\n",
      "Requirement already satisfied: filelock in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/sougata/anaconda3/envs/mykepler/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sougata/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==0.8.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sougata/anaconda3/envs/mykepler/lib/python3.10/site-packages (from jinja2->torch->fairseq==0.8.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sougata/.local/lib/python3.10/site-packages (from sympy->torch->fairseq==0.8.0) (1.3.0)\n",
      "Installing collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 0.8.0\n",
      "    Uninstalling fairseq-0.8.0:\n",
      "      Successfully uninstalled fairseq-0.8.0\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../..\n",
    "python -m pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854b2bd8151297c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. Encode the entity descriptions with the GPT-2 BPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b43f735dfb9b6f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:51:10.150729Z",
     "start_time": "2024-04-17T17:51:05.312385Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-21 15:29:52--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.230.111.15, 54.230.111.84, 54.230.111.29, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.230.111.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [text/plain]\n",
      "Saving to: ‘../data/gpt2_bpe/encoder.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4%  104M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 81.3M 0s\n",
      "   100K .......... .......... .......... .......... .......... 14% 86.1M 0s\n",
      "   150K .......... .......... .......... .......... .......... 19%  106M 0s\n",
      "   200K .......... .......... .......... .......... .......... 24%  259M 0s\n",
      "   250K .......... .......... .......... .......... .......... 29%  268M 0s\n",
      "   300K .......... .......... .......... .......... .......... 34%  243M 0s\n",
      "   350K .......... .......... .......... .......... .......... 39%  227M 0s\n",
      "   400K .......... .......... .......... .......... .......... 44%  304M 0s\n",
      "   450K .......... .......... .......... .......... .......... 49%  264M 0s\n",
      "   500K .......... .......... .......... .......... .......... 54%  255M 0s\n",
      "   550K .......... .......... .......... .......... .......... 58%  286M 0s\n",
      "   600K .......... .......... .......... .......... .......... 63%  252M 0s\n",
      "   650K .......... .......... .......... .......... .......... 68%  311M 0s\n",
      "   700K .......... .......... .......... .......... .......... 73%  334M 0s\n",
      "   750K .......... .......... .......... .......... .......... 78%  273M 0s\n",
      "   800K .......... .......... .......... .......... .......... 83%  344M 0s\n",
      "   850K .......... .......... .......... .......... .......... 88%  312M 0s\n",
      "   900K .......... .......... .......... .......... .......... 93%  339M 0s\n",
      "   950K .......... .......... .......... .......... .......... 98%  317M 0s\n",
      "  1000K .......... .......                                    100%  405M=0.005s\n",
      "\n",
      "2024-04-21 15:29:52 (203 MB/s) - ‘../data/gpt2_bpe/encoder.json’ saved [1042301/1042301]\n",
      "\n",
      "--2024-04-21 15:29:52--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.230.111.15, 54.230.111.84, 54.230.111.29, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.230.111.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘../data/gpt2_bpe/vocab.bpe’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 11% 68.3M 0s\n",
      "    50K .......... .......... .......... .......... .......... 22% 49.3M 0s\n",
      "   100K .......... .......... .......... .......... .......... 33%  105M 0s\n",
      "   150K .......... .......... .......... .......... .......... 44%  404M 0s\n",
      "   200K .......... .......... .......... .......... .......... 56%  356M 0s\n",
      "   250K .......... .......... .......... .......... .......... 67%  164M 0s\n",
      "   300K .......... .......... .......... .......... .......... 78%  397M 0s\n",
      "   350K .......... .......... .......... .......... .......... 89%  420M 0s\n",
      "   400K .......... .......... .......... .......... .....     100%  432M=0.003s\n",
      "\n",
      "2024-04-21 15:29:52 (142 MB/s) - ‘../data/gpt2_bpe/vocab.bpe’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/gpt2_bpe\n",
    "wget -O ../data/gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "wget -O ../data/gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a68f379-fbb8-49dd-a109-5d52c2094c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_bpe_encoder.sh  pretrain.sh\t\t       singleLabel2kepler_v2.py\n",
      "pretrain.py\t  singleLabel2kepler_v2.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5722ae2-31df-4d91-9fdf-b4faeabc3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python ../../examples/roberta/multiprocessing_bpe_encoder.py \\\n",
    "    --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "    --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "    --inputs ../data/output/single/nodes.txt \\\n",
    "    --outputs ../data/output/single/nodes.bpe \\\n",
    "    --keep-empty \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999e452c05d080b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2. Do negative sampling and dump the whole training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "299ef6c3b0bc945f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:51:13.307817Z",
     "start_time": "2024-04-17T17:51:11.618042Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-21 16:00:53.934957 load finish\n",
      "2024-04-21 16:00:53.967690 preparation finished\n",
      "2024-04-21 16:00:55.061555 training set finished\n",
      "2024-04-21 16:00:55.121704 all finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python ../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath ../data/output/single/KE1 \\\n",
    "    -ns 1 \\\n",
    "    --ent_desc ../data/output/single/nodes.bpe \\\n",
    "    --train ../data/output/single/train.txt \\\n",
    "    --valid ../data/output/single/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19491d497766b21e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3. then randomly split the KE training data into smaller parts and the number of training instances in each part aligns with the MLM training data\n",
    "For our case it will be just one split, since our data is small.\n",
    "\n",
    "Question: what does the negative_sampling_size = 1 do? it could be that the relation triples are false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ccac80a7fe44706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:51:15.265041Z",
     "start_time": "2024-04-17T17:51:14.988045Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data will be splited into 1 splits\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python ../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/single/KE1 \\\n",
    "    --split_size 6834352 \\\n",
    "    --negative_sampling_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27bce6070479558",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. We then binarize them for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48e1031600c7930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:51:40.482811Z",
     "start_time": "2024-04-17T17:51:17.702209Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-21 16:02:24--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.230.111.15, 54.230.111.29, 54.230.111.20, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.230.111.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 603290 (589K) [text/plain]\n",
      "Saving to: ‘../data/gpt2_bpe/dict.txt’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  8% 86.5M 0s\n",
      "    50K .......... .......... .......... .......... .......... 16%  118M 0s\n",
      "   100K .......... .......... .......... .......... .......... 25%  259M 0s\n",
      "   150K .......... .......... .......... .......... .......... 33%  216M 0s\n",
      "   200K .......... .......... .......... .......... .......... 42%  286M 0s\n",
      "   250K .......... .......... .......... .......... .......... 50%  290M 0s\n",
      "   300K .......... .......... .......... .......... .......... 59%  500M 0s\n",
      "   350K .......... .......... .......... .......... .......... 67%  475M 0s\n",
      "   400K .......... .......... .......... .......... .......... 76%  425M 0s\n",
      "   450K .......... .......... .......... .......... .......... 84%  476M 0s\n",
      "   500K .......... .......... .......... .......... .......... 93%  487M 0s\n",
      "   550K .......... .......... .......... .........            100%  477M=0.002s\n",
      "\n",
      "2024-04-21 16:02:24 (247 MB/s) - ‘../data/gpt2_bpe/dict.txt’ saved [603290/603290]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/single/KE1_0/head/train.bpe', validpref='../data/output/single/KE1_0/head/valid.bpe', testpref=None, destdir='../data/output/single/KE1_0/head', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/head/train.bpe: 11274 sents, 136303 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/head/valid.bpe: 594 sents, 7030 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/single/KE1_0/head\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/single/KE1_0/tail/train.bpe', validpref='../data/output/single/KE1_0/tail/valid.bpe', testpref=None, destdir='../data/output/single/KE1_0/tail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/tail/train.bpe: 11274 sents, 113267 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/tail/valid.bpe: 594 sents, 5940 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/single/KE1_0/tail\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/single/KE1_0/negHead/train.bpe', validpref='../data/output/single/KE1_0/negHead/valid.bpe', testpref=None, destdir='../data/output/single/KE1_0/negHead', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/negHead/train.bpe: 11274 sents, 149598 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/negHead/valid.bpe: 594 sents, 7549 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/single/KE1_0/negHead\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/single/KE1_0/negTail/train.bpe', validpref='../data/output/single/KE1_0/negTail/valid.bpe', testpref=None, destdir='../data/output/single/KE1_0/negTail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/negTail/train.bpe: 11274 sents, 150413 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/KE1_0/negTail/valid.bpe: 594 sents, 8233 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/single/KE1_0/negTail\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "wget -O ../data/gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "KE_Data=../data/output/single/KE1_0/\n",
    "for SPLIT in head tail negHead negTail;\n",
    "  do\n",
    "    python -m fairseq_cli.preprocess \\\n",
    "      --only-source \\\n",
    "      --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "      --trainpref ${KE_Data}${SPLIT}/train.bpe \\\n",
    "      --validpref ${KE_Data}${SPLIT}/valid.bpe \\\n",
    "      --destdir ${KE_Data}${SPLIT} \\\n",
    "      --workers 60; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8025db8c495182b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We now start with MLM data preprocessing:\n",
    "\n",
    "\n",
    "1. Now we encode the nodes_train, nodes_train and nodes_valid with the GPT-2 BPE:\n",
    "   (gpt2_bpe is already downloaded during the KE data preparation, we reuse that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0cd6deae6cbd2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:51:52.618008Z",
     "start_time": "2024-04-17T17:51:42.472772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir -p ../data/output/single/MLM\n",
    "\n",
    "for SPLIT in train valid test; do \\\n",
    "    python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "        --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "        --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "        --inputs ../data/output/single/nodes_${SPLIT}.txt \\\n",
    "        --outputs ../data/output/single/MLM/nodes_${SPLIT}.bpe \\\n",
    "        --keep-empty \\\n",
    "        --workers 60; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f69010e6eb7ac5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2. We then preprocess/binarize the data using the GPT-2 fairseq dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b00815d3e4ec9b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:52:00.761373Z",
     "start_time": "2024-04-17T17:51:54.112074Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/single/MLM/nodes_train.bpe', validpref='../data/output/single/MLM/nodes_valid.bpe', testpref='../data/output/single/MLM/nodes_test.bpe', destdir='../data/output/single/MLM-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/MLM/nodes_train.bpe: 4019 sents, 53423 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/MLM/nodes_valid.bpe: 212 sents, 2599 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/single/MLM/nodes_test.bpe: 1058 sents, 14138 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/single/MLM-bin\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir -p ../data/output/single/MLM-bin\n",
    "\n",
    "python -m fairseq_cli.preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "    --trainpref ../data/output/single/MLM/nodes_train.bpe \\\n",
    "    --validpref ../data/output/single/MLM/nodes_valid.bpe \\\n",
    "    --testpref ../data/output/single/MLM/nodes_test.bpe \\\n",
    "    --destdir ../data/output/single/MLM-bin \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2ce7d3fd5a9f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "All preprocessing is done, now we try out training the model with our data.\n",
    "\n",
    "We first download the pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e428b133c6d2b07",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# mkdir ../data/keplerModels\\n# \\n# if ! [ -f ../data/keplerModels/KEPLERforNLP.pt ]; then\\n#     wget -o ../data/keplerModels/KEPLERforNLP.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a21e5254-ceac-4b88-88e9-8ec58cbe8a1a/KEPLERforNLP.pt\\n# fi\\n# if ! [ -f ../data/keplerModels/KEPLERforKE.p ]; then\\nwget -o ../data/keplerModels/KEPLERforKE.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a684dc30-6a1a-4613-97ad-0144ae84e1ca/KEPLERforKE.pt\\n# fi\\n'' returned non-zero exit status 8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# mkdir ../data/keplerModels\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# if ! [ -f ../data/keplerModels/KEPLERforNLP.pt ]; then\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m#     wget -o ../data/keplerModels/KEPLERforNLP.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a21e5254-ceac-4b88-88e9-8ec58cbe8a1a/KEPLERforNLP.pt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# fi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# if ! [ -f ../data/keplerModels/KEPLERforKE.p ]; then\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mwget -o ../data/keplerModels/KEPLERforKE.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a684dc30-6a1a-4613-97ad-0144ae84e1ca/KEPLERforKE.pt\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# fi\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# mkdir ../data/keplerModels\\n# \\n# if ! [ -f ../data/keplerModels/KEPLERforNLP.pt ]; then\\n#     wget -o ../data/keplerModels/KEPLERforNLP.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a21e5254-ceac-4b88-88e9-8ec58cbe8a1a/KEPLERforNLP.pt\\n# fi\\n# if ! [ -f ../data/keplerModels/KEPLERforKE.p ]; then\\nwget -o ../data/keplerModels/KEPLERforKE.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a684dc30-6a1a-4613-97ad-0144ae84e1ca/KEPLERforKE.pt\\n# fi\\n'' returned non-zero exit status 8."
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "\n",
    "# mkdir ../data/keplerModels\n",
    "# \n",
    "# if ! [ -f ../data/keplerModels/KEPLERforNLP.pt ]; then\n",
    "#     wget -o ../data/keplerModels/KEPLERforNLP.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a21e5254-ceac-4b88-88e9-8ec58cbe8a1a/KEPLERforNLP.pt\n",
    "# fi\n",
    "# if ! [ -f ../data/keplerModels/KEPLERforKE.p ]; then\n",
    "#     wget -o ../data/keplerModels/KEPLERforKE.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a684dc30-6a1a-4613-97ad-0144ae84e1ca/KEPLERforKE.pt\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b13183ee99f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:52:06.364009Z",
     "start_time": "2024-04-17T17:52:06.236032Z"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir ../data/keplerModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28e06b4b3d4730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:54:57.830818Z",
     "start_time": "2024-04-17T17:52:07.478048Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -o ../data/keplerModels/KEPLERforNLP.pt https://cloud.tsinghua.edu.cn/seafhttp/files/70495ae5-48a0-48e4-9c1a-fe4893e80d3f/KEPLERforNLP.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40dbc688962e98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T17:58:22.032645Z",
     "start_time": "2024-04-17T17:55:32.592268Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -d -o ../data/keplerModels/KEPLERforKE.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a3b23761-e0bd-4850-b8c4-3788ce6cea3f/KEPLERforKE.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc15c847d3385d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then we first train on the NLP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7756b7fda225dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T19:16:30.510901Z",
     "start_time": "2024-04-17T19:16:06.447609Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sougata/projects/MyKEPLER/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "TOTAL_UPDATES=125000                                    # Total number of training steps\n",
    "WARMUP_UPDATES=10000                                    # Warmup the learning rate over this many updates\n",
    "LR=6e-04                                                # Peak LR for polynomial LR scheduler.\n",
    "NUM_CLASSES=2                           \n",
    "MAX_SENTENCES=3                                         # Batch size.\n",
    "NUM_NODES=1\t\t\t                                    # Number of machines\n",
    "ROBERTA_PATH=../data/checkpoints/checkpoint_last.pt\n",
    "# ROBERTA_PATH=../data/keplerModels/KEPLERforNLP.pt       # Path to the original roberta model\n",
    "CHECKPOINT_PATH=../data/checkpoints                     # Directory to store the checkpoints\n",
    "UPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\n",
    "\n",
    "DATA_DIR=../data/output/single\n",
    "\n",
    "#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\n",
    "KE_DATA=$DATA_DIR/KE1_0:\n",
    "\n",
    "DIST_SIZE=`expr $NUM_NODES`\n",
    "\n",
    "python -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\n",
    "        --save-dir $CHECKPOINT_PATH \\\n",
    "        --max-sentences $MAX_SENTENCES \\\n",
    "        --tokens-per-sample 512 \\\n",
    "        --task MLMetKE \\\n",
    "        --sample-break-mode complete \\\n",
    "        --required-batch-size-multiple 1 \\\n",
    "        --arch roberta_base \\\n",
    "        --criterion MLMetKE \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
    "        --clip-norm 0.0 \\\n",
    "        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
    "        --update-freq \"$UPDATE_FREQ\" \\\n",
    "        --negative-sample-size 1 --ke-model TransE \\\n",
    "        --init-token 0 \\\n",
    "        --separator-token 2 \\\n",
    "        --gamma 4 --nrelation 822 \\\n",
    "        --skip-invalid-size-inputs-valid-test \\\n",
    "        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
    "        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\n",
    "        --log-format simple --log-interval 1 > out_single.log \\\n",
    "        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3ea7a221741e",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c87ebfc9bd8dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sougata/anaconda3/envs/mykepler/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/sougata/anaconda3/envs/mykepler/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq_cli/eval_lm.py\", line 227, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq_cli/eval_lm.py\", line 223, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq_cli/eval_lm.py\", line 59, in main\n",
      "    models, args = checkpoint_utils.load_model_ensemble(\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/checkpoint_utils.py\", line 157, in load_model_ensemble\n",
      "    ensemble, args, _task = load_model_ensemble_and_task(filenames, arg_overrides, task)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/checkpoint_utils.py\", line 176, in load_model_ensemble_and_task\n",
      "    model.load_state_dict(state['model'], strict=True)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/models/fairseq_model.py\", line 69, in load_state_dict\n",
      "    return super().load_state_dict(state_dict, strict)\n",
      "  File \"/home/sougata/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2153, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for RobertaModel:\n",
      "\tsize mismatch for decoder.sentence_encoder.embed_tokens.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([50264, 768]).\n",
      "\tsize mismatch for decoder.lm_head.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([50264, 768]).\n",
      "\tsize mismatch for decoder.lm_head.bias: copying a param with shape torch.Size([50265]) from checkpoint, the shape in current model is torch.Size([50264]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='language_modeling', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3072, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, gen_subset='test', num_shards=1, shard_id=0, path='../data/checkpoints/checkpoint_last.pt', remove_bpe=None, quiet=False, model_overrides='{}', results_path=None, output_word_probs=False, output_word_stats=False, context_window=2560, softmax_batch=1024, momentum=0.99, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, data='../data/output/single/MLM-bin', sample_break_mode='complete', tokens_per_sample=1024, lazy_load=False, raw_text=False, output_dictionary_size=-1, self_target=False, future_target=False, past_target=False, add_bos_token=False, max_target_positions=None)\n",
      "| dictionary: 50264 types\n",
      "| loading model(s) from ../data/checkpoints/checkpoint_last.pt\n",
      "WARNING: deleting classification head (wikiData) from checkpoint not present in current model: ke_heads.wikiData.gamma\n",
      "WARNING: deleting classification head (wikiData) from checkpoint not present in current model: ke_heads.wikiData.emb_range\n",
      "WARNING: deleting classification head (wikiData) from checkpoint not present in current model: ke_heads.wikiData.relation_emb.weight\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\\n\\npython -m fairseq_cli.eval_lm ../data/output/single/MLM-bin \\\\\\n    --path ../data/checkpoints/checkpoint_last.pt \\\\\\n    --sample-break-mode complete --max-tokens 3072 \\\\\\n    --context-window 2560 --softmax-batch 1024\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython -m fairseq_cli.eval_lm ../data/output/single/MLM-bin \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --path ../data/checkpoints/checkpoint_last.pt \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --sample-break-mode complete --max-tokens 3072 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --context-window 2560 --softmax-batch 1024\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\\n\\npython -m fairseq_cli.eval_lm ../data/output/single/MLM-bin \\\\\\n    --path ../data/checkpoints/checkpoint_last.pt \\\\\\n    --sample-break-mode complete --max-tokens 3072 \\\\\\n    --context-window 2560 --softmax-batch 1024\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python -m fairseq_cli.eval_lm ../data/output/single/MLM-bin \\\n",
    "    --path ../data/checkpoints/checkpoint_last.pt \\\n",
    "    --sample-break-mode complete --max-tokens 3072 \\\n",
    "    --context-window 2560 --softmax-batch 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7feba10-7318-40cd-bc78-bbb43b73d5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
