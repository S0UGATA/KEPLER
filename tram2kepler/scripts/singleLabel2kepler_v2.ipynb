{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get tram single label data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb1bf61ccb51e514"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p../data/input\n",
    "!wget -O../data/input/single_label.json https: // raw.githubusercontent.com/center- for -threat-informed-defense/tram/main/data/tram2-data/single_label.json"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fafa36e6fc8fdee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this version, we will consider the text, tactic and document title, all 3 of them as nodes.\n",
    "The ontology then will be:\n",
    "\n",
    "Nodes: \n",
    "    text, technique, doc_title\n",
    "    \n",
    "Relationships: \n",
    "    uses, found-in\n",
    "\n",
    "Graph triple types will be:\n",
    "    text uses technique\n",
    "    text found-in doc_title\n",
    "    technique found-in doc_title"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "599d97678482b091"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/input/single_label.json')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f78d849a99e5804"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9065632917dda87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting all unique labels, doc_titles and text:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a11df2ebf91f89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_techniques = data['label'].explode().dropna().unique()\n",
    "all_techniques"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30af359f89d46de8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_titles = data['doc_title'].explode().dropna().unique()\n",
    "doc_titles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37005b13e7409616"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = data['text'].to_numpy()\n",
    "text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7afaf4f6e9d6f3cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding them all in one place labels, text, doc_titles:\n",
    "there are 50 labels, 149 doc_titles and 5089 text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae7539994504a132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nodes = np.concatenate((all_techniques, doc_titles, text))\n",
    "nodes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8feb6375c394f709"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The node list will then have \n",
    "    0-49 techniques\n",
    "    50-198 doc_titles \n",
    "    199-5287 text\n",
    "    \n",
    "Now to make the numeric triples, we will use the indexes of the nodes from the nodes list.\n",
    "\n",
    "\n",
    "Let us say that of the two relationships, uses = 0 and found-in = 1\n",
    "\n",
    "1. we make the triples for text uses technique\n",
    "2. we make the triples for text found-in doc_title\n",
    "3. we make the triples for technique found-in doc_title"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c3a4f932718d8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "triples = []\n",
    "tech2doc = []\n",
    "\n",
    "np_data = data.to_numpy()\n",
    "\n",
    "for row in np_data:\n",
    "    text_index = np.where(nodes == row[0])[0][0]\n",
    "    technique_index = np.where(nodes == row[1])[0][0]\n",
    "    doc_title_index = np.where(nodes == row[2])[0][0]\n",
    "\n",
    "    triples.append((text_index, 0, technique_index))\n",
    "    triples.append((text_index, 1, doc_title_index))\n",
    "    tech2doc.append((technique_index, 1, doc_title_index))\n",
    "\n",
    "tech2doc = np.unique(tech2doc, axis=0)\n",
    "triples = np.array(triples)\n",
    "triples = np.append(triples, tech2doc, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "285010d75de580e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "triples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81537cb5a95cf7de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(triples)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a764d37b4b88e34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert len(triples) == 2 * len(np_data) + len(tech2doc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c73d7e1731c3ec27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "split the triples into validation, test and save them to a file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5cdbedc0434fb2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p../data/output/single"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffaa092cfdc6cb7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = \"../data/output/single\"\n",
    "pd.DataFrame(triples).to_csv(output + '/triples.txt', index=False, header=False, sep=' ')\n",
    "train, valid = train_test_split(triples, test_size=0.05)\n",
    "pd.DataFrame(train).to_csv(output + '/train.txt', index=False, header=False, sep=' ')\n",
    "pd.DataFrame(valid).to_csv(output + '/valid.txt', index=False, header=False, sep=' ')\n",
    "assert len(train) + len(valid) == len(triples)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17aea83473e80888"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also train test validation split the nodes.txt for MLM "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20e884a9dc0d5054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def write_file(file_path, _list):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _row in _list:\n",
    "            f.write(_row.replace(\"\\n\", r\"\\n\").replace(\"\\t\", r\"\\t\") + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7227ffd7e1cac4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_train, n_test = train_test_split(nodes, test_size=0.2)\n",
    "\n",
    "# pd.DataFrame(n_test).to_csv('./data/output/single/nodes_test.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "n_train, n_valid = train_test_split(n_train, test_size=0.05)\n",
    "#pd.DataFrame(n_train).to_csv('./data/output/single/nodes_train.txt', index=False, header=False, sep=' ')\n",
    "#pd.DataFrame(n_valid).to_csv('./data/output/single/nodes_valid.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "assert len(n_train) + len(n_test) + len(n_valid) == len(nodes)\n",
    "\n",
    "write_file(output + '/nodes_train.txt', n_train)\n",
    "write_file(output + '/nodes_valid.txt', n_valid)\n",
    "write_file(output + '/nodes_test.txt', n_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42cba7dd0a1007c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "save the nodes to a file, this is somewhat tricky, since some of the node texts contain newline characters, and we need to preserve them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65138b47f06d9c1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "write_file(output + '/nodes.txt', nodes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629835bdd91d2459"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we follow Kepler@s Readme.md and prepare the KE and MLM data from the above files.\n",
    "    We will use the nodes...txt as our MLM data.\n",
    "    We will use the triples...txt as our KE data.\n",
    "    \n",
    "\n",
    "We first install the local version of kepler, which is built by extending fairsec:\n",
    "We now start with KE data preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9a7127ab0453243"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --editable../../"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09453c865812ce0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Encode the entity descriptions with the GPT-2 BPE:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9854b2bd8151297c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p../data/gpt2_bpe\n",
    "!wget -O../data/gpt2_bpe/encoder.json https: // dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "!wget -O../data/gpt2_bpe/vocab.bpe https: // dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "\n",
    "!python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "    --encoder-json../data/gpt2_bpe/encoder.json \\\n",
    "    --vocab-bpe../data/gpt2_bpe/vocab.bpe \\\n",
    "    --inputs../data/output/single/nodes.txt \\\n",
    "    --outputs../data/output/single/nodes.bpe \\\n",
    "    --keep-empty \\\n",
    "    --workers 60"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b43f735dfb9b6f4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Do negative sampling and dump the whole training and validation data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d999e452c05d080b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath../data/output/single/KE1 \\\n",
    "    -ns 1 \\\n",
    "    --ent_desc../data/output/single/nodes.bpe \\\n",
    "    --train../data/output/single/train.txt \\\n",
    "    --valid../data/output/single/valid.txt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299ef6c3b0bc945f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. then randomly split the KE training data into smaller parts and the number of training instances in each part aligns with the MLM training data\n",
    "For our case it will be just one split, since our data is small."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19491d497766b21e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/single/KE1 \\\n",
    "    --split_size 6834352 \\\n",
    "    --negative_sampling_size 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ccac80a7fe44706"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. We then binarize them for training:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b27bce6070479558"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O ../data/gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "KE_Data=../data/output/single/KE1_0/\n",
    "for SPLIT in head tail negHead negTail;\n",
    "  do\n",
    "    python -m fairseq_cli.preprocess \\\n",
    "      --only-source \\\n",
    "      --srcdict ../data/gpt2_bpe//dict.txt \\\n",
    "      --trainpref ${KE_Data}${SPLIT}/train.bpe \\\n",
    "      --validpref ${KE_Data}${SPLIT}/valid.bpe \\\n",
    "      --destdir ${KE_Data}${SPLIT} \\\n",
    "      --workers 60; \\\n",
    "  done"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e1031600c7930"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now start with MLM data preprocessing:\n",
    "\n",
    "\n",
    "1. Now we encode the nodes_train, nodes_train and nodes_valid with the GPT-2 BPE:\n",
    "   (gpt2_bpe is already downloaded during the KE data preparation, we reuse that.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8025db8c495182b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/output/single/MLM\n",
    "\n",
    "for SPLIT in train valid test; do \\\n",
    "    python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "        --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "        --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "        --inputs ../data/output/single/nodes_${SPLIT}.txt \\\n",
    "        --outputs ../data/output/single/MLM/nodes_${SPLIT}.bpe \\\n",
    "        --keep-empty \\\n",
    "        --workers 60; \\\n",
    "done"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0cd6deae6cbd2df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. We then preprocess/binarize the data using the GPT-2 fairseq dictionary:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29f69010e6eb7ac5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/output/single/MLM-bin\n",
    "\n",
    "fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "    --trainpref ../data/output/single/MLM/nodes_train.bpe \\\n",
    "    --validpref ../data/output/single/MLM/nodes_valid.bpe \\\n",
    "    --testpref ../data/output/single/MLM/nodes_test.bpe \\\n",
    "    --destdir ../data/output/single/MLM-bin \\\n",
    "    --workers 60"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b00815d3e4ec9b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "All preprocessing is done, now we try out training the model with our data.\n",
    "\n",
    "We first download the pretrained models:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acb2ce7d3fd5a9f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir ../data/keplerModels\n",
    "\n",
    "if ! [ -f ../data/keplerModels/KEPLERforNLP.pt ]; then\n",
    "    wget -o ../data/keplerModels/KEPLERforNLP.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a21e5254-ceac-4b88-88e9-8ec58cbe8a1a/KEPLERforNLP.pt\n",
    "fi\n",
    "if ! [ -f ../data/keplerModels/KEPLERforKE.p ]; then\n",
    "    wget -o ../data/keplerModels/KEPLERforKE.pt https://cloud.tsinghua.edu.cn/seafhttp/files/a684dc30-6a1a-4613-97ad-0144ae84e1ca/KEPLERforKE.pt\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aafe0205a95e34c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we first train on the NLP model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ffc15c847d3385d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TOTAL_UPDATES=125000                                    # Total number of training steps\n",
    "WARMUP_UPDATES=10000                                    # Warmup the learning rate over this many updates\n",
    "LR=6e-04                                                # Peak LR for polynomial LR scheduler.\n",
    "NUM_CLASSES=2                           \n",
    "MAX_SENTENCES=3                                         # Batch size.\n",
    "NUM_NODES=1\t\t\t                                    # Number of machines\n",
    "ROBERTA_PATH=\"../data/keplerModels/KEPLERforNLP.pt\"     # Path to the original roberta model\n",
    "CHECKPOINT_PATH=\"../data/checkpoints\"                   # Directory to store the checkpoints\n",
    "UPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\n",
    "\n",
    "DATA_DIR=../data/output/single\n",
    "\n",
    "#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\n",
    "KE_DATA=$DATA_DIR/KE1_0:\n",
    "\n",
    "DIST_SIZE=`expr $NUM_NODES \\* 4`\n",
    "\n",
    "fairseq-train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\n",
    "        --save-dir $CHECKPOINT_PATH \\\n",
    "        --max-sentences $MAX_SENTENCES \\\n",
    "        --tokens-per-sample 512 \\\n",
    "        --task MLMetKE \\\n",
    "        --sample-break-mode complete \\\n",
    "        --required-batch-size-multiple 1 \\\n",
    "        --arch roberta_base \\\n",
    "        --criterion MLMetKE \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
    "        --clip-norm 0.0 \\\n",
    "        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
    "        --update-freq \"$UPDATE_FREQ\" \\\n",
    "        --negative-sample-size 1 --ke-model TransE \\\n",
    "        --init-token 0 \\\n",
    "        --separator-token 2 \\\n",
    "        --gamma 4 --nrelation 822 \\\n",
    "        --skip-invalid-size-inputs-valid-test \\\n",
    "        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
    "        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\n",
    "        --log-format simple --log-interval 1 \\\n",
    "        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db7756b7fda225dd"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "acd586a30a900481"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
