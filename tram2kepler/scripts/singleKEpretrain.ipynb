{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sougata/anaconda3/envs/k310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/sougata/anaconda3/envs/k310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq_cli/train.py\", line 330, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq_cli/train.py\", line 326, in cli_main\n",
      "    main(args)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq_cli/train.py\", line 68, in main\n",
      "    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/checkpoint_utils.py\", line 125, in load_checkpoint\n",
      "    epoch_itr = trainer.get_train_iterator(epoch=itr_state['epoch'])\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/trainer.py\", line 226, in get_train_iterator\n",
      "    self.task.load_dataset(self.args.train_subset, epoch=epoch, combine=combine)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/tasks/MLMetKE.py\", line 308, in load_dataset\n",
      "    KEdataset=self.load_KE_dataset(split,self.args.KEdata, epoch,combine)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/tasks/MLMetKE.py\", line 263, in load_KE_dataset\n",
      "    head=desc_dataset(\"head\",self.source_dictionary)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/tasks/MLMetKE.py\", line 234, in desc_dataset\n",
      "    dataset = PrependTokenDataset(dataset, self.args.init_token)\n",
      "  File \"/home/sougata/projects/MyKEPLER/fairseq/data/prepend_token_dataset.py\", line 18, in __init__\n",
      "    self._sizes = np.array(dataset.sizes) + 1\n",
      "AttributeError: 'NoneType' object has no attribute 'sizes'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\\n\\nTOTAL_UPDATES=1                                    # Total number of training steps\\nWARMUP_UPDATES=1                                    # Warmup the learning rate over this many updates\\nLR=6e-04                                                # Peak LR for polynomial LR scheduler.\\nNUM_CLASSES=2                           \\nMAX_SENTENCES=3                                         # Batch size.\\nNUM_NODES=1\\t\\t\\t                                    # Number of machines\\nROBERTA_PATH=../data/keplerModels/KEPLERforKE.pt       # Path to the original roberta model\\nCHECKPOINT_PATH=../data/checkpoints/single/ke/                     # Directory to store the checkpoints\\nUPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\\n\\nDATA_DIR=../data/output/single\\n\\n#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\\nKE_DATA=$DATA_DIR/KE1_0:\\n\\nDIST_SIZE=`expr $NUM_NODES`\\n\\npython -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\\\\n        --save-dir $CHECKPOINT_PATH \\\\\\n        --max-sentences $MAX_SENTENCES \\\\\\n        --tokens-per-sample 512 \\\\\\n        --task MLMetKE \\\\\\n        --sample-break-mode complete \\\\\\n        --required-batch-size-multiple 1 \\\\\\n        --arch roberta_base \\\\\\n        --criterion MLMetKE \\\\\\n        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\\\\n        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\\\\n        --clip-norm 0.0 \\\\\\n        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\\\\n        --update-freq \"$UPDATE_FREQ\" \\\\\\n        --negative-sample-size 1 --ke-model TransE \\\\\\n        --init-token 0 \\\\\\n        --separator-token 2 \\\\\\n        --gamma 4 --nrelation 822 \\\\\\n        --skip-invalid-size-inputs-valid-test \\\\\\n        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\\\\n        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\\\\n        --log-format simple --log-interval 1 > out_single_ke.log \\\\\\n        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTOTAL_UPDATES=1                                    # Total number of training steps\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mWARMUP_UPDATES=1                                    # Warmup the learning rate over this many updates\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mLR=6e-04                                                # Peak LR for polynomial LR scheduler.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNUM_CLASSES=2                           \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMAX_SENTENCES=3                                         # Batch size.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNUM_NODES=1\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m                                    # Number of machines\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mROBERTA_PATH=../data/keplerModels/KEPLERforKE.pt       # Path to the original roberta model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mCHECKPOINT_PATH=../data/checkpoints/single/ke/                     # Directory to store the checkpoints\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mUPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDATA_DIR=../data/output/single\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mKE_DATA=$DATA_DIR/KE1_0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDIST_SIZE=`expr $NUM_NODES`\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --save-dir $CHECKPOINT_PATH \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --max-sentences $MAX_SENTENCES \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --tokens-per-sample 512 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --task MLMetKE \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --sample-break-mode complete \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --required-batch-size-multiple 1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --arch roberta_base \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --criterion MLMetKE \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --optimizer adam --adam-betas \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(0.9, 0.98)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m --adam-eps 1e-06 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --clip-norm 0.0 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --update-freq \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$UPDATE_FREQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --negative-sample-size 1 --ke-model TransE \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --init-token 0 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --separator-token 2 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --gamma 4 --nrelation 822 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --skip-invalid-size-inputs-valid-test \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --reset-optimizer --distributed-world-size \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;132;43;01m{DIST_SIZE}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m --ddp-backend no_c10d --distributed-port 23456 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        --log-format simple --log-interval 1 > out_single_ke.log \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\\n\\nTOTAL_UPDATES=1                                    # Total number of training steps\\nWARMUP_UPDATES=1                                    # Warmup the learning rate over this many updates\\nLR=6e-04                                                # Peak LR for polynomial LR scheduler.\\nNUM_CLASSES=2                           \\nMAX_SENTENCES=3                                         # Batch size.\\nNUM_NODES=1\\t\\t\\t                                    # Number of machines\\nROBERTA_PATH=../data/keplerModels/KEPLERforKE.pt       # Path to the original roberta model\\nCHECKPOINT_PATH=../data/checkpoints/single/ke/                     # Directory to store the checkpoints\\nUPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\\n\\nDATA_DIR=../data/output/single\\n\\n#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\\nKE_DATA=$DATA_DIR/KE1_0:\\n\\nDIST_SIZE=`expr $NUM_NODES`\\n\\npython -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\\\\n        --save-dir $CHECKPOINT_PATH \\\\\\n        --max-sentences $MAX_SENTENCES \\\\\\n        --tokens-per-sample 512 \\\\\\n        --task MLMetKE \\\\\\n        --sample-break-mode complete \\\\\\n        --required-batch-size-multiple 1 \\\\\\n        --arch roberta_base \\\\\\n        --criterion MLMetKE \\\\\\n        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\\\\n        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\\\\n        --clip-norm 0.0 \\\\\\n        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\\\\n        --update-freq \"$UPDATE_FREQ\" \\\\\\n        --negative-sample-size 1 --ke-model TransE \\\\\\n        --init-token 0 \\\\\\n        --separator-token 2 \\\\\\n        --gamma 4 --nrelation 822 \\\\\\n        --skip-invalid-size-inputs-valid-test \\\\\\n        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\\\\n        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\\\\n        --log-format simple --log-interval 1 > out_single_ke.log \\\\\\n        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "TOTAL_UPDATES=1                                    # Total number of training steps\n",
    "WARMUP_UPDATES=1                                    # Warmup the learning rate over this many updates\n",
    "LR=6e-04                                                # Peak LR for polynomial LR scheduler.\n",
    "NUM_CLASSES=2                           \n",
    "MAX_SENTENCES=3                                         # Batch size.\n",
    "NUM_NODES=1\t\t\t                                    # Number of machines\n",
    "ROBERTA_PATH=../data/keplerModels/KEPLERforKE.pt       # Path to the original roberta model\n",
    "CHECKPOINT_PATH=../data/checkpoints/single/ke/                     # Directory to store the checkpoints\n",
    "UPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\n",
    "\n",
    "DATA_DIR=../data/output/single\n",
    "\n",
    "#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\n",
    "KE_DATA=$DATA_DIR/KE1_0:\n",
    "\n",
    "DIST_SIZE=`expr $NUM_NODES`\n",
    "\n",
    "python -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\n",
    "        --save-dir $CHECKPOINT_PATH \\\n",
    "        --max-sentences $MAX_SENTENCES \\\n",
    "        --tokens-per-sample 512 \\\n",
    "        --task MLMetKE \\\n",
    "        --sample-break-mode complete \\\n",
    "        --required-batch-size-multiple 1 \\\n",
    "        --arch roberta_base \\\n",
    "        --criterion MLMetKE \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
    "        --clip-norm 0.0 \\\n",
    "        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
    "        --update-freq \"$UPDATE_FREQ\" \\\n",
    "        --negative-sample-size 1 --ke-model TransE \\\n",
    "        --init-token 0 \\\n",
    "        --separator-token 2 \\\n",
    "        --gamma 4 --nrelation 822 \\\n",
    "        --skip-invalid-size-inputs-valid-test \\\n",
    "        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
    "        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\n",
    "        --log-format simple --log-interval 1 > out_single_ke.log \\\n",
    "        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2b883-d17a-444a-9372-c83d1ed9b1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
