{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd236a9073b36eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.461215Z",
     "start_time": "2024-04-21T17:28:07.082068Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67b34b706edd68",
   "metadata": {},
   "source": [
    "Get tram multi label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df936a45a30f3332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.702131Z",
     "start_time": "2024-04-21T17:28:08.463055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-21 20:23:23--  https://raw.githubusercontent.com/center-for-threat-informed-defense/tram/main/data/tram2-data/multi_label.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5846167 (5.6M) [text/plain]\n",
      "Saving to: ‘../data/input/multi_label.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 46.1M 0s\n",
      "    50K .......... .......... .......... .......... ..........  1% 86.8M 0s\n",
      "   100K .......... .......... .......... .......... ..........  2%  194M 0s\n",
      "   150K .......... .......... .......... .......... ..........  3%  231M 0s\n",
      "   200K .......... .......... .......... .......... ..........  4%  279M 0s\n",
      "   250K .......... .......... .......... .......... ..........  5%  111M 0s\n",
      "   300K .......... .......... .......... .......... ..........  6%  462M 0s\n",
      "   350K .......... .......... .......... .......... ..........  7%  503M 0s\n",
      "   400K .......... .......... .......... .......... ..........  7%  437M 0s\n",
      "   450K .......... .......... .......... .......... ..........  8%  538M 0s\n",
      "   500K .......... .......... .......... .......... ..........  9%  194M 0s\n",
      "   550K .......... .......... .......... .......... .......... 10%  466M 0s\n",
      "   600K .......... .......... .......... .......... .......... 11%  455M 0s\n",
      "   650K .......... .......... .......... .......... .......... 12%  439M 0s\n",
      "   700K .......... .......... .......... .......... .......... 13%  505M 0s\n",
      "   750K .......... .......... .......... .......... .......... 14%  513M 0s\n",
      "   800K .......... .......... .......... .......... .......... 14%  385M 0s\n",
      "   850K .......... .......... .......... .......... .......... 15%  456M 0s\n",
      "   900K .......... .......... .......... .......... .......... 16%  490M 0s\n",
      "   950K .......... .......... .......... .......... .......... 17%  501M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 18%  439M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 19%  467M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 20%  483M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 21%  485M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 21%  414M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 22%  453M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 23%  454M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 24%  524M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 25%  496M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 26%  524M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 27%  522M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 28%  492M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 28%  418M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 29%  504M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 30%  525M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 31%  501M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 32%  434M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 33%  472M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 34%  466M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 35%  501M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 35%  488M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 36%  380M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 37%  504M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 38%  456M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 39%  501M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 40%  450M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 41%  483M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 42%  505M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 42%  498M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 43%  418M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 44%  487M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 45%  471M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 46%  426M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 47%  462M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 48%  526M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 49%  510M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 49%  457M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 50%  417M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 51%  530M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 52%  532M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 53%  520M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 54%  433M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 55%  493M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 56%  479M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 56%  503M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 57%  401M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 58%  477M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 59%  521M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 60%  532M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 61%  480M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 62%  495M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 63%  448M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 63%  512M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 64%  431M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 65%  494M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 66%  480M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 67%  505M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 68%  465M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 69%  515M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 70%  420M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 70%  462M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 71%  395M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 72%  502M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 73%  509M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 74%  525M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 75%  481M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 76%  496M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 77%  512M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 77%  531M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 78%  437M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 79%  535M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 80%  457M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 81%  496M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 82%  451M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 83%  535M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 84%  518M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 84%  526M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 85%  410M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 86%  519M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 87%  525M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 88%  530M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 89%  459M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 90%  522M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 91%  529M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 91%  518M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 92%  433M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 93%  525M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 94%  495M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 95%  537M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 96%  470M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 97%  526M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 98%  520M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 98%  502M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 99%  436M 0s\n",
      "  5700K .........                                             100%  765M=0.01s\n",
      "\n",
      "2024-04-21 20:23:24 (402 MB/s) - ‘../data/input/multi_label.json’ saved [5846167/5846167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O ../data/input/multi_label.json https://raw.githubusercontent.com/center-for-threat-informed-defense/tram/main/data/tram2-data/multi_label.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e5d5fad821ac9",
   "metadata": {},
   "source": [
    "In this version, we will consider the sentence, techniques and document title, all 3 of them as nodes.\n",
    "The ontology then will be:\n",
    "\n",
    "Nodes: \n",
    "    sentence, technique, doc_title\n",
    "    \n",
    "Relationships: \n",
    "    uses, used-in, found-in\n",
    "\n",
    "Graph triple types will be:\n",
    "    sentence uses technique\n",
    "    sentence found-in doc_title\n",
    "    technique used-in doc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3da462024125ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.789908Z",
     "start_time": "2024-04-21T17:28:08.703106Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/input/multi_label.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c94625d2be17d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.804611Z",
     "start_time": "2024-04-21T17:28:08.791649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "      <th>doc_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title: NotPetya Technical Analysis – A Triple ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Executive Summary This technical analysis prov...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For more information on CrowdStrike’s proactiv...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NotPetya combines ransomware with the ability ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It spreads to Microsoft Windows machines using...</td>\n",
       "      <td>[T1210]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19173</th>\n",
       "      <td>[2] Eclypsium Blog - TrickBot Now Offers 'Tric...</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19174</th>\n",
       "      <td>Initial Version March 24, 2021:</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19175</th>\n",
       "      <td>Added MITRE ATT&amp;CK Technique T1592.003 used fo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19176</th>\n",
       "      <td>Added new MITRE ATT&amp;CKs and updated Table 1</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19177</th>\n",
       "      <td>This product is provided subject to this Notif...</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19178 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence   labels  \\\n",
       "0      title: NotPetya Technical Analysis – A Triple ...       []   \n",
       "1      Executive Summary This technical analysis prov...       []   \n",
       "2      For more information on CrowdStrike’s proactiv...       []   \n",
       "3      NotPetya combines ransomware with the ability ...       []   \n",
       "4      It spreads to Microsoft Windows machines using...  [T1210]   \n",
       "...                                                  ...      ...   \n",
       "19173  [2] Eclypsium Blog - TrickBot Now Offers 'Tric...       []   \n",
       "19174                    Initial Version March 24, 2021:       []   \n",
       "19175  Added MITRE ATT&CK Technique T1592.003 used fo...       []   \n",
       "19176       Added new MITRE ATT&CKs and updated Table 1        []   \n",
       "19177  This product is provided subject to this Notif...       []   \n",
       "\n",
       "                                               doc_title  \n",
       "0      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "1      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "2      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "3      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "4      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "...                                                  ...  \n",
       "19173                          AA21076A TrickBot Malware  \n",
       "19174                          AA21076A TrickBot Malware  \n",
       "19175                          AA21076A TrickBot Malware  \n",
       "19176                          AA21076A TrickBot Malware  \n",
       "19177                          AA21076A TrickBot Malware  \n",
       "\n",
       "[19178 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95cc7a91fb9f1d",
   "metadata": {},
   "source": [
    "Getting all unique labels, doc_titles and text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef72a6e2489defcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.811276Z",
     "start_time": "2024-04-21T17:28:08.805605Z"
    }
   },
   "outputs": [],
   "source": [
    "all_techniques = data['labels'].explode().dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc9ca190617b0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.815262Z",
     "start_time": "2024-04-21T17:28:08.812248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1210', 'T1570', 'T1140', 'T1059.003', 'T1218.011', 'T1057',\n",
       "       'T1518.001', 'T1106', 'T1003.001', 'T1082', 'T1016', 'T1078',\n",
       "       'T1047', 'T1027', 'T1056.001', 'T1083', 'T1053.005', 'T1484.001',\n",
       "       'T1005', 'T1055', 'T1204.002', 'T1574.002', 'T1071.001', 'T1090',\n",
       "       'T1105', 'T1070.004', 'T1562.001', 'T1033', 'T1219', 'T1547.001',\n",
       "       'T1566.001', 'T1021.001', 'T1543.003', 'T1569.002', 'T1036.005',\n",
       "       'T1112', 'T1041', 'T1110', 'T1190', 'T1113', 'T1564.001', 'T1012',\n",
       "       'T1573.001', 'T1095', 'T1552.001', 'T1074.001', 'T1548.002',\n",
       "       'T1068', 'T1072', 'T1557.001'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c97dce368bc366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.819157Z",
     "start_time": "2024-04-21T17:28:08.816200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b1072a23019e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.828856Z",
     "start_time": "2024-04-21T17:28:08.820089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NotPetya Technical Analysis  A Triple Threat File Encryption MFT Encryption Credential Theft',\n",
       "       'Earth Zhulong Familiar Patterns Target Southeast Asian Firms',\n",
       "       'Malware Spotlight Camaro Dragons TinyNote Backdoor',\n",
       "       'Rorschach  A New Sophisticated and Fast Ransomware  Check Point Research',\n",
       "       'Bypassing Intel CET with Counterfeit Objects  OffSec',\n",
       "       'Emotet Strikes Again  LNK File Leads to Domain Wide Ransomware  The DFIR Report',\n",
       "       'Malware Analysis LummaC2 Stealer',\n",
       "       'FedEx Phishing Campaign Abusing TrustedForm and PAAY',\n",
       "       'Take a NetWalk on the Wild Side',\n",
       "       'Malicious OAuth applications used to compromise email servers and spread spam  Microsoft Security Blog',\n",
       "       'Nefilim Ransomware',\n",
       "       'Deja Vu All Over Again Tax Scammers at Large',\n",
       "       'Threat Assessment Black Basta Ransomware',\n",
       "       'Hafniuminspired cyberattacks neutralized by AI',\n",
       "       'eSentire Threat Intelligence Malware Analysis BatLoader',\n",
       "       'Early Bird Catches the Wormhole Observations from the StellarParticle Campaign',\n",
       "       '3CXDesktopApp Backdoored in a Suspected Lazarus Campaign\\xa0\\xa0',\n",
       "       'Not just an infostealer Gopuram backdoor deployed through 3CX supply chain attack',\n",
       "       'AA21200A Tactics Techniques and Procedures of Indicted APT40 Actors Associated with Chinas MSS Hainan State Security Department',\n",
       "       'Operation Spalax Targeted malware attacks in Colombia',\n",
       "       'LAPSUS Recent techniques tactics and procedures',\n",
       "       'Detecting Credential Stealing Attacks Through Active InNetwork Defense',\n",
       "       'Understanding DNS attacks Identifying and patching vulnerabilities  Snyk',\n",
       "       'EvilExtractor  AllinOne Stealer',\n",
       "       'LockBit 20 How This RaaS Operates and How to Protect Against It',\n",
       "       'SOC Team Essentials  How to Investigate and Track the 8220 Gang Cloud Threat',\n",
       "       'Fantasy  a new Agrius wiper deployed through a supplychain attack',\n",
       "       'AA22320A Iranian GovernmentSponsored APT Actors Compromise Federal Network Deploy Crypto Miner Credential Harvester',\n",
       "       'New Horabot campaign targets the Americas',\n",
       "       'Vice Society leverages PrintNightmare in ransomware attacks',\n",
       "       'UNC215 Spotlight on a Chinese Espionage Campaign in Israel',\n",
       "       'Dark Web Profile MuddyWater APT Group',\n",
       "       'Cobalt Strike a Defenders Guide  Part 2',\n",
       "       'Tailoring Sandbox Techniques to Hidden Threats',\n",
       "       'Babadeda Crypter targeting crypto NFT and DeFi communities',\n",
       "       'Threat Assessment BlackCat Ransomware',\n",
       "       'AA21200B Chinese StateSponsored Cyber Operations Observed TTPs',\n",
       "       'Vulnerability in Essential Addons for Elementor Leads to Mass Infection',\n",
       "       'StopRansomware Royal Ransomware',\n",
       "       'GuLoader VBScript Variant Returns with PowerShell Updates',\n",
       "       'Xollam the Latest Face of TargetCompany',\n",
       "       'Operation Tainted Love  Chinese APTs Target Telcos in New Attacks',\n",
       "       'Defending Users NAS Devices From Evolving Threats',\n",
       "       'Enigma Stealer Targets Cryptocurrency Industry with Fake Jobs',\n",
       "       'ExConti and FIN7 Actors Collaborate with New Domino Backdoor',\n",
       "       'Fat Cats',\n",
       "       'Analysis on recent wiper attacks examples and how wiper malware works',\n",
       "       'Higaisa or Winnti APT41 backdoors old and new',\n",
       "       'Operation Harvest A Deep Dive into a Longterm Campaign',\n",
       "       'Prilex Brazilian PoS malware evolution',\n",
       "       'The Rising Trend of OneNote Documents for Malware delivery',\n",
       "       'ZeroDay Vulnerability in MOVEit Transfer Exploited for Data Theft',\n",
       "       'Iranian GovernmentSponsored APT Actors Compromise Federal Network Deploy Crypto Miner Credential Harvester',\n",
       "       'FinSpy unseen findings',\n",
       "       'Vice Society Profiling a Persistent Threat to the Education Sector',\n",
       "       'Spike in LokiBot Activity During Final Week of 2022',\n",
       "       'Breaking Pedersen Hashes in Practice',\n",
       "       'Nationstate threat actor Mint Sandstorm refines tradecraft to attack highvalue targets',\n",
       "       'GuLoader Demystified Unraveling its Vectored Exception Handler Approach',\n",
       "       'Pack it Secretly Earth Pretas Updated Stealthy Strategies',\n",
       "       'Malware Reverse Engineering for Beginners  Part 2',\n",
       "       'Supply Chain Risk from Gigabyte App Center Backdoor  Eclypsium  Supply Chain Security for the Modern Enterprise',\n",
       "       'Gotta Catch Em All  Understanding the NetSupport RAT Campaigns Hiding Behind Pokemon Lures',\n",
       "       'Uncommon infection methodspart 2',\n",
       "       'German users targeted with Gootkit banker or REvil ransomware',\n",
       "       'New IcedID variants shift from bank fraud to malware delivery',\n",
       "       'BumbleBee Roasts Its Way to Domain Admin',\n",
       "       'Operation CMDStealer Financially Motivated Campaign Leverages CMDBased Scripts and LOLBaS for Online Banking Theft in Portugal Peru and Mexico',\n",
       "       'AA20336A Advanced Persistent Threat Actors Targeting US Think Tanks',\n",
       "       'Do Not Cross The RedLine Stealer Detections and Analysis',\n",
       "       'Conti Team One Splinter Group Resurfaces as Royal Ransomware with Callback Phishing Attacks',\n",
       "       'Threat Advisory 3CX Softphone Supply Chain Compromise',\n",
       "       'Malware Disguised as Document from Ukraines Energoatom Delivers Havoc Demon Backdoor',\n",
       "       'In the footsteps of the Fancy Bear PowerPoint\\xa0mouseover event abused to deliver Graphite implants',\n",
       "       'Can You See It Now An Emerging LockBit Campaign',\n",
       "       'WTB Remote Mac Exploitation Via Custom URL Schemes',\n",
       "       'Dead or Alive An Emotet Story', 'Conti Ransomware',\n",
       "       'Microsoft research uncovers new Zerobot capabilities  Microsoft Security Blog',\n",
       "       'Just Because Its Old Doesnt Mean You Throw It Away Including Malware',\n",
       "       'Carbon Blacks TrueBot Detection',\n",
       "       'ITG10 Likely Targeting South Korean Entities of Interest to the Democratic Peoples Republic of Korea DPRK',\n",
       "       'Abusing cloud services to fly under the radar',\n",
       "       'Transparent Tribe APT36  PakistanAligned Threat Actor Expands Interest in Indian Education Sector',\n",
       "       'SYS01 Stealer Will Steal Your Facebook Info',\n",
       "       'Banking Trojan Techniques How Financially Motivated Malware Became Infrastructure',\n",
       "       'Tracking Traces of Malware Disguised as Hancom Office Document File and Being Distributed RedEyes',\n",
       "       'Attackers use domain fronting technique to target Myanmar with Cobalt Strike',\n",
       "       'Akira Ransomware is bringin 1988 back',\n",
       "       'WTB Adwind Trojan Circumvents Antivirus Software To Infect Your PC',\n",
       "       'Hungry for data ModPipe backdoor hits POS software used in hospitality sector',\n",
       "       'Update 2 3CX users under DLLsideloading attack What you need to know',\n",
       "       'GoBruteforcer GolangBased Botnet Actively Harvests Web Servers',\n",
       "       'The LockBit ransomware kinda comes for macOS',\n",
       "       'IronNetInjector Turlas New Malware Loading Tool',\n",
       "       'Dissecting One of APT29s Fileless WMI and PowerShell Backdoors POSHSPY',\n",
       "       'Hancitor Infection Chain Analysis An Examination of its Unpacking Routine and Execution Techniques',\n",
       "       'Smoking Out a DARKSIDE Affiliates Supply Chain Software Compromise',\n",
       "       'AA20258A Chinese Ministry of State SecurityAffiliated Cyber Threat Actor Activity',\n",
       "       'Phishing Campaign Targets Chinese Nuclear Energy Industry',\n",
       "       'Latin American Governments Targeted By Ransomware',\n",
       "       'Chinese Threat Actor Used Modified Cobalt Strike Variant to Attack Taiwanese Critical Infrastructure',\n",
       "       'OpenSource Gh0st RAT Still Haunting Inboxes 15 Years After Release',\n",
       "       'CrowdStrike Uncovers I2Pminer MacOS Mineware Variant',\n",
       "       'Evasive NoEscape Ransomware Uses Reflective DLL Injection',\n",
       "       'Stolen certificates in two waves of ransomware and wiper attacks',\n",
       "       'BlueNoroff introduces new methods bypassing MoTW',\n",
       "       'Fork in the Ice The New Era of IcedID',\n",
       "       'Kimsuky Strikes Again  New Social Engineering Campaign Aims to Steal Credentials and Gather Strategic Intelligence',\n",
       "       'Warning New attack campaign utilized a new 0day RCE vulnerability on Microsoft Exchange Server',\n",
       "       'Recent TZW Campaigns Revealed As Part of GlobeImposter Malware Family',\n",
       "       'MERCURY and DEV1084 Destructive attack on hybrid environment',\n",
       "       'ZipJar a little bit unexpected attack chain',\n",
       "       'Whos swimming in South Korean waters Meet ScarCrufts Dolphin',\n",
       "       'Iron Tigers SysUpdate Reappears Adds Linux Targeting',\n",
       "       'Unwrapping Ursnifs Gifts  The DFIR Report',\n",
       "       'Qakbot Returns to ISO Delivery For Now',\n",
       "       'BeeWare of Trigona An Emerging Ransomware Strain',\n",
       "       'SharpPanda APT Campaign Expands its Arsenal Targeting G20 Nations',\n",
       "       'Increasing The Sting of HIVE Ransomware',\n",
       "       'ViperSoftX Updates Encryption Steals Data',\n",
       "       'CISA Red Team Shares Key Findings to Improve Monitoring and Hardening of Networks',\n",
       "       'McAfee Defenders Blog NetWalker',\n",
       "       'Technical Analysis Black Basta Malware Overview',\n",
       "       'Earth Pretas Cyberespionage Campaign Hits Over 200',\n",
       "       'Updated New Evidence Emerges to Suggest WatchDog Was Behind Crypto Campaign',\n",
       "       'These arent the apps youre looking for fake installers targeting Southeast and East Asia',\n",
       "       'Tax firms targeted by precision malware attacks',\n",
       "       'BazarLoader Mocks Researchers in December 2020 Malspam Campaign',\n",
       "       'Investigation with a twist an accidental APT attack and averted data destruction',\n",
       "       'Threat actors strive to cause Tax Day headaches',\n",
       "       'Revisiting the NSISbased crypter',\n",
       "       '\\xa0LockBit Ransomware 20 Resurfaces',\n",
       "       'Too Log Didnt Read  Unknown Actor Using CLFS Log Files for Stealth',\n",
       "       'Ransom Cartel Ransomware A Possible Connection With REvil',\n",
       "       'Malicious ISO File Leads to Domain Wide Ransomware  The DFIR Report',\n",
       "       'SeroXen RAT for sale',\n",
       "       'A lookback under the TA410 umbrella Its cyberespionage TTPs and activity',\n",
       "       'When byte code bites Who checks the contents of compiled Python files',\n",
       "       'MoonBounce the dark side of UEFI firmware',\n",
       "       'Threat Actors Use MSBuild to Deliver RATs Filelessly',\n",
       "       'How to Detect Cobalt Strike',\n",
       "       'New RapperBot Campaign  We Know What You Bruting for this Time',\n",
       "       'Inside the Mind of a Cyber Attacker from Malware creation to Data Exfiltration Part 1',\n",
       "       'CatB Ransomware  File Locker Sharpens Its Claws to Steal Data with MSDTC Service DLL Hijacking',\n",
       "       'Analyzing Solorigate the compromised DLL file that started a sophisticated cyberattack and how Microsoft Defender helps protect customers',\n",
       "       'Horabot campaign targeted businesses for more than two years before finally being discovered',\n",
       "       'StopRansomware Hive Ransomware',\n",
       "       'Linux malware strengthens links between Lazarus and the 3CX supplychain attack',\n",
       "       'Tomiris called they want their Turla malware back',\n",
       "       'AA21076A TrickBot Malware'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_titles = data['doc_title'].explode().dropna().unique()\n",
    "doc_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d2c4ecea9d3414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.832431Z",
     "start_time": "2024-04-21T17:28:08.829734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "180ba8e04e31f7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.836607Z",
     "start_time": "2024-04-21T17:28:08.834458Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = data['sentence'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1513a864623c9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.840769Z",
     "start_time": "2024-04-21T17:28:08.837572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19178,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac99a1660728ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.844422Z",
     "start_time": "2024-04-21T17:28:08.841626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['title: NotPetya Technical Analysis – A Triple Threat: File Encryption, MFT Encryption, Credential Theft url: https://www.crowdstrike.com/blog/petrwrap-ransomware-technical-analysis-triple-threat-file-encryption-mft-encryption-credential-theft/ Update: Due to naming convention consistency in the industry, CrowdStrike is now calling this variant of Petya – NotPetya. ',\n",
       "       'Executive Summary This technical analysis provides an in-depth analysis and review of NotPetya. ',\n",
       "       'For more information on CrowdStrike’s proactive protection features see the earlier CrowdStrike blog on how Falcon Endpoint Protection prevents the NotPetya attack. ',\n",
       "       ...,\n",
       "       'Added MITRE ATT&CK Technique T1592.003 used for reconnaissance May 20, 2021:',\n",
       "       'Added new MITRE ATT&CKs and updated Table 1 ',\n",
       "       'This product is provided subject to this Notification and this Privacy & Use policy. '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6f61357ac1847",
   "metadata": {},
   "source": [
    "Adding them all in one place labels, text, doc_titles: there are 50 labels, 151 doc_titles and 19178 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edeebd7d5321bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.848749Z",
     "start_time": "2024-04-21T17:28:08.845326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1210', 'T1570', 'T1140', ...,\n",
       "       'Added MITRE ATT&CK Technique T1592.003 used for reconnaissance May 20, 2021:',\n",
       "       'Added new MITRE ATT&CKs and updated Table 1 ',\n",
       "       'This product is provided subject to this Notification and this Privacy & Use policy. '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = np.concatenate((all_techniques, doc_titles, sentence))\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa738d1c1fb9f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.852501Z",
     "start_time": "2024-04-21T17:28:08.849825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19379,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "243bdc03669e1218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.855733Z",
     "start_time": "2024-04-21T17:28:08.853440Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(nodes) == len(all_techniques) + len(doc_titles) + len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407ac10c440a991",
   "metadata": {},
   "source": [
    "The node list will then have \n",
    "    0-49 techniques\n",
    "    50-200 doc_titles \n",
    "    201-19378 text\n",
    "    \n",
    "Now to make the numeric triples, we will use the indexes of the nodes from the nodes list.\n",
    "\n",
    "\n",
    "Let us say that of the two relationships, uses = 0 and used-in = 1, found-in = 2\n",
    "\n",
    "1. we make the triples for sentence uses technique\n",
    "2. we make the triples for sentence found-in doc_title\n",
    "3. we make the triples for technique used-in doc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad595dd2f9eaf7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:40:56.929395Z",
     "start_time": "2024-04-21T17:40:43.588312Z"
    }
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "tech2doc = []\n",
    "\n",
    "np_data = data.to_numpy()\n",
    "\n",
    "for row in np_data:\n",
    "    sentence_index = np.where(nodes == row[0])[0][0]\n",
    "    technique_indices = []\n",
    "    technique_indices.extend([np.where(nodes == technique)[0][0] for technique in row[1]])\n",
    "    doc_title_index = np.where(nodes == row[2])[0][0]\n",
    "    \n",
    "    triples.append((sentence_index, 2, doc_title_index))\n",
    "\n",
    "    if technique_indices:\n",
    "        triples.extend([(sentence_index, 0, technique_index) for technique_index in technique_indices])\n",
    "    \n",
    "    tech2doc.extend([(technique_index, 1, doc_title_index) for technique_index in technique_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d23cd55881237c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:42:24.556784Z",
     "start_time": "2024-04-21T17:42:24.553329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24321"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(triples)\n",
    "# print(tech2doc)\n",
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c85831102fe74c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:41:33.737399Z",
     "start_time": "2024-04-21T17:41:33.716408Z"
    }
   },
   "outputs": [],
   "source": [
    "tech2doc = np.unique(tech2doc, axis=0)\n",
    "triples = np.array(triples)\n",
    "triples = np.append(triples, tech2doc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9030db88c61c8f",
   "metadata": {},
   "source": [
    "split the triples into train, validation, test and save them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "655f8f83c7915c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:44:03.079668Z",
     "start_time": "2024-04-21T17:44:03.072202Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/output/multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c556e388463d82ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:44:05.687193Z",
     "start_time": "2024-04-21T17:44:05.630035Z"
    }
   },
   "outputs": [],
   "source": [
    "output = \"../data/output/multi\"\n",
    "pd.DataFrame(triples).to_csv(output + '/triples.txt', index=False, header=False, sep=' ')\n",
    "train, valid = train_test_split(triples, test_size=0.05)\n",
    "pd.DataFrame(train).to_csv(output + '/train.txt', index=False, header=False, sep=' ')\n",
    "pd.DataFrame(valid).to_csv(output + '/valid.txt', index=False, header=False, sep=' ')\n",
    "assert len(train) + len(valid) == len(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de7ad6ac7972c2",
   "metadata": {},
   "source": [
    "Also train test validation split the nodes.txt for MLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e95844c051cfd3d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:45:26.227896Z",
     "start_time": "2024-04-21T17:45:26.222531Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(file_path, _list):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _row in _list:\n",
    "            f.write(_row.replace(\"\\n\", r\"\\n\").replace(\"\\t\", r\"\\t\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7156d270cc5f8ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:45:28.746938Z",
     "start_time": "2024-04-21T17:45:28.720828Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train, n_test = train_test_split(nodes, test_size=0.2)\n",
    "\n",
    "n_train, n_valid = train_test_split(n_train, test_size=0.05)\n",
    "\n",
    "assert len(n_train) + len(n_test) + len(n_valid) == len(nodes)\n",
    "\n",
    "write_file(output + '/nodes_train.txt', n_train)\n",
    "write_file(output + '/nodes_valid.txt', n_valid)\n",
    "write_file(output + '/nodes_test.txt', n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba031ef9409d5f40",
   "metadata": {},
   "source": [
    "save the nodes to a file, this is somewhat tricky, since some of the node texts contain newline characters, and we need to preserve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cef5fb4d5612a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:45:39.804703Z",
     "start_time": "2024-04-21T17:45:39.786409Z"
    }
   },
   "outputs": [],
   "source": [
    "write_file(output + '/nodes.txt', nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a165c6bbaeadeb",
   "metadata": {},
   "source": [
    "Now we follow Kepler@s Readme.md and prepare the KE and MLM data from the above files.\n",
    "\n",
    "    We will use the nodes...txt as our MLM data.\n",
    "    We will use the triples...txt as our KE data.\n",
    "    \n",
    "\n",
    "We first install the local version of kepler, which is built by extending fairsec:\n",
    "We now start with KE data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21855481dca366",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9e1284a830614",
   "metadata": {},
   "source": [
    "1. Encode the entity descriptions with the GPT-2 BPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d40c2a86fd04a3da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:03:44.046654Z",
     "start_time": "2024-04-21T18:03:19.299103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 10000 lines\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# mkdir -p ../data/gpt2_bpe\n",
    "# wget -O ../data/gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "# wget -O ../data/gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python ../../examples/roberta/multiprocessing_bpe_encoder.py \\\n",
    "    --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "    --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "    --inputs ../data/output/multi/nodes.txt \\\n",
    "    --outputs ../data/output/multi/nodes.bpe \\\n",
    "    --keep-empty \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d49e2d02a6433",
   "metadata": {},
   "source": [
    "2. Do negative sampling and dump the whole training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84b4c9cc932ab3f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:05:31.394911Z",
     "start_time": "2024-04-21T18:05:28.270520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-21 20:30:01.198337 load finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sougata/projects/MyKEPLER/tram2kepler/scripts/../../examples/KEPLER/Pretrain/KGpreprocess.py\", line 125, in <module>\n",
      "    os.mkdir(args.dumpPath)\n",
      "FileExistsError: [Errno 17] File exists: '../data/output/multi/KE1'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\npython ../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath ../data/output/multi/KE1 \\\\\\n    -ns 1 \\\\\\n    --ent_desc ../data/output/multi/nodes.bpe \\\\\\n    --train ../data/output/multi/train.txt \\\\\\n    --valid ../data/output/multi/valid.txt\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbash\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mpython ../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath ../data/output/multi/KE1 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    -ns 1 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    --ent_desc ../data/output/multi/nodes.bpe \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    --train ../data/output/multi/train.txt \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    --valid ../data/output/multi/valid.txt\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001B[0m, in \u001B[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001B[0;34m(line, cell)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     line \u001B[38;5;241m=\u001B[39m script\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshebang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001B[0m, in \u001B[0;36mScriptMagics.shebang\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mraise_error \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001B[39;00m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001B[39;00m\n\u001B[1;32m    313\u001B[0m     \u001B[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     rc \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m--> 315\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(rc, cell)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command 'b'\\npython ../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath ../data/output/multi/KE1 \\\\\\n    -ns 1 \\\\\\n    --ent_desc ../data/output/multi/nodes.bpe \\\\\\n    --train ../data/output/multi/train.txt \\\\\\n    --valid ../data/output/multi/valid.txt\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python ../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath ../data/output/multi/KE1 \\\n",
    "    -ns 1 \\\n",
    "    --ent_desc ../data/output/multi/nodes.bpe \\\n",
    "    --train ../data/output/multi/train.txt \\\n",
    "    --valid ../data/output/multi/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd1faad355263",
   "metadata": {},
   "source": [
    "3. then randomly split the KE training data into smaller parts and the number of training instances in each part aligns with the MLM training data\n",
    "For our case it will be just one split, since our data is small.\n",
    "\n",
    "Question: what does the negative_sampling_size = 1 do? it could be that the relation triples are false.\n",
    "Ans: it has 1 to 1 size of negative sampling, so for each positive triple, there is one negative triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81740bca42331380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:06:01.056909Z",
     "start_time": "2024-04-21T18:06:00.702697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data will be splited into 1 splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sougata/projects/MyKEPLER/tram2kepler/scripts/../../examples/KEPLER/Pretrain/splitDump.py\", line 36, in <module>\n",
      "    os.mkdir(nPath)\n",
      "FileExistsError: [Errno 17] File exists: '../data/output/multi/KE1_0'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\\n\\npython ../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/multi/KE1 \\\\\\n    --split_size 6834352 \\\\\\n    --negative_sampling_size 1\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbash\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mpython ../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/multi/KE1 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    --split_size 6834352 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    --negative_sampling_size 1\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001B[0m, in \u001B[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001B[0;34m(line, cell)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     line \u001B[38;5;241m=\u001B[39m script\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshebang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001B[0m, in \u001B[0;36mScriptMagics.shebang\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mraise_error \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001B[39;00m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001B[39;00m\n\u001B[1;32m    313\u001B[0m     \u001B[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     rc \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m--> 315\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(rc, cell)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command 'b'\\nexport LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\\n\\npython ../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/multi/KE1 \\\\\\n    --split_size 6834352 \\\\\\n    --negative_sampling_size 1\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python ../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/multi/KE1 \\\n",
    "    --split_size 6834352 \\\n",
    "    --negative_sampling_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "443c0140c262b304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:08:37.828493Z",
     "start_time": "2024-04-21T18:08:14.748647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/head/train.bpe', validpref='../data/output/multi/KE1_0/head/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/head', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/train.bpe: 24675 sents, 954690 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/valid.bpe: 1299 sents, 50086 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/head\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/tail/train.bpe', validpref='../data/output/multi/KE1_0/tail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/tail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/train.bpe: 24675 sents, 305235 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/valid.bpe: 1299 sents, 15970 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/tail\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negHead/train.bpe', validpref='../data/output/multi/KE1_0/negHead/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negHead', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/train.bpe: 24675 sents, 979241 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/valid.bpe: 1299 sents, 52476 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negHead\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negTail/train.bpe', validpref='../data/output/multi/KE1_0/negTail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negTail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/train.bpe: 24675 sents, 986859 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/valid.bpe: 1299 sents, 49332 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negTail\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# wget -O ../data/gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "KE_Data=../data/output/multi/KE1_0/\n",
    "for SPLIT in head tail negHead negTail;\n",
    "  do\n",
    "    python -m fairseq_cli.preprocess \\\n",
    "      --only-source \\\n",
    "      --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "      --trainpref ${KE_Data}${SPLIT}/train.bpe \\\n",
    "      --validpref ${KE_Data}${SPLIT}/valid.bpe \\\n",
    "      --destdir ${KE_Data}${SPLIT} \\\n",
    "      --workers 60; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b51d97ba40a524",
   "metadata": {},
   "source": [
    "4. We then binarize them for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf539eb5ebea5814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:09:30.801792Z",
     "start_time": "2024-04-21T18:09:08.712651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/head/train.bpe', validpref='../data/output/multi/KE1_0/head/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/head', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/train.bpe: 24675 sents, 954690 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/valid.bpe: 1299 sents, 50086 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/head\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/tail/train.bpe', validpref='../data/output/multi/KE1_0/tail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/tail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/train.bpe: 24675 sents, 305235 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/valid.bpe: 1299 sents, 15970 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/tail\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negHead/train.bpe', validpref='../data/output/multi/KE1_0/negHead/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negHead', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/train.bpe: 24675 sents, 979241 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/valid.bpe: 1299 sents, 52476 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negHead\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negTail/train.bpe', validpref='../data/output/multi/KE1_0/negTail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negTail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/train.bpe: 24675 sents, 986859 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/valid.bpe: 1299 sents, 49332 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negTail\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# wget -O ../data/gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "KE_Data=../data/output/multi/KE1_0/\n",
    "for SPLIT in head tail negHead negTail;\n",
    "  do\n",
    "    python -m fairseq_cli.preprocess \\\n",
    "      --only-source \\\n",
    "      --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "      --trainpref ${KE_Data}${SPLIT}/train.bpe \\\n",
    "      --validpref ${KE_Data}${SPLIT}/valid.bpe \\\n",
    "      --destdir ${KE_Data}${SPLIT} \\\n",
    "      --workers 60; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b26c9ea21a1c7b",
   "metadata": {},
   "source": [
    "We now start with MLM data preprocessing:\n",
    "\n",
    "\n",
    "1. Now we encode the nodes_train, nodes_train and nodes_valid with the GPT-2 BPE:\n",
    "   (gpt2_bpe is already downloaded during the KE data preparation, we reuse that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27f45e3f3e087a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:12:10.415575Z",
     "start_time": "2024-04-21T18:11:58.982707Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 10000 lines\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir -p ../data/output/multi/MLM\n",
    "\n",
    "for SPLIT in train valid test; do \\\n",
    "    python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "        --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "        --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "        --inputs ../data/output/multi/nodes_${SPLIT}.txt \\\n",
    "        --outputs ../data/output/multi/MLM/nodes_${SPLIT}.bpe \\\n",
    "        --keep-empty \\\n",
    "        --workers 60; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce4d7c38cf0c7f",
   "metadata": {},
   "source": [
    "2. We then preprocess/binarize the data using the GPT-2 fairseq dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "194dc596ddddd599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:13:21.109728Z",
     "start_time": "2024-04-21T18:13:14.268878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/MLM/nodes_train.bpe', validpref='../data/output/multi/MLM/nodes_valid.bpe', testpref='../data/output/multi/MLM/nodes_test.bpe', destdir='../data/output/multi/MLM-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/MLM/nodes_train.bpe: 14727 sents, 586783 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/MLM/nodes_valid.bpe: 776 sents, 28915 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/MLM/nodes_test.bpe: 3876 sents, 144516 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/MLM-bin\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir -p ../data/output/multi/MLM-bin\n",
    "\n",
    "python -m fairseq_cli.preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "    --trainpref ../data/output/multi/MLM/nodes_train.bpe \\\n",
    "    --validpref ../data/output/multi/MLM/nodes_valid.bpe \\\n",
    "    --testpref ../data/output/multi/MLM/nodes_test.bpe \\\n",
    "    --destdir ../data/output/multi/MLM-bin \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3636e1cd925de4",
   "metadata": {},
   "source": [
    "All preprocessing is done, now we try out training the model with our data.\n",
    "\n",
    "Then we first train on the NLP model, since the kepler NLP pretrained model checkpoint is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43998fc0daaf9b39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:18:50.227779Z",
     "start_time": "2024-04-21T18:18:25.197513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/multi/’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1, log_format='simple', tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_init_scale=2, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='MLMetKE', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', task='MLMetKE', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=3, required_batch_size_multiple=1, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, max_tokens_valid=None, max_sentences_valid=3, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=23456, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, arch='roberta_base', max_epoch=0, max_update=0, clip_norm=0.0, sentence_avg=False, update_freq=[784], lr=[0.0006], min_lr=-1, use_bmuf=False, save_dir='../data/multi/checkpoints', restore_file='../data/keplerModels/KEPLERforNLP.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=10000, end_learning_rate=0.0, power=1.0, total_num_update=125000, data='../data/output/multi/MLM-bin', KEdata='../data/output/multi/KE1_0:', KEdata2='', sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, freq_weighted_replacement=False, mask_whole_words=False, negative_sample_size=1, ke_model='TransE', ke_head_name='wikiData', ke_head_name2='wordnet', init_token=0, separator_token=2, gamma=4.0, gamma2=12.0, nrelation=822, nrelation2=20, relation_desc=False, double_ke=False, relemb_from_desc=False, dropout=0.1, attention_dropout=0.1, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_fn='gelu', pooler_activation_fn='tanh', activation_dropout=0.0, pooler_dropout=0.0)\n",
      "| dictionary: 50264 types\n",
      "| loaded 776 examples from: ../data/output/multi/MLM-bin/valid\n",
      "| NO mask whold words\n",
      "| loaded 1299 examples from: ../data/output/multi/KE1_0/head/valid\n",
      "| loaded 1299 examples from: ../data/output/multi/KE1_0/tail/valid\n",
      "| loaded 1299 examples from: ../data/output/multi/KE1_0/negHead/valid\n",
      "| loaded 1299 examples from: ../data/output/multi/KE1_0/negTail/valid\n",
      "| loaded 1299 examples from: ../data/output/multi/KE1_0/head/valid\n",
      "| loaded 1299 examples from: ../data/output/multi/KE1_0/tail/valid\n",
      "MLMdata 57 KEdata 1299\n",
      "RobertaModel(\n",
      "  (decoder): RobertaEncoder(\n",
      "    (sentence_encoder): TransformerSentenceEncoder(\n",
      "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
      "      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x TransformerSentenceEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): RobertaLMHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_heads): ModuleDict()\n",
      "  (ke_heads): ModuleDict(\n",
      "    (wikiData): RobertaKnowledgeEmbeddingHead(\n",
      "      (relation_emb): Embedding(822, 768)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| model roberta_base, criterion MLMetKELoss\n",
      "| num. model params: 125327963 (num. trained: 125327961)\n",
      "| training on 1 GPUs\n",
      "| max tokens per GPU = None and max sentences per GPU = 3\n",
      "checkpoint_path: ../data/keplerModels/KEPLERforNLP.pt\n",
      "| loaded checkpoint ../data/keplerModels/KEPLERforNLP.pt (epoch 10 @ 0 updates)\n",
      "| loading train data for epoch 10\n",
      "| loaded 14727 examples from: ../data/output/multi/MLM-bin/train\n",
      "| NO mask whold words\n",
      "| loaded 24675 examples from: ../data/output/multi/KE1_0/head/train\n",
      "| loaded 24675 examples from: ../data/output/multi/KE1_0/tail/train\n",
      "| loaded 24675 examples from: ../data/output/multi/KE1_0/negHead/train\n",
      "| loaded 24675 examples from: ../data/output/multi/KE1_0/negTail/train\n",
      "| loaded 24675 examples from: ../data/output/multi/KE1_0/head/train\n",
      "| loaded 24675 examples from: ../data/output/multi/KE1_0/tail/train\n",
      "MLMdata 1154 KEdata 24675\n",
      "| WARNING: 1323 samples have invalid sizes and will be skipped, max_positions=(512, 2147483647), first few sample ids=[1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sougata/projects/MyKEPLER/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 011:      1 / 10 loss=5.596, ppl=48.38, wps=8674, ups=0, wpb=1358134.000, bsz=4704.000, num_updates=2, lr=1.2e-07, gnorm=15.587, clip=0.000, oom=0.000, loss_scale=2.000, wall=312, train_wall=67283, ke_loss=1.4716, mlm_loss=4.12465\n",
      "| epoch 011:      2 / 10 loss=5.599, ppl=48.47, wps=8735, ups=0, wpb=1361871.667, bsz=4704.000, num_updates=3, lr=1.8e-07, gnorm=15.438, clip=0.000, oom=0.000, loss_scale=2.000, wall=468, train_wall=67433, ke_loss=1.46849, mlm_loss=4.13062\n",
      "| epoch 011:      3 / 10 loss=5.596, ppl=48.38, wps=8711, ups=0, wpb=1360867.250, bsz=4704.000, num_updates=4, lr=2.4e-07, gnorm=15.226, clip=0.000, oom=0.000, loss_scale=2.000, wall=624, train_wall=67584, ke_loss=1.46462, mlm_loss=4.13178\n",
      "| epoch 011:      4 / 10 loss=5.597, ppl=48.39, wps=8704, ups=0, wpb=1361983.800, bsz=4704.000, num_updates=5, lr=3e-07, gnorm=15.353, clip=0.000, oom=0.000, loss_scale=2.000, wall=782, train_wall=67736, ke_loss=1.46802, mlm_loss=4.12863\n",
      "| epoch 011:      5 / 10 loss=5.594, ppl=48.31, wps=8716, ups=0, wpb=1361853.167, bsz=4704.000, num_updates=6, lr=3.6e-07, gnorm=15.245, clip=0.000, oom=0.000, loss_scale=2.000, wall=937, train_wall=67886, ke_loss=1.46742, mlm_loss=4.12696\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir ../data/checkpoints/multi/\n",
    "\n",
    "TOTAL_UPDATES=125000                                    # Total number of training steps\n",
    "WARMUP_UPDATES=10000                                    # Warmup the learning rate over this many updates\n",
    "LR=6e-04                                                # Peak LR for polynomial LR scheduler.\n",
    "NUM_CLASSES=2                           \n",
    "MAX_SENTENCES=3                                         # Batch size.\n",
    "NUM_NODES=1\t\t\t                                    # Number of machines\n",
    "ROBERTA_PATH=../data/keplerModels/KEPLERforNLP.pt       # Path to the original roberta model\n",
    "CHECKPOINT_PATH=../data/checkpoints/multi               # Directory to store the checkpoints\n",
    "UPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\n",
    "\n",
    "DATA_DIR=../data/output/multi\n",
    "\n",
    "#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\n",
    "KE_DATA=$DATA_DIR/KE1_0:\n",
    "\n",
    "DIST_SIZE=`expr $NUM_NODES`\n",
    "\n",
    "python -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\n",
    "        --save-dir $CHECKPOINT_PATH \\\n",
    "        --max-sentences $MAX_SENTENCES \\\n",
    "        --tokens-per-sample 512 \\\n",
    "        --task MLMetKE \\\n",
    "        --sample-break-mode complete \\\n",
    "        --required-batch-size-multiple 1 \\\n",
    "        --arch roberta_base \\\n",
    "        --criterion MLMetKE \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
    "        --clip-norm 0.0 \\\n",
    "        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
    "        --update-freq \"$UPDATE_FREQ\" \\\n",
    "        --negative-sample-size 1 --ke-model TransE \\\n",
    "        --init-token 0 \\\n",
    "        --separator-token 2 \\\n",
    "        --gamma 4 --nrelation 822 \\\n",
    "        --skip-invalid-size-inputs-valid-test \\\n",
    "        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
    "        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\n",
    "        --log-format simple --log-interval 1 > out_multi.log \\\n",
    "        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate model",
   "id": "f9b2983b5d41bde6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da575c9d0abd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m fairseq-eval-lm ../data/output/multi/MLM-bin \\\n",
    "    --path ../data/checkpoints/multi/checkpoint_best.pt \\\n",
    "    --sample-break-mode complete --max-tokens 3072 \\\n",
    "    --context-window 2560 --softmax-batch 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
