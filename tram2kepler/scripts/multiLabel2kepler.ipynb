{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dd236a9073b36eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.461215Z",
     "start_time": "2024-04-21T17:28:07.082068Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a67b34b706edd68",
   "metadata": {},
   "source": [
    "Get tram multi label data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df936a45a30f3332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.702131Z",
     "start_time": "2024-04-21T17:28:08.463055Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-21 22:04:32--  https://raw.githubusercontent.com/center-for-threat-informed-defense/tram/main/data/tram2-data/multi_label.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5846167 (5.6M) [text/plain]\n",
      "Saving to: ‘../data/input/multi_label.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 80.9M 0s\n",
      "    50K .......... .......... .......... .......... ..........  1% 80.0M 0s\n",
      "   100K .......... .......... .......... .......... ..........  2%  119M 0s\n",
      "   150K .......... .......... .......... .......... ..........  3%  227M 0s\n",
      "   200K .......... .......... .......... .......... ..........  4%  154M 0s\n",
      "   250K .......... .......... .......... .......... ..........  5%  328M 0s\n",
      "   300K .......... .......... .......... .......... ..........  6% 85.1M 0s\n",
      "   350K .......... .......... .......... .......... ..........  7%  350M 0s\n",
      "   400K .......... .......... .......... .......... ..........  7%  288M 0s\n",
      "   450K .......... .......... .......... .......... ..........  8%  341M 0s\n",
      "   500K .......... .......... .......... .......... ..........  9%  303M 0s\n",
      "   550K .......... .......... .......... .......... .......... 10%  370M 0s\n",
      "   600K .......... .......... .......... .......... .......... 11%  291M 0s\n",
      "   650K .......... .......... .......... .......... .......... 12%  302M 0s\n",
      "   700K .......... .......... .......... .......... .......... 13%  318M 0s\n",
      "   750K .......... .......... .......... .......... .......... 14%  323M 0s\n",
      "   800K .......... .......... .......... .......... .......... 14%  263M 0s\n",
      "   850K .......... .......... .......... .......... .......... 15%  328M 0s\n",
      "   900K .......... .......... .......... .......... .......... 16%  311M 0s\n",
      "   950K .......... .......... .......... .......... .......... 17%  318M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 18%  280M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 19%  278M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 20%  314M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 21%  329M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 21%  260M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 22%  348M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 23%  379M 0s\n",
      "  1350K .......... .......... .......... .......... .......... 24%  319M 0s\n",
      "  1400K .......... .......... .......... .......... .......... 25%  307M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 26%  325M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 27%  311M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 28%  321M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 28%  259M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 29%  319M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 30%  349M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 31%  356M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 32%  324M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 33%  335M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 34%  311M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 35%  345M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 35%  296M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 36%  342M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 37%  321M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 38%  387M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 39%  329M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 40%  350M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 41%  304M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 42%  345M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 42%  270M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 43%  337M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 44%  365M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 45%  359M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 46%  340M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 47%  353M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 48%  373M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 49%  363M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 49%  282M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 50%  379M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 51%  372M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 52%  390M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 53%  363M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 54%  286M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 55%  381M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 56%  375M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 56%  370M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 57%  347M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 58%  361M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 59%  364M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 60%  376M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 61%  306M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 62%  371M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 63%  348M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 63%  365M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 64%  351M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 65%  366M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 66%  386M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 67%  319M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 68%  294M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 69%  361M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 70%  372M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 70%  363M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 71%  314M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 72%  378M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 73%  369M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 74%  367M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 75%  305M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 76%  348M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 77%  375M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 77%  383M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 78%  354M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 79%  382M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 80%  372M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 81%  379M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 82%  300M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 83%  357M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 84%  368M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 84%  368M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 85%  357M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 86%  382M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 87%  387M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 88%  359M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 89%  290M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 90%  378M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 91%  382M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 91%  384M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 92%  330M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 93%  381M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 94%  370M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 95%  376M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 96%  302M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 97%  358M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 98%  363M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 98%  350M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 99%  342M 0s\n",
      "  5700K .........                                             100%  445M=0.02s\n",
      "\n",
      "2024-04-21 22:04:32 (304 MB/s) - ‘../data/input/multi_label.json’ saved [5846167/5846167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O ../data/input/multi_label.json https://raw.githubusercontent.com/center-for-threat-informed-defense/tram/main/data/tram2-data/multi_label.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e5d5fad821ac9",
   "metadata": {},
   "source": [
    "In this version, we will consider the sentence, techniques and document title, all 3 of them as nodes.\n",
    "The ontology then will be:\n",
    "\n",
    "Nodes: \n",
    "    sentence, technique, doc_title\n",
    "    \n",
    "Relationships: \n",
    "    uses, used-in, found-in\n",
    "\n",
    "Graph triple types will be:\n",
    "    sentence uses technique\n",
    "    sentence found-in doc_title\n",
    "    technique used-in doc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3da462024125ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.789908Z",
     "start_time": "2024-04-21T17:28:08.703106Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('../data/input/multi_label.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c94625d2be17d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.804611Z",
     "start_time": "2024-04-21T17:28:08.791649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "      <th>doc_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title: NotPetya Technical Analysis – A Triple ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Executive Summary This technical analysis prov...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For more information on CrowdStrike’s proactiv...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NotPetya combines ransomware with the ability ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It spreads to Microsoft Windows machines using...</td>\n",
       "      <td>[T1210]</td>\n",
       "      <td>NotPetya Technical Analysis  A Triple Threat F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19173</th>\n",
       "      <td>[2] Eclypsium Blog - TrickBot Now Offers 'Tric...</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19174</th>\n",
       "      <td>Initial Version March 24, 2021:</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19175</th>\n",
       "      <td>Added MITRE ATT&amp;CK Technique T1592.003 used fo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19176</th>\n",
       "      <td>Added new MITRE ATT&amp;CKs and updated Table 1</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19177</th>\n",
       "      <td>This product is provided subject to this Notif...</td>\n",
       "      <td>[]</td>\n",
       "      <td>AA21076A TrickBot Malware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19178 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence   labels  \\\n",
       "0      title: NotPetya Technical Analysis – A Triple ...       []   \n",
       "1      Executive Summary This technical analysis prov...       []   \n",
       "2      For more information on CrowdStrike’s proactiv...       []   \n",
       "3      NotPetya combines ransomware with the ability ...       []   \n",
       "4      It spreads to Microsoft Windows machines using...  [T1210]   \n",
       "...                                                  ...      ...   \n",
       "19173  [2] Eclypsium Blog - TrickBot Now Offers 'Tric...       []   \n",
       "19174                    Initial Version March 24, 2021:       []   \n",
       "19175  Added MITRE ATT&CK Technique T1592.003 used fo...       []   \n",
       "19176       Added new MITRE ATT&CKs and updated Table 1        []   \n",
       "19177  This product is provided subject to this Notif...       []   \n",
       "\n",
       "                                               doc_title  \n",
       "0      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "1      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "2      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "3      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "4      NotPetya Technical Analysis  A Triple Threat F...  \n",
       "...                                                  ...  \n",
       "19173                          AA21076A TrickBot Malware  \n",
       "19174                          AA21076A TrickBot Malware  \n",
       "19175                          AA21076A TrickBot Malware  \n",
       "19176                          AA21076A TrickBot Malware  \n",
       "19177                          AA21076A TrickBot Malware  \n",
       "\n",
       "[19178 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95cc7a91fb9f1d",
   "metadata": {},
   "source": [
    "Getting all unique labels, doc_titles and text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef72a6e2489defcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.811276Z",
     "start_time": "2024-04-21T17:28:08.805605Z"
    }
   },
   "outputs": [],
   "source": [
    "all_techniques = data['labels'].explode().dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc9ca190617b0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.815262Z",
     "start_time": "2024-04-21T17:28:08.812248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1210', 'T1570', 'T1140', 'T1059.003', 'T1218.011', 'T1057',\n",
       "       'T1518.001', 'T1106', 'T1003.001', 'T1082', 'T1016', 'T1078',\n",
       "       'T1047', 'T1027', 'T1056.001', 'T1083', 'T1053.005', 'T1484.001',\n",
       "       'T1005', 'T1055', 'T1204.002', 'T1574.002', 'T1071.001', 'T1090',\n",
       "       'T1105', 'T1070.004', 'T1562.001', 'T1033', 'T1219', 'T1547.001',\n",
       "       'T1566.001', 'T1021.001', 'T1543.003', 'T1569.002', 'T1036.005',\n",
       "       'T1112', 'T1041', 'T1110', 'T1190', 'T1113', 'T1564.001', 'T1012',\n",
       "       'T1573.001', 'T1095', 'T1552.001', 'T1074.001', 'T1548.002',\n",
       "       'T1068', 'T1072', 'T1557.001'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c97dce368bc366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.819157Z",
     "start_time": "2024-04-21T17:28:08.816200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6b1072a23019e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.828856Z",
     "start_time": "2024-04-21T17:28:08.820089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NotPetya Technical Analysis  A Triple Threat File Encryption MFT Encryption Credential Theft',\n",
       "       'Earth Zhulong Familiar Patterns Target Southeast Asian Firms',\n",
       "       'Malware Spotlight Camaro Dragons TinyNote Backdoor',\n",
       "       'Rorschach  A New Sophisticated and Fast Ransomware  Check Point Research',\n",
       "       'Bypassing Intel CET with Counterfeit Objects  OffSec',\n",
       "       'Emotet Strikes Again  LNK File Leads to Domain Wide Ransomware  The DFIR Report',\n",
       "       'Malware Analysis LummaC2 Stealer',\n",
       "       'FedEx Phishing Campaign Abusing TrustedForm and PAAY',\n",
       "       'Take a NetWalk on the Wild Side',\n",
       "       'Malicious OAuth applications used to compromise email servers and spread spam  Microsoft Security Blog',\n",
       "       'Nefilim Ransomware',\n",
       "       'Deja Vu All Over Again Tax Scammers at Large',\n",
       "       'Threat Assessment Black Basta Ransomware',\n",
       "       'Hafniuminspired cyberattacks neutralized by AI',\n",
       "       'eSentire Threat Intelligence Malware Analysis BatLoader',\n",
       "       'Early Bird Catches the Wormhole Observations from the StellarParticle Campaign',\n",
       "       '3CXDesktopApp Backdoored in a Suspected Lazarus Campaign\\xa0\\xa0',\n",
       "       'Not just an infostealer Gopuram backdoor deployed through 3CX supply chain attack',\n",
       "       'AA21200A Tactics Techniques and Procedures of Indicted APT40 Actors Associated with Chinas MSS Hainan State Security Department',\n",
       "       'Operation Spalax Targeted malware attacks in Colombia',\n",
       "       'LAPSUS Recent techniques tactics and procedures',\n",
       "       'Detecting Credential Stealing Attacks Through Active InNetwork Defense',\n",
       "       'Understanding DNS attacks Identifying and patching vulnerabilities  Snyk',\n",
       "       'EvilExtractor  AllinOne Stealer',\n",
       "       'LockBit 20 How This RaaS Operates and How to Protect Against It',\n",
       "       'SOC Team Essentials  How to Investigate and Track the 8220 Gang Cloud Threat',\n",
       "       'Fantasy  a new Agrius wiper deployed through a supplychain attack',\n",
       "       'AA22320A Iranian GovernmentSponsored APT Actors Compromise Federal Network Deploy Crypto Miner Credential Harvester',\n",
       "       'New Horabot campaign targets the Americas',\n",
       "       'Vice Society leverages PrintNightmare in ransomware attacks',\n",
       "       'UNC215 Spotlight on a Chinese Espionage Campaign in Israel',\n",
       "       'Dark Web Profile MuddyWater APT Group',\n",
       "       'Cobalt Strike a Defenders Guide  Part 2',\n",
       "       'Tailoring Sandbox Techniques to Hidden Threats',\n",
       "       'Babadeda Crypter targeting crypto NFT and DeFi communities',\n",
       "       'Threat Assessment BlackCat Ransomware',\n",
       "       'AA21200B Chinese StateSponsored Cyber Operations Observed TTPs',\n",
       "       'Vulnerability in Essential Addons for Elementor Leads to Mass Infection',\n",
       "       'StopRansomware Royal Ransomware',\n",
       "       'GuLoader VBScript Variant Returns with PowerShell Updates',\n",
       "       'Xollam the Latest Face of TargetCompany',\n",
       "       'Operation Tainted Love  Chinese APTs Target Telcos in New Attacks',\n",
       "       'Defending Users NAS Devices From Evolving Threats',\n",
       "       'Enigma Stealer Targets Cryptocurrency Industry with Fake Jobs',\n",
       "       'ExConti and FIN7 Actors Collaborate with New Domino Backdoor',\n",
       "       'Fat Cats',\n",
       "       'Analysis on recent wiper attacks examples and how wiper malware works',\n",
       "       'Higaisa or Winnti APT41 backdoors old and new',\n",
       "       'Operation Harvest A Deep Dive into a Longterm Campaign',\n",
       "       'Prilex Brazilian PoS malware evolution',\n",
       "       'The Rising Trend of OneNote Documents for Malware delivery',\n",
       "       'ZeroDay Vulnerability in MOVEit Transfer Exploited for Data Theft',\n",
       "       'Iranian GovernmentSponsored APT Actors Compromise Federal Network Deploy Crypto Miner Credential Harvester',\n",
       "       'FinSpy unseen findings',\n",
       "       'Vice Society Profiling a Persistent Threat to the Education Sector',\n",
       "       'Spike in LokiBot Activity During Final Week of 2022',\n",
       "       'Breaking Pedersen Hashes in Practice',\n",
       "       'Nationstate threat actor Mint Sandstorm refines tradecraft to attack highvalue targets',\n",
       "       'GuLoader Demystified Unraveling its Vectored Exception Handler Approach',\n",
       "       'Pack it Secretly Earth Pretas Updated Stealthy Strategies',\n",
       "       'Malware Reverse Engineering for Beginners  Part 2',\n",
       "       'Supply Chain Risk from Gigabyte App Center Backdoor  Eclypsium  Supply Chain Security for the Modern Enterprise',\n",
       "       'Gotta Catch Em All  Understanding the NetSupport RAT Campaigns Hiding Behind Pokemon Lures',\n",
       "       'Uncommon infection methodspart 2',\n",
       "       'German users targeted with Gootkit banker or REvil ransomware',\n",
       "       'New IcedID variants shift from bank fraud to malware delivery',\n",
       "       'BumbleBee Roasts Its Way to Domain Admin',\n",
       "       'Operation CMDStealer Financially Motivated Campaign Leverages CMDBased Scripts and LOLBaS for Online Banking Theft in Portugal Peru and Mexico',\n",
       "       'AA20336A Advanced Persistent Threat Actors Targeting US Think Tanks',\n",
       "       'Do Not Cross The RedLine Stealer Detections and Analysis',\n",
       "       'Conti Team One Splinter Group Resurfaces as Royal Ransomware with Callback Phishing Attacks',\n",
       "       'Threat Advisory 3CX Softphone Supply Chain Compromise',\n",
       "       'Malware Disguised as Document from Ukraines Energoatom Delivers Havoc Demon Backdoor',\n",
       "       'In the footsteps of the Fancy Bear PowerPoint\\xa0mouseover event abused to deliver Graphite implants',\n",
       "       'Can You See It Now An Emerging LockBit Campaign',\n",
       "       'WTB Remote Mac Exploitation Via Custom URL Schemes',\n",
       "       'Dead or Alive An Emotet Story', 'Conti Ransomware',\n",
       "       'Microsoft research uncovers new Zerobot capabilities  Microsoft Security Blog',\n",
       "       'Just Because Its Old Doesnt Mean You Throw It Away Including Malware',\n",
       "       'Carbon Blacks TrueBot Detection',\n",
       "       'ITG10 Likely Targeting South Korean Entities of Interest to the Democratic Peoples Republic of Korea DPRK',\n",
       "       'Abusing cloud services to fly under the radar',\n",
       "       'Transparent Tribe APT36  PakistanAligned Threat Actor Expands Interest in Indian Education Sector',\n",
       "       'SYS01 Stealer Will Steal Your Facebook Info',\n",
       "       'Banking Trojan Techniques How Financially Motivated Malware Became Infrastructure',\n",
       "       'Tracking Traces of Malware Disguised as Hancom Office Document File and Being Distributed RedEyes',\n",
       "       'Attackers use domain fronting technique to target Myanmar with Cobalt Strike',\n",
       "       'Akira Ransomware is bringin 1988 back',\n",
       "       'WTB Adwind Trojan Circumvents Antivirus Software To Infect Your PC',\n",
       "       'Hungry for data ModPipe backdoor hits POS software used in hospitality sector',\n",
       "       'Update 2 3CX users under DLLsideloading attack What you need to know',\n",
       "       'GoBruteforcer GolangBased Botnet Actively Harvests Web Servers',\n",
       "       'The LockBit ransomware kinda comes for macOS',\n",
       "       'IronNetInjector Turlas New Malware Loading Tool',\n",
       "       'Dissecting One of APT29s Fileless WMI and PowerShell Backdoors POSHSPY',\n",
       "       'Hancitor Infection Chain Analysis An Examination of its Unpacking Routine and Execution Techniques',\n",
       "       'Smoking Out a DARKSIDE Affiliates Supply Chain Software Compromise',\n",
       "       'AA20258A Chinese Ministry of State SecurityAffiliated Cyber Threat Actor Activity',\n",
       "       'Phishing Campaign Targets Chinese Nuclear Energy Industry',\n",
       "       'Latin American Governments Targeted By Ransomware',\n",
       "       'Chinese Threat Actor Used Modified Cobalt Strike Variant to Attack Taiwanese Critical Infrastructure',\n",
       "       'OpenSource Gh0st RAT Still Haunting Inboxes 15 Years After Release',\n",
       "       'CrowdStrike Uncovers I2Pminer MacOS Mineware Variant',\n",
       "       'Evasive NoEscape Ransomware Uses Reflective DLL Injection',\n",
       "       'Stolen certificates in two waves of ransomware and wiper attacks',\n",
       "       'BlueNoroff introduces new methods bypassing MoTW',\n",
       "       'Fork in the Ice The New Era of IcedID',\n",
       "       'Kimsuky Strikes Again  New Social Engineering Campaign Aims to Steal Credentials and Gather Strategic Intelligence',\n",
       "       'Warning New attack campaign utilized a new 0day RCE vulnerability on Microsoft Exchange Server',\n",
       "       'Recent TZW Campaigns Revealed As Part of GlobeImposter Malware Family',\n",
       "       'MERCURY and DEV1084 Destructive attack on hybrid environment',\n",
       "       'ZipJar a little bit unexpected attack chain',\n",
       "       'Whos swimming in South Korean waters Meet ScarCrufts Dolphin',\n",
       "       'Iron Tigers SysUpdate Reappears Adds Linux Targeting',\n",
       "       'Unwrapping Ursnifs Gifts  The DFIR Report',\n",
       "       'Qakbot Returns to ISO Delivery For Now',\n",
       "       'BeeWare of Trigona An Emerging Ransomware Strain',\n",
       "       'SharpPanda APT Campaign Expands its Arsenal Targeting G20 Nations',\n",
       "       'Increasing The Sting of HIVE Ransomware',\n",
       "       'ViperSoftX Updates Encryption Steals Data',\n",
       "       'CISA Red Team Shares Key Findings to Improve Monitoring and Hardening of Networks',\n",
       "       'McAfee Defenders Blog NetWalker',\n",
       "       'Technical Analysis Black Basta Malware Overview',\n",
       "       'Earth Pretas Cyberespionage Campaign Hits Over 200',\n",
       "       'Updated New Evidence Emerges to Suggest WatchDog Was Behind Crypto Campaign',\n",
       "       'These arent the apps youre looking for fake installers targeting Southeast and East Asia',\n",
       "       'Tax firms targeted by precision malware attacks',\n",
       "       'BazarLoader Mocks Researchers in December 2020 Malspam Campaign',\n",
       "       'Investigation with a twist an accidental APT attack and averted data destruction',\n",
       "       'Threat actors strive to cause Tax Day headaches',\n",
       "       'Revisiting the NSISbased crypter',\n",
       "       '\\xa0LockBit Ransomware 20 Resurfaces',\n",
       "       'Too Log Didnt Read  Unknown Actor Using CLFS Log Files for Stealth',\n",
       "       'Ransom Cartel Ransomware A Possible Connection With REvil',\n",
       "       'Malicious ISO File Leads to Domain Wide Ransomware  The DFIR Report',\n",
       "       'SeroXen RAT for sale',\n",
       "       'A lookback under the TA410 umbrella Its cyberespionage TTPs and activity',\n",
       "       'When byte code bites Who checks the contents of compiled Python files',\n",
       "       'MoonBounce the dark side of UEFI firmware',\n",
       "       'Threat Actors Use MSBuild to Deliver RATs Filelessly',\n",
       "       'How to Detect Cobalt Strike',\n",
       "       'New RapperBot Campaign  We Know What You Bruting for this Time',\n",
       "       'Inside the Mind of a Cyber Attacker from Malware creation to Data Exfiltration Part 1',\n",
       "       'CatB Ransomware  File Locker Sharpens Its Claws to Steal Data with MSDTC Service DLL Hijacking',\n",
       "       'Analyzing Solorigate the compromised DLL file that started a sophisticated cyberattack and how Microsoft Defender helps protect customers',\n",
       "       'Horabot campaign targeted businesses for more than two years before finally being discovered',\n",
       "       'StopRansomware Hive Ransomware',\n",
       "       'Linux malware strengthens links between Lazarus and the 3CX supplychain attack',\n",
       "       'Tomiris called they want their Turla malware back',\n",
       "       'AA21076A TrickBot Malware'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_titles = data['doc_title'].explode().dropna().unique()\n",
    "doc_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d2c4ecea9d3414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.832431Z",
     "start_time": "2024-04-21T17:28:08.829734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "180ba8e04e31f7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.836607Z",
     "start_time": "2024-04-21T17:28:08.834458Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = data['sentence'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1513a864623c9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.840769Z",
     "start_time": "2024-04-21T17:28:08.837572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19178,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ac99a1660728ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.844422Z",
     "start_time": "2024-04-21T17:28:08.841626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['title: NotPetya Technical Analysis – A Triple Threat: File Encryption, MFT Encryption, Credential Theft url: https://www.crowdstrike.com/blog/petrwrap-ransomware-technical-analysis-triple-threat-file-encryption-mft-encryption-credential-theft/ Update: Due to naming convention consistency in the industry, CrowdStrike is now calling this variant of Petya – NotPetya. ',\n",
       "       'Executive Summary This technical analysis provides an in-depth analysis and review of NotPetya. ',\n",
       "       'For more information on CrowdStrike’s proactive protection features see the earlier CrowdStrike blog on how Falcon Endpoint Protection prevents the NotPetya attack. ',\n",
       "       ...,\n",
       "       'Added MITRE ATT&CK Technique T1592.003 used for reconnaissance May 20, 2021:',\n",
       "       'Added new MITRE ATT&CKs and updated Table 1 ',\n",
       "       'This product is provided subject to this Notification and this Privacy & Use policy. '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6f61357ac1847",
   "metadata": {},
   "source": [
    "Adding them all in one place labels, text, doc_titles: there are 50 labels, 151 doc_titles and 19178 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edeebd7d5321bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.848749Z",
     "start_time": "2024-04-21T17:28:08.845326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T1210', 'T1570', 'T1140', ...,\n",
       "       'Added MITRE ATT&CK Technique T1592.003 used for reconnaissance May 20, 2021:',\n",
       "       'Added new MITRE ATT&CKs and updated Table 1 ',\n",
       "       'This product is provided subject to this Notification and this Privacy & Use policy. '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = np.concatenate((all_techniques, doc_titles, sentence))\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fa738d1c1fb9f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.852501Z",
     "start_time": "2024-04-21T17:28:08.849825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19379,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "243bdc03669e1218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:28:08.855733Z",
     "start_time": "2024-04-21T17:28:08.853440Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(nodes) == len(all_techniques) + len(doc_titles) + len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407ac10c440a991",
   "metadata": {},
   "source": [
    "The node list will then have \n",
    "    0-49 techniques\n",
    "    50-200 doc_titles \n",
    "    201-19378 text\n",
    "    \n",
    "Now to make the numeric triples, we will use the indexes of the nodes from the nodes list.\n",
    "\n",
    "\n",
    "Let us say that of the two relationships, uses = 0 and used-in = 1, found-in = 2\n",
    "\n",
    "1. we make the triples for sentence uses technique\n",
    "2. we make the triples for sentence found-in doc_title\n",
    "3. we make the triples for technique used-in doc_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad595dd2f9eaf7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:40:56.929395Z",
     "start_time": "2024-04-21T17:40:43.588312Z"
    }
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "tech2doc = []\n",
    "\n",
    "np_data = data.to_numpy()\n",
    "\n",
    "for row in np_data:\n",
    "    sentence_index = np.where(nodes == row[0])[0][0]\n",
    "    technique_indices = []\n",
    "    technique_indices.extend([np.where(nodes == technique)[0][0] for technique in row[1]])\n",
    "    doc_title_index = np.where(nodes == row[2])[0][0]\n",
    "    \n",
    "    triples.append((sentence_index, 2, doc_title_index))\n",
    "\n",
    "    if technique_indices:\n",
    "        triples.extend([(sentence_index, 0, technique_index) for technique_index in technique_indices])\n",
    "    \n",
    "    tech2doc.extend([(technique_index, 1, doc_title_index) for technique_index in technique_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d23cd55881237c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:42:24.556784Z",
     "start_time": "2024-04-21T17:42:24.553329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24321"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(triples)\n",
    "# print(tech2doc)\n",
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c85831102fe74c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:41:33.737399Z",
     "start_time": "2024-04-21T17:41:33.716408Z"
    }
   },
   "outputs": [],
   "source": [
    "tech2doc = np.unique(tech2doc, axis=0)\n",
    "triples = np.array(triples)\n",
    "triples = np.append(triples, tech2doc, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9030db88c61c8f",
   "metadata": {},
   "source": [
    "split the triples into train, validation, test and save them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "655f8f83c7915c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:44:03.079668Z",
     "start_time": "2024-04-21T17:44:03.072202Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/output/multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c556e388463d82ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:44:05.687193Z",
     "start_time": "2024-04-21T17:44:05.630035Z"
    }
   },
   "outputs": [],
   "source": [
    "output = \"../data/output/multi\"\n",
    "pd.DataFrame(triples).to_csv(output + '/triples.txt', index=False, header=False, sep=' ')\n",
    "train, valid = train_test_split(triples, test_size=0.05)\n",
    "pd.DataFrame(train).to_csv(output + '/train.txt', index=False, header=False, sep=' ')\n",
    "pd.DataFrame(valid).to_csv(output + '/valid.txt', index=False, header=False, sep=' ')\n",
    "assert len(train) + len(valid) == len(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de7ad6ac7972c2",
   "metadata": {},
   "source": [
    "Also train test validation split the nodes.txt for MLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e95844c051cfd3d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:45:26.227896Z",
     "start_time": "2024-04-21T17:45:26.222531Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(file_path, _list):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _row in _list:\n",
    "            f.write(_row.replace(\"\\n\", r\"\\n\").replace(\"\\t\", r\"\\t\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7156d270cc5f8ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:45:28.746938Z",
     "start_time": "2024-04-21T17:45:28.720828Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train, n_test = train_test_split(nodes, test_size=0.2)\n",
    "\n",
    "n_train, n_valid = train_test_split(n_train, test_size=0.05)\n",
    "\n",
    "assert len(n_train) + len(n_test) + len(n_valid) == len(nodes)\n",
    "\n",
    "write_file(output + '/nodes_train.txt', n_train)\n",
    "write_file(output + '/nodes_valid.txt', n_valid)\n",
    "write_file(output + '/nodes_test.txt', n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba031ef9409d5f40",
   "metadata": {},
   "source": [
    "save the nodes to a file, this is somewhat tricky, since some of the node texts contain newline characters, and we need to preserve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cef5fb4d5612a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T17:45:39.804703Z",
     "start_time": "2024-04-21T17:45:39.786409Z"
    }
   },
   "outputs": [],
   "source": [
    "write_file(output + '/nodes.txt', nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a165c6bbaeadeb",
   "metadata": {},
   "source": [
    "Now we follow Kepler@s Readme.md and prepare the KE and MLM data from the above files.\n",
    "\n",
    "    We will use the nodes...txt as our MLM data.\n",
    "    We will use the triples...txt as our KE data.\n",
    "    \n",
    "\n",
    "We first install the local version of kepler, which is built by extending fairsec:\n",
    "We now start with KE data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e21855481dca366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/sougata/projects/MyKEPLER\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cffi in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (1.16.0)\n",
      "Requirement already satisfied: fastBPE in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (0.1.0)\n",
      "Requirement already satisfied: numpy in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (1.26.4)\n",
      "Requirement already satisfied: regex in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (2023.12.25)\n",
      "Requirement already satisfied: torch in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/sougata/.local/lib/python3.10/site-packages (from fairseq==0.8.0) (4.66.2)\n",
      "Requirement already satisfied: pycparser in /home/sougata/.local/lib/python3.10/site-packages (from cffi->fairseq==0.8.0) (2.22)\n",
      "Requirement already satisfied: filelock in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/sougata/anaconda3/envs/mykepler/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/sougata/.local/lib/python3.10/site-packages (from torch->fairseq==0.8.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sougata/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq==0.8.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sougata/anaconda3/envs/mykepler/lib/python3.10/site-packages (from jinja2->torch->fairseq==0.8.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sougata/.local/lib/python3.10/site-packages (from sympy->torch->fairseq==0.8.0) (1.3.0)\n",
      "Installing collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 0.8.0\n",
      "    Uninstalling fairseq-0.8.0:\n",
      "      Successfully uninstalled fairseq-0.8.0\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../..\n",
    "python -m pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9e1284a830614",
   "metadata": {},
   "source": [
    "1. Encode the entity descriptions with the GPT-2 BPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d40c2a86fd04a3da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:03:44.046654Z",
     "start_time": "2024-04-21T18:03:19.299103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 10000 lines\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# mkdir -p ../data/gpt2_bpe\n",
    "# wget -O ../data/gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "# wget -O ../data/gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python ../../examples/roberta/multiprocessing_bpe_encoder.py \\\n",
    "    --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "    --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "    --inputs ../data/output/multi/nodes.txt \\\n",
    "    --outputs ../data/output/multi/nodes.bpe \\\n",
    "    --keep-empty \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d49e2d02a6433",
   "metadata": {},
   "source": [
    "2. Do negative sampling and dump the whole training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84b4c9cc932ab3f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:05:31.394911Z",
     "start_time": "2024-04-21T18:05:28.270520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-21 22:05:38.268515 load finish\n",
      "2024-04-21 22:05:38.353494 preparation finished\n",
      "2024-04-21 22:05:40.921061 training set finished\n",
      "2024-04-21 22:05:41.068764 all finished\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python ../../examples/KEPLER/Pretrain/KGpreprocess.py --dumpPath ../data/output/multi/KE1 \\\n",
    "    -ns 1 \\\n",
    "    --ent_desc ../data/output/multi/nodes.bpe \\\n",
    "    --train ../data/output/multi/train.txt \\\n",
    "    --valid ../data/output/multi/valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd1faad355263",
   "metadata": {},
   "source": [
    "3. then randomly split the KE training data into smaller parts and the number of training instances in each part aligns with the MLM training data\n",
    "For our case it will be just one split, since our data is small.\n",
    "\n",
    "Question: what does the negative_sampling_size = 1 do? it could be that the relation triples are false.\n",
    "Ans: it has 1 to 1 size of negative sampling, so for each positive triple, there is one negative triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81740bca42331380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:06:01.056909Z",
     "start_time": "2024-04-21T18:06:00.702697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data will be splited into 1 splits\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "python ../../examples/KEPLER/Pretrain/splitDump.py --Path ../data/output/multi/KE1 \\\n",
    "    --split_size 6834352 \\\n",
    "    --negative_sampling_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "443c0140c262b304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:08:37.828493Z",
     "start_time": "2024-04-21T18:08:14.748647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/head/train.bpe', validpref='../data/output/multi/KE1_0/head/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/head', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/train.bpe: 24675 sents, 953992 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/valid.bpe: 1299 sents, 50784 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/head\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/tail/train.bpe', validpref='../data/output/multi/KE1_0/tail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/tail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/train.bpe: 24675 sents, 305178 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/valid.bpe: 1299 sents, 16027 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/tail\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negHead/train.bpe', validpref='../data/output/multi/KE1_0/negHead/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negHead', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/train.bpe: 24675 sents, 941162 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/valid.bpe: 1299 sents, 47775 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negHead\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negTail/train.bpe', validpref='../data/output/multi/KE1_0/negTail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negTail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/train.bpe: 24675 sents, 963282 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/valid.bpe: 1299 sents, 44553 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negTail\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# wget -O ../data/gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "KE_Data=../data/output/multi/KE1_0/\n",
    "for SPLIT in head tail negHead negTail;\n",
    "  do\n",
    "    python -m fairseq_cli.preprocess \\\n",
    "      --only-source \\\n",
    "      --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "      --trainpref ${KE_Data}${SPLIT}/train.bpe \\\n",
    "      --validpref ${KE_Data}${SPLIT}/valid.bpe \\\n",
    "      --destdir ${KE_Data}${SPLIT} \\\n",
    "      --workers 60; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b51d97ba40a524",
   "metadata": {},
   "source": [
    "4. We then binarize them for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf539eb5ebea5814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:09:30.801792Z",
     "start_time": "2024-04-21T18:09:08.712651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/head/train.bpe', validpref='../data/output/multi/KE1_0/head/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/head', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/train.bpe: 24675 sents, 953992 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/head/valid.bpe: 1299 sents, 50784 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/head\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/tail/train.bpe', validpref='../data/output/multi/KE1_0/tail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/tail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/train.bpe: 24675 sents, 305178 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/tail/valid.bpe: 1299 sents, 16027 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/tail\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negHead/train.bpe', validpref='../data/output/multi/KE1_0/negHead/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negHead', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/train.bpe: 24675 sents, 941162 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negHead/valid.bpe: 1299 sents, 47775 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negHead\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/KE1_0/negTail/train.bpe', validpref='../data/output/multi/KE1_0/negTail/valid.bpe', testpref=None, destdir='../data/output/multi/KE1_0/negTail', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/train.bpe: 24675 sents, 963282 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/KE1_0/negTail/valid.bpe: 1299 sents, 44553 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/KE1_0/negTail\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# wget -O ../data/gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "KE_Data=../data/output/multi/KE1_0/\n",
    "for SPLIT in head tail negHead negTail;\n",
    "  do\n",
    "    python -m fairseq_cli.preprocess \\\n",
    "      --only-source \\\n",
    "      --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "      --trainpref ${KE_Data}${SPLIT}/train.bpe \\\n",
    "      --validpref ${KE_Data}${SPLIT}/valid.bpe \\\n",
    "      --destdir ${KE_Data}${SPLIT} \\\n",
    "      --workers 60; \\\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b26c9ea21a1c7b",
   "metadata": {},
   "source": [
    "We now start with MLM data preprocessing:\n",
    "\n",
    "\n",
    "1. Now we encode the nodes_train, nodes_train and nodes_valid with the GPT-2 BPE:\n",
    "   (gpt2_bpe is already downloaded during the KE data preparation, we reuse that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27f45e3f3e087a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:12:10.415575Z",
     "start_time": "2024-04-21T18:11:58.982707Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 10000 lines\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir -p ../data/output/multi/MLM\n",
    "\n",
    "for SPLIT in train valid test; do \\\n",
    "    python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "        --encoder-json ../data/gpt2_bpe/encoder.json \\\n",
    "        --vocab-bpe ../data/gpt2_bpe/vocab.bpe \\\n",
    "        --inputs ../data/output/multi/nodes_${SPLIT}.txt \\\n",
    "        --outputs ../data/output/multi/MLM/nodes_${SPLIT}.bpe \\\n",
    "        --keep-empty \\\n",
    "        --workers 60; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce4d7c38cf0c7f",
   "metadata": {},
   "source": [
    "2. We then preprocess/binarize the data using the GPT-2 fairseq dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "194dc596ddddd599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:13:21.109728Z",
     "start_time": "2024-04-21T18:13:14.268878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang=None, target_lang=None, trainpref='../data/output/multi/MLM/nodes_train.bpe', validpref='../data/output/multi/MLM/nodes_valid.bpe', testpref='../data/output/multi/MLM/nodes_test.bpe', destdir='../data/output/multi/MLM-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='../data/gpt2_bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, bert=False)\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/MLM/nodes_train.bpe: 14727 sents, 589525 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/MLM/nodes_valid.bpe: 776 sents, 27794 tokens, 0.0% replaced by <unk>\n",
      "| [None] Dictionary: 50263 types\n",
      "| [None] ../data/output/multi/MLM/nodes_test.bpe: 3876 sents, 142895 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../data/output/multi/MLM-bin\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir -p ../data/output/multi/MLM-bin\n",
    "\n",
    "python -m fairseq_cli.preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ../data/gpt2_bpe/dict.txt \\\n",
    "    --trainpref ../data/output/multi/MLM/nodes_train.bpe \\\n",
    "    --validpref ../data/output/multi/MLM/nodes_valid.bpe \\\n",
    "    --testpref ../data/output/multi/MLM/nodes_test.bpe \\\n",
    "    --destdir ../data/output/multi/MLM-bin \\\n",
    "    --workers 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3636e1cd925de4",
   "metadata": {},
   "source": [
    "All preprocessing is done, now we try out training the model with our data.\n",
    "\n",
    "Then we first train on the NLP model, since the kepler NLP pretrained model checkpoint is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43998fc0daaf9b39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T18:18:50.227779Z",
     "start_time": "2024-04-21T18:18:25.197513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sougata/projects/MyKEPLER/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export LD_LIBRARY_PATH=/home/sougata/.local/lib/python3.10/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "mkdir ../data/checkpoints/multi/\n",
    "\n",
    "TOTAL_UPDATES=125000                                    # Total number of training steps\n",
    "WARMUP_UPDATES=10000                                    # Warmup the learning rate over this many updates\n",
    "LR=6e-04                                                # Peak LR for polynomial LR scheduler.\n",
    "NUM_CLASSES=2                           \n",
    "MAX_SENTENCES=3                                         # Batch size.\n",
    "NUM_NODES=1\t\t\t                                    # Number of machines\n",
    "ROBERTA_PATH=../data/keplerModels/KEPLERforNLP.pt       # Path to the original roberta model\n",
    "CHECKPOINT_PATH=../data/checkpoints/multi               # Directory to store the checkpoints\n",
    "UPDATE_FREQ=`expr 784 / $NUM_NODES`                     # Increase the batch size\n",
    "\n",
    "DATA_DIR=../data/output/multi\n",
    "\n",
    "#Path to the preprocessed KE dataset, each item corresponds to a data directory for one epoch\n",
    "KE_DATA=$DATA_DIR/KE1_0:\n",
    "\n",
    "DIST_SIZE=`expr $NUM_NODES`\n",
    "\n",
    "python -m fairseq_cli.train $DATA_DIR/MLM-bin --KEdata $KE_DATA --restore-file $ROBERTA_PATH \\\n",
    "        --save-dir $CHECKPOINT_PATH \\\n",
    "        --max-sentences $MAX_SENTENCES \\\n",
    "        --tokens-per-sample 512 \\\n",
    "        --task MLMetKE \\\n",
    "        --sample-break-mode complete \\\n",
    "        --required-batch-size-multiple 1 \\\n",
    "        --arch roberta_base \\\n",
    "        --criterion MLMetKE \\\n",
    "        --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "        --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
    "        --clip-norm 0.0 \\\n",
    "        --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
    "        --update-freq \"$UPDATE_FREQ\" \\\n",
    "        --negative-sample-size 1 --ke-model TransE \\\n",
    "        --init-token 0 \\\n",
    "        --separator-token 2 \\\n",
    "        --gamma 4 --nrelation 822 \\\n",
    "        --skip-invalid-size-inputs-valid-test \\\n",
    "        --fp16 --fp16-init-scale 2 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
    "        --reset-optimizer --distributed-world-size \"${DIST_SIZE}\" --ddp-backend no_c10d --distributed-port 23456 \\\n",
    "        --log-format simple --log-interval 1 > out_multi.log \\\n",
    "        #--relation-desc  #Add this option to encode the relation descriptions as relation embeddings (KEPLER-Rel in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2983b5d41bde6",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da575c9d0abd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m fairseq-eval-lm ../data/output/multi/MLM-bin \\\n",
    "    --path ../data/checkpoints/multi/checkpoint_best.pt \\\n",
    "    --sample-break-mode complete --max-tokens 3072 \\\n",
    "    --context-window 2560 --softmax-batch 1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
